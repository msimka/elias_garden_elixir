{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UFF DeepSeek 6.7B Training - Tank Building Methodology\n",
        "\n",
        "Training UFF (UFM Federation Framework) model using Tank Building methodology:\n",
        "- **Model**: DeepSeek 6.7B-FP16\n",
        "- **Training**: RL + Supervised Fine-tuning\n",
        "- **Supervision**: Claude Code architectural guidance\n",
        "- **GPU**: Kaggle P100 (16GB VRAM)\n",
        "- **Time Limit**: 12 hours (within 20h/week)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Install dependencies\n",
        "!pip install torch transformers deepspeed accelerate wandb\n",
        "!pip install datasets tokenizers\n",
        "!nvidia-smi  # Check GPU availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import torch\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import deepspeed\n",
        "from datetime import datetime\n",
        "\n",
        "print(f\"\ud83d\ude80 Starting UFF DeepSeek training at {datetime.now()}\")\n",
        "print(f\"\ud83d\udd25 GPU Available: {torch.cuda.is_available()}\")\n",
        "print(f\"\ud83d\udcbe GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Load Tank Building training data\n",
        "with open('/kaggle/input/uff-tank-building-corpus/training_data.json', 'r') as f:\n",
        "    training_data = json.load(f)\n",
        "\n",
        "print(f\"\ud83d\udcca Loaded {len(training_data)} Tank Building sessions\")\n",
        "print(f\"\ud83d\udccb Stages represented: {set(session.get('tank_building_stage') for session in training_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Initialize DeepSeek model and tokenizer\n",
        "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Add padding token if missing\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "print(f\"\u2705 Model loaded: {model_name}\")\n",
        "print(f\"\ud83d\udccf Model parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Initialize DeepSpeed training\n",
        "with open('/kaggle/input/uff-tank-building-corpus/deepspeed_config.json', 'r') as f:\n",
        "    ds_config = json.load(f)\n",
        "\n",
        "model_engine, optimizer, _, _ = deepspeed.initialize(\n",
        "    model=model,\n",
        "    config=ds_config\n",
        ")\n",
        "\n",
        "print(\"\u26a1 DeepSpeed initialized for UFF training\")\n",
        "print(f\"\ud83c\udfaf Batch size: {ds_config['train_micro_batch_size_per_gpu']}\")\n",
        "print(f\"\ud83d\udd04 Gradient accumulation: {ds_config['gradient_accumulation_steps']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Training loop with Tank Building methodology\n",
        "import time\n",
        "\n",
        "def train_uff_model(training_data, model_engine, tokenizer, max_hours=12):\n",
        "    \"\"\"Train UFF model with Tank Building methodology\"\"\"\n",
        "    \n",
        "    start_time = time.time()\n",
        "    max_seconds = max_hours * 3600\n",
        "    \n",
        "    step = 0\n",
        "    total_loss = 0\n",
        "    \n",
        "    print(f\"\ud83c\udfd7\ufe0f  Starting Tank Building training for {max_hours} hours\")\n",
        "    \n",
        "    for epoch in range(100):  # Large number, will be limited by time\n",
        "        if time.time() - start_time > max_seconds:\n",
        "            print(f\"\u23f0 Reached {max_hours} hour limit, stopping training\")\n",
        "            break\n",
        "            \n",
        "        for session in training_data:\n",
        "            if time.time() - start_time > max_seconds:\n",
        "                break\n",
        "                \n",
        "            # Process Tank Building session\n",
        "            stage = session.get('tank_building_stage', 'stage_1')\n",
        "            component_code = session.get('component_code', '')\n",
        "            claude_feedback = session.get('claude_feedback', [])\n",
        "            \n",
        "            # Create training prompt\n",
        "            prompt = f\"Tank Building {stage}: {component_code}\"\n",
        "            \n",
        "            # Tokenize and train\n",
        "            inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "            inputs = {k: v.to(model_engine.device) for k, v in inputs.items()}\n",
        "            \n",
        "            outputs = model_engine(**inputs, labels=inputs['input_ids'])\n",
        "            loss = outputs.loss\n",
        "            \n",
        "            model_engine.backward(loss)\n",
        "            model_engine.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            step += 1\n",
        "            \n",
        "            # Log progress every 100 steps\n",
        "            if step % 100 == 0:\n",
        "                avg_loss = total_loss / step\n",
        "                elapsed = (time.time() - start_time) / 3600\n",
        "                print(f\"Step {step} | Loss: {avg_loss:.4f} | Elapsed: {elapsed:.2f}h | Stage: {stage}\")\n",
        "                \n",
        "    return step, total_loss / step\n",
        "\n",
        "# Run training\n",
        "steps, avg_loss = train_uff_model(training_data, model_engine, tokenizer, max_hours=12)\n",
        "print(f\"\ud83c\udf89 Training completed! Steps: {steps}, Average Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Save trained model\n",
        "output_dir = \"./uff_deepseek_trained\"\n",
        "model_engine.save_checkpoint(output_dir)\n",
        "\n",
        "# Also save in HuggingFace format for easy deployment\n",
        "model.save_pretrained(\"./uff_model_hf\")\n",
        "tokenizer.save_pretrained(\"./uff_model_hf\")\n",
        "\n",
        "print(\"\ud83d\udcbe Model saved successfully!\")\n",
        "print(f\"\ud83d\udcc1 DeepSpeed checkpoint: {output_dir}\")\n",
        "print(f\"\ud83d\udcc1 HuggingFace format: ./uff_model_hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Test component generation\n",
        "def test_uff_generation(prompt, max_length=200):\n",
        "    \"\"\"Test UFF model component generation\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test Tank Building stages\n",
        "test_prompts = [\n",
        "    \"Tank Building stage_1: Create a simple file reader component\",\n",
        "    \"Tank Building stage_2: Extend file reader with error handling\", \n",
        "    \"Tank Building stage_3: Optimize file reader for production\",\n",
        "    \"Tank Building stage_4: Add advanced features to file reader\"\n",
        "]\n",
        "\n",
        "print(\"\ud83e\uddea Testing UFF component generation:\")\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n--- Test {i} ---\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    result = test_uff_generation(prompt)\n",
        "    print(f\"Generated: {result[len(prompt):]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create training report\n",
        "training_report = {\n",
        "    \"model\": \"UFF DeepSeek 6.7B-FP16\",\n",
        "    \"methodology\": \"Tank Building + Claude Supervision\",\n",
        "    \"training_steps\": steps,\n",
        "    \"average_loss\": avg_loss,\n",
        "    \"training_time_hours\": 12,\n",
        "    \"gpu\": \"Kaggle P100 16GB\",\n",
        "    \"dataset_size\": len(training_data),\n",
        "    \"timestamp\": datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "with open(\"uff_training_report.json\", \"w\") as f:\n",
        "    json.dump(training_report, f, indent=2)\n",
        "    \n",
        "print(\"\ud83d\udcca Training report saved!\")\n",
        "print(json.dumps(training_report, indent=2))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}