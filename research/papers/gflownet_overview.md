---

# ELIAS ULM Learning Content
ulm_metadata:
  source_file: "/Users/mikesimka/elias_garden_elixir/apps/mfc/learning_sandbox/ulm_inbox/GFlowNets.rtf"
  content_type: "paper"
  language: "en"
  converted_date: "2025-08-31T03:05:50.618492Z"
  converter_version: "mfc-1.0"
  ready_for_learning: true
---
# Converted Document **Source Format:** unknown **Extraction Method:** native_elixir_rtf_parser **Extracted At:** 2025-08-31 03:05:50.616218Z --- Why care about GFlowNets? Why should you care about GFlowNets? Is it the next Transformer? Is it Yoshua bengio's pet project? Or is it one of those ideas that are.\n\nSo 2010s a0 a0 when all the cool kids are training a0 large language models in the 2020s? Today, I'll talk about why you should care a0 about GFlowNets. And why it is the future. My name is Edward Hu. I'm not exactly a0 impartial here because Yoshua Bengio a0 a0 is my PhD advisor,. And I directly worked a0 on GFlowNet,. But I also love what works! I invented low-rank adaptation, or a0 LoRA, when I was a researcher at a0 a0 Microsoft working with GPT-3 --. And a0 by the way here's a video on LoRA. Today, I'm a research scientist at a0 OpenAI.\n\nSo, I want to talk about what a0 a0 makes GFlowNets.\n\nSo exciting,. And how a0 it's going to shape the future of AI. First of all, GFlowNet sounds like a type of a0 neural network like a Transformer or a ResNet. However, it is not. GFlowNet stands for generative a0 flow networks. And is a learning algorithm. And before I tell you more a0 about the algorithm itself, a0 a0 I'm going to start with the problem it solves. The problems GFlowNets solve So, if you ask an AI practitioner a0 what their worst nightmare is, a0 a0 most people will tell you it's either a0 over-fitting or hyper-parameter tuning. And by the way, I have another video a0 on muTransfer which is a technique that a0 a0 allows you to tune hyper-parameters a0 for a large model much more easily, a0 a0 which I'll link here,. But today we're a0 going to talk about overfitting. Overfitting usually happens when we a0 ask the model to maximize something. For example, we might be maximizing the likelihood a0 of a dataset; the best way -- the perfect way a0 a0 -- to maximize the likelihood of a dataset is to a0 memorize it completely without "understanding" it. And if you just memorize the a0 answers to certain questions, a0 a0 it's not going to help you to generalize a0 to the questions you've never seen before. And by the way, something similar happens a0 in reinforcement learning as well, a0 a0 where if you maximize the reward, a0 very often what you find is hacks. A concrete example: drug discovery I'm going to give you a really a0 concrete scenario where this shows up. Say you're inventing a new drug a0 molecule through trials. And errors. What you're doing is basically reinforcement a0 learning albeit with a really expensive reward a0 a0 function, because the real reward function a0 is: you take this drug. And run a clinical a0 a0 trial. And you get a reward in the end, a0. But it's extremely slow. And expensive. What people do in practice is a0 they collect some clinical data a0 a0. And then they train a neural network a0 to simulate the real reward function.\n\nNow, you can imagine what happens a0 if you maximize a reward under a0 a0 this proxy model that is far from perfect. You're going to find a molecule that obtains a0 an extremely high reward under this model, a0 a0. But the chance of that molecule being actually a0 the drug you want is low, because there's.\n\nSo a0 a0 many ways to trick a neural network into a0 thinking a molecule has a high reward. However, this reward function is still a0 capturing some information about drug a0 a0 worthiness, even though we don't trust it 100%. Practically speaking, we don't just want the a0 a0 single best molecule under this a0 reward -- we want many good ones. Even better, these good molecules should be a0 as different as possible from one another. What we do. Then is we try a0 them all in the real world, a0 a0. And hopefully some of them are actually good. If we take just the best one, a0 it is almost guaranteed that a0 a0 this one molecule is exploiting some a0 imperfections in our reward model. In this example here, I'm highlighting a0 the importance of having diversity as a0 a0 opposed to finding just the max like in maximum a0 likelihood estimation or reward maximization. In fact, here's an idea: say we have a0 a reward function for molecules. What If instead of just getting a single a0 molecule, we have a generator of molecules, a0 a0. And here's what a generator does: it generates a a0 molecule with a probability that is proportional a0 a0 to the reward, meaning that if a molecule has a a0 really high reward,. Then it's more likely that a0 a0 it's going to be generated,. And if they have a0 a bunch of molecules that are equally highly a0 a0 likely under the reward function,. Then a0 they are equally likely to be generated.\n\nNow, as I generate molecules using this generator, a0 we're going to get candidates,. And most of them a0 a0 are going to have high rewards because low-reward a0 ones have low probabilities getting generated. Imagine an objective function that allows a0 you to train a generator like that. The objective function. And the a0 algorithm that allows you to do a0 a0 that could be generative flow network or GFlowNet, a0 a0. And this drug discovery example is actually the a0 motivating use case in the first GFlowNet paper. However, GFlowNet is much a0 more than just drug discovery. What GFlowNet really is The high level takeway is that a0 GFlowNet is a novel training algorithm. Instead of looking at a dataset or a0 a reward function. And ask "how can I a0 a0 find the function that maximizes this?" GFlowNet a0 asks "okay, how can I find a sampler -- a neural a0 a0 network sampler -- that allows me to sample a0 proportional to a given reward function?" If we're given a dataset a0 instead of a reward function, a0 a0 there's another paper from our lab a0 which says "okay, given a dataset, a0 a0 I'm going to first learn an energy-based a0 model,. And then I'll train a sampler." The sampler will sample proportional a0 to my energy-based model.\n\nSo, GFlowNet is really a0 shifting the question we ask: a0 a0 instead of maximizing something, a0 we're matching a distribution. And finally, I'm going to give you a quick example a0 of how this can be useful in the real world, and, a0 a0 actually I'm going to give you two a0 examples -- two papers that I led. And I'm happy to dive into these a0 two papers in future videos, a0 a0. But today I'm just going to give you a a0 quick taste of what it could look like. Applications: GFlowNet-EM The first paper is called GFlowNet-EM, a0 a0. And we're tackling a problem a0 fundamental to machine learning. And the second one is going to be more empirical: a0 it has something to do with large language models. Many of us have heard of the a0 expectation-maximization algorithm, a0 a0 which is used to find maximum likelihood a0 estimates in latent-variable models.\n\nSo here, the big idea is that in the expectation a0 step, we want to sample from a posterior a0 a0 distribution over latent variables. And what a0 happens is that this posterior distribution a0 a0 for non-trivial models is usually intractable,.\n\nSo a0 people do things like Markov Chain Monte Carlo, a0 a0 where they make simplifying assumptions a0.\n\nSo the posterior becomes easy to model.\n\nSo now, I have this intractable a0 posterior distribution which can a0 a0 be described by a reward function. Reinforcement learning can help us a0 find the maximum of this posterior, a0 a0. But we want to match the distribution. We want to draw samples from the distribution a0 for learning, which is a hard inference problem, a0 a0. And here GFlowNet converts this hard inference a0 problem, usually solved with simulation, a0 a0 into something we can use a neural network to a0 solve,. And we love training big neural network a0 a0 these days, because we're good at it, which a0 makes GFlowNet a bridge between classical a0 a0 problems in machine learning. And scaling a0 neural networks, which is the future of AI. Applications: Better LLM reasoning In the second example, we a0 have a large language model, a0 a0. And we want to use it to solve, say, a0 a certain kind of reasoning task. However, we don't have a lot of data points. What a0 we have is, say, maybe 10, 20, or 50 data points. What we going to do, instead of doing a0 fine-tuning, which easily leads to overfitting, a0 a0 we're actually going to search the posterior a0 or potential reasoning chains under this model. Usually, people either find the a0 most likely reasoning chain using a0 a0 reinforcement learning or a0 maybe few-shot prompting, a0 a0 basically hoping the model will come up with a a0 good reasoning chain if we ask it really nicely. Here, we're using GFlowNet to actually a0 train the model to directly sample good a0 a0 reasoning chains that could have led a0 to the correct answer under the model a0 a0 proportional to how likely the reasoning a0 chain can lead to the correct answer.\n\nSo the result here is that we're able a0 to boost data efficiency in many cases. This paper will actually be an oral a0 presentation at ICLR this year, a0 a0.\n\nSo maybe I'll see many of you in person. Conclusion.\n\nSo long story short, GFlowNet is not a0 a new neural network architecture. It's a new learning algorithm that allows you a0 to train a sampler that samples proportional a0 a0 to a reward function,. And it has many a0 applications going forward, especially a0 a0 as we focus on improving the generalization a0. And data efficiency of our neural networks. On the theoretical side, it has connections a0 to maximum-entropy reinforcement learning a0 a0 with path consistency objectives, which I'm a0 happy to dive deeper into in a future video. If you find this video helpful, a0 please like, subscribe, a0 a0. And share it with somebody else who might be a0 interested. I'll see you in the next video! --- *Converted using Multi-Format Converter (Tank Building Stage 2)*