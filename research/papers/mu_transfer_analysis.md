---

# ELIAS ULM Learning Content
ulm_metadata:
  source_file: "/Users/mikesimka/elias_garden_elixir/apps/mfc/learning_sandbox/ulm_inbox/muTransfer.rtf"
  content_type: "general"
  language: "en"
  converted_date: "2025-08-31T03:20:31.192505Z"
  converter_version: "mfc-1.0"
  ready_for_learning: true
---
# Converted Document **Source Format:** unknown **Extraction Method:** native_elixir_rtf_parser **Extracted At:** 2025-08-31 03:20:31.190142Z --- Intro Training a large model like GPT 4 requires thousands of GPUs,. But what if you can tune its hyperparameters on just one GPU? Transfer makes the hyperparameter optimization of an enormous neural network a lot cheaper by optimizing hyperparameters on a much smaller model. And transferring the optimal combination to a large model. It has been used by Cerebras-GPT. And cited by Chichella from Google DeepMind. And the GPT-4 technical report. My name is Edward Hu. I'm currently a PhD student advised by Yoshua Bengio. I created Transfer with Greg Yang.\n\nNow a co-founder of xAI. By the end of the video, you will know what Transfer does, why it works,. And how to apply it today. First, let's break down the mechanics of Transfer. Transfer in 3 steps It has three simple steps. First, parameterize your model in the Parametrization, or P. Second, search for hyperparameters on the small model. Finally, copy the optimal hyperparameters of a small model to a large model. Let's unpack the first step, parameterizing the model in P, with an example. Parameterizations describe how we change the model initialization. And learning rates as a function of width. For example, when we double the width, should we make the learning rate smaller by a factor of two, smaller by a factor of the square root of two, or not at all? These choices for every parameter throughout the network determine the parameterization. In this Jupyter notebook, we have a multi-layer perception, one parameterized in PyTorch default. And another in P. This one-line change that I've highlighted here parameterizes the model in P,. And when used with the Optimizer, we completely change the hyperparameter stability of the model. In this plot, the x-axis is the learning rate,. And different curves represent different widths. I recommend following this notebook which I have linked in the video description.\n\nNow, you might be wondering: why these changes? The default scaling in PyTorch or JAX is designed for stability during initialization. But not during training. We'll come back to this later in the video. In step two, we optimize hyperparameters as usual on the small model. Finally, we simply copy the optimal hyperparameters to a bigger version of the network without needing to retune them. It's important to keep in mind that Transfer doesn't tell you what the optimal hyperparameters are for the large model. Instead, it tells you how to transfer optimal hyperparameters from a small model to a large model. For example, instead of saying that the optimal learning rate is 2, Transfer tells you that for some layers, if you double the width, the optimal learning should halve.\n\nNow, let's talk about why P. And Transfer work. Why P. And Transfer work The most common operation in a neural network is matrix-vector multiplication, which is a series of dot products. Within each dot product, we sum over width-many products. In the infinite width limit, this summation has infinite terms. And gives us a coordinate of the preactivation. The key principle behind P is that this sum should neither explode to Infinity nor vanish to zero because neither of these two are useful for downstream tasks. To ensure that we steer away from these two extremes, it's important that we understand the two convergence laws governing the result of this summation: the central limit theorem. And the law of large numbers. If the individual pieces we are summing are independent. And identically distributed, or IID, with zero mean, the result of the sum behaves according to the central limit theorem. This occurs in a neural network because of the random initialization, which typically has zero mean. And IID coordinates. If these pieces we sum are IID. But have nonzero mean the result behaves according to the law of large numbers. This also occurs in the neural network because gradient updates are highly correlated with the input data,. And this correlation gives a nonzero mean.\n\nNow, we have a problem because according to the central limit theorem, we need to normalize by the square root of width to stabilize the sum,. But according to the law of large numbers, we need to normalize by width instead of its square root. P carefully manages these two behaviors.\n\nSo none of the layer activations explode or vanish. A finite-width neural network always learns some features in every layer. But the infinite-width limit is much less forgiving. There's a trichotomy of exploding to infinity, feature learning,. And vanishing to zero because any incorrect scaling is instantly amplified to the extreme by the infinite sum over width. In fact, P is the unique parameterization that maximizes feature learning, because other stable parameterizations fail to learn features in some layers. The in P actually stands for maximal update for this reason. P is also unique in that it is the only parameterization that gives empirical hyperparameter stability, as shown in this animation, where we sweep learning rate for different widths. Check out Greg Yang's 2-hour talk in the video description for a more technical take on P. How to apply Transfer today Finally, how can you use P. And Transfer today? The easiest way is through the P repo on GitHub. We have examples on MLP, Resnet,. And Transformer in the repo, linked in the video description. You can use our package directly in your code by changing a few lines of code. Last. But not the least, how can we check if P has been correctly implemented? First, the activations after a few steps of training should stay stable across widths. We call this the coordinate check. We should see this stability for P. But not for standard parameterization. Once we have passed the coordinate check, a more expensive. But more reliable check is to verify hyperparameter stability through a sweep of a chosen hyperparameter, learning rate for example, for different model widths. It is important that the hyperparameters for the smallest model are optimal in this case. You can see this example in the Jupyter notebook for more detail. If you encounter difficulties, a good resource is the issues in the P repo. You might discover that others have resolved similar issues through discussions with Greg. And me. If you can't find an answer feel free to open a new issue,. And we'll try our best to respond. Thank you for watching,. And I hope this helps! --- *Converted using Multi-Format Converter (Tank Building Stage 2)*