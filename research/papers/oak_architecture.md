---

# ELIAS ULM Learning Content
ulm_metadata:
  source_file: "/Users/mikesimka/elias_garden_elixir/apps/mfc/learning_sandbox/ulm_inbox/oak_architecture.rtf"
  content_type: "paper"
  language: "en"
  converted_date: "2025-08-31T01:16:36.551521Z"
  converter_version: "mfc-1.0"
  ready_for_learning: true
---
# Converted Document **Source Format:** unknown **Extraction Method:** native_elixir_rtf_parser **Extracted At:** 2025-08-31 01:16:36.548990Z --- [Applause] I had I had.\n\nSo much fun talking with with.\n\nSo many of you here today. Um. And you know we need a conference on reinforcement. It's it's clear. I wasn't sure. But now I think it's clear. This was the right move. Um.\n\nSo I have prepared for you a talk uh on the oak architecture. It's a vision of super intelligence from experience. It really is my attempt to um address the issues at the center of of AI. And I want to start just by recognizing how um how difficult. And important is the task of AI. Um.\n\nSo AI AI is a grand quest. Uh we're trying to understand how people work. We're trying to make people. We're trying to make ourselves powerful. Um. And this is a profound intellectual milestone. It's going to change everything. You know this,. But it's good to take a moment. And pause. And recognize what we're doing is incredibly hard, incredibly important. As an intellectual milestone. I think it'll be comparable to the origin of life on the earth at least when we under when we when some part of the of the of the planet understands how it works. And how it thinks. And how it uh can be.\n\nSo uh transformative to our to the to the to the planet.\n\nOkay. But it's also a continuation of things that what we've always done. And it's just the next big step.\n\nSo now myself I um I think this is just going to be good. Lots of people are worried about it. I think it's it's going to be good. It's an unalloyed good. And I think the greatest advances are still ahead of us. It's a marathon. I think you know good for this group that the path to full AI strong AI runs through reinforcement learning. And not I think through things like uh to non-experential things like large language models. The biggest bottleneck is strangely is we have inadequate learning algorithms. You may think we have our deep learning. And we that's the one thing we know. But I think it's not like that at all. I think it's it's more like we they are very our algorithms are very crude. They need to be they need to be better. And that is what we should be working on.\n\nOkay.\n\nSo,. And then myself uh I've tried to think deeply about this intelligence for half a century. Every day I'm sort of in the trenches designing algorithms, trying to design algorithms, seeking better algorithms for reinforcement learning, for learning from experience. And I follow this Alberta plan for AI research which you may know about. Mike. And Patrick. And I did it a couple years ago. And today I'm going to talk about the vision of an overall agent architecture AI agent architecture called oak. And I think it provides a line of sight towards towards our grand prize of understanding mind.\n\nOkay.\n\nSo those are my my introductory comments.\n\nNow let's talk about oak. I think it's fun just to start with the name.\n\nSo you which may be a mystery oak. It comes from the idea of options. And knowledge.\n\nNow, as many of you are very familiar with, an option is a pair. Well, actually, you may think it's a triple. I sort of have dropped the initiation set in in in my for many last couple decades. Um,.\n\nSo for me, it's a pair of a policy, a way of behaving,. And a way of deciding to stop behaving in the Okay,.\n\nSo just those two, just a pair. And in oak the agent has lots of options. And it's going to learn its knowledge will be about what happens when you follow the option.\n\nSo in this way the agent is meant to learn a high level transition model of the world that enables planning with larger jumps. And hopefully carves the world at its joints. Um yeah.\n\nSo that's that's uh where the name is from. I think it's is a grand challenge. And a grand quest. And so I show it like this that we are seeking the holy grail. The holy grail of AI.\n\nLet me put that up in a way it's easier to read. Uh we want this we want this AI design that is domain general contains I'm going to say nothing specific to the world. We want a general idea.\n\nSo I'll talk about that more in a minute. But let me just put down my three main uh design goals. It should be domain general. It should be experential. That is the mind should grow from runtime experience not from a special training phase. And third it should be open-ended in its sophistication in its abstractions.\n\nSo that um it's a it can it can form any any concepts in its mind that are needed to deal with whatever world it's connected to limited only by its computational resources.\n\nSo those are the three main deterata. And we'll talk about them. I guess first I want to establish some words.\n\nI want to talk about I'm going to talk about design time. And runtime. Design time. And runtime. When you're in the factory being designed. Then your robot goes out. And lives its world that's its runtime sort of a lot a lot the way Leslie spoke um.\n\nSo the age is designed s at at design time we would be building in any domain knowledge that we might have farther away from the chest might be just pardon me.\n\nOkay, cool. At design time is when you're building in any of your domain knowledge. And then at runtime is when you're actually interacting with the world. Learning from experience, making plans that are specific to the part of the world you're in.\n\nSo like uh I don't know a large language model everything is done as design time. And when it goes out to be used in the world it doesn't do anything. My emphasis is going to be the other way around. We want I want to do all the important things at runtime online on the job. And in a minute I'll be talking about a big world. But a big the idea of a big world a big complex world is that you're you're not going to be able to build things in it. You're not going to be have your agent know everything about the world in the factory because the world is huge. And you can't know everything. Your a your robot is small compared to the world. And uh.\n\nSo if you want to be able to learn arbitrary open-ended abstractions um you need the world to find out whether the right abstractions for for the part of the world you're uh running into.\n\nSo you got to do it at runtime.\n\nSo I don't know.\n\nSo I'm going to talk about this a lot. Runtime. Everything has to be done at runtime. And why that's actually a good way to think about it.\n\nOkay,.\n\nSo let's just ask the question. Yeah, should an agent's design this is a question for you guys, right? Should the agents design reflect the world in which it's expected to be used? Good thing about this question is both answers are wrong. I mean both answers are right.\n\nSo you can um yeah if you want something to uh perform. Well and you want to put it out there. And have it do something good right away, you know, you want to you want to put design domain knowledge in there. You want it to reflect the world. But if you want a good design,.\n\nSo I'm gonna say no. My quest, my quest is that the design should not depend on the world at all.\n\nOkay. Uh it should be domain general.\n\nNow this is really just you know there there are multiple questions. And there are multiple uses. And like you know I I I have to respect totally you know someone who wants to do an application make something that's useful make something that performs. Well sure that's important. But also important is let's understand the mind in a simple way let's let's what we what we would want is a a conceptually simple understanding of what's going on inside a mind. That's that is in some sense the grand quest of of AI is to understand what it means to achieve goals, what it means to understand an arbitrary world. It's it should be simple. And if you the world the actual domain that you're going to interact with is arbitrarily complex. And so your goal your job your agent's job is to go out there. And learn all the fiddly wonky little details. And highle structures of the world that it's going to encounter. But you really don't want to understand what it's doing at that in terms of all those domain specific details. You want a level of understanding that is at a higher level. And is based on principles. And not the intricate complex details that that are in the world.\n\nSo it's sort of different purposes.\n\nSomeone who wants to go have something that will perform really. Well right away or someone who wants to have a conceptual understanding of what a mind is. And how intelligence works.\n\nSo my quest is to do the latter. And so it's only natural that uh that we would want to exclude those things.\n\nSo we of course I have to reference the bitter lesson at this point. Um. And I'll just read this. The actual contents of minds are part of the arbitrary intrinsically complex outside world. They are not what should be built in as their complexity is endless genuinely endless. Our our agent has a big computer. And we don't want to understand everything going on in there. Instead, we should build in only the meta methods that can find. And capture this arbitrary complexity. In short, we want agents that can discover like we can, not which contain what we have already discovered.\n\nSo that's the idea for the purpose here in a scientific conference trying to understand what a mind is, how it should work. We want we want a level of description that is above all those domain specific uh endless details.\n\nOkay.\n\nNow second question, should the agent learn from special training data or should it learn only from runtime experience? only. You must be triggering a little bit on the word only. Only from runtime experience. Well, for me the agent should learn only from runtime experience should be entirely experential. And the reason is again um we want a conceptually simple design. And if it's possible uh to learn only at runtime that would be a simpler a simpler understanding. And um.\n\nSo we should seek this.\n\nSo the form of the argument is going to be there's certain things that that have to be done at runtime. And and. But and could be done at design time. And uh.\n\nSo basically everything has to be done at runtime maybe also at design time. But it ha at at runtime it ha you have to be able to learn you have to be able to change your abstractions you have to um be able to make a model of the world you have to be able to plan with that model every all those things have to be done at runtime because you're going to encounter the world. The world might not be like what you expected. And certainly you won't know all the intricate details. And all the abstractions that are needed for for the part of the world that you're interacting with.\n\nSo all those things have to be done at runtime.\n\nNow you could also do some of them at design time. But it's sort of in some sense that would be optional. You know, that might speed you up,. But since they all have to you have to be capable of being done at at runtime, why not make a conceptually simple design. And just doesn't worry about trying to get a head start on the problem,. But just has the uh the u runtime aspects. And so that's what that's what my quest is. That's the that's the the uh journey I'm the quest I'm setting out on. And I'm hoping you're going to join me.\n\nOkay.\n\nSo I've talked about the big world perspective.\n\nSo let's let's do that. Let's talk let's make let's gain a common knowledge amongst us of what that means. The big world perspective or the big world hypothesis something that's been floating around at least in Alberta for like five years. And we've all come comfortable with it. Um. And it's really influences all of our thoughts. And our designs.\n\nSo the idea is simply that the world is bigger, more complex than the agent. Uh. And it's much bigger really. It's bigger than this. It's it's big, you know, it's really big. And it's got to be much much bigger than the agent because the world contains, you know, billions of other agents. And um. And all of course all the atoms. And all the intricacies of uh the objects spread around. Um the world is much more big. And and the world contains what what is happening in all those other agents matters to you. It matters to you what's going on in the minds of your your friends. And your loved ones. And your enemies. Um all all those things are important to you. And they have to be taken into consideration. And so you know the upshot is that nothing you the agent is going to be doing is going to be exact. And it's.\n\nSo and it's not going to be optimal. It's going to be approximate when you make a value function. It's going to be of course be an approximate value function. Uh. And your policy will be will not be the optimal policy. Your transition models will be much enormous enormous reductions. They're sitting inside your head. You know, your model of the world. And the world is out there much much bigger, right? Um even a single state of the world, you can never hold it in your head. You can never hold in your head all the the states in everyone else's minds.\n\nSo, one of the most important consequences of this is that the world ends up appearing non-stationary. Uh,. And I'm citing a little paper on this that Dave Silver. And and a coupe. And I wrote where we just made this point. The world, you can probably see it. The if you don't have a model, you don't have a good sense of the state of the world. And it's it's big. And sometimes you're there, sometimes you're there, things look the same, your function approximator cannot capture everything, the world's going to look non-stationary. And so you have to learn at runtime because you're going to you can't you can't have built in everything about the the whole big world at design time. You have to learn at runtime. You're going to encounter some particular part of the of the world at runtime. And you want to uh customize. And be appropriate for that part. You know, you go to you go to work, you're you're you're AI agent that's supposed to be play a productive role in society. And it goes to work. And it meets its co-workers. It has to remember the name of the guy it's working with. You know, that was not in the domain knowledge. It has to remember uh the work that they've done on that project. What's working well? What's not working well? What what are they trying to do? All those you know everything you do in your life uh is is is un could not have been foreseen. And to to um.\n\nSo I know it should be obvious you have to learn during your life. You have to plan during your life. I'm Leslie Caling made this point that planning is required because the world is big. Um,. And so this also applies to your abstractions. You know, my third desitter is that we want an open-ended abstraction. We want to be get more. And more sophisticated understanding of this particular world. We have to find, you know, what's the right joints. And ideas that are involved in the world that we're encountering. And so, um, yeah, you you may be able to also add abstractions. Maybe you believe in objects like Leslie. And and you want to build that in,. But that doesn't get you out of having to to have the ability to create new abstractions at runtime. And so, if you're going to have to create them at runtime, you know, why don't we just do it in one place. And and. And uh that'll be a very good start. You know, I think of a design for an AI, the perfect design. It would not be a huge thing. It would not be like an encyclopedia or a library worth of knowledge. It would be like uh well, when I make for me actually it almost fits on a slide. You write in the pseudo code, you know, maybe it's three slides.\n\nOkay. I thought I think something of that that order. Five pages for your uh description of all the essential elements that are domain independent. And and. And and yet are capable of arbitrary um open-ended abstractions.\n\nOkay. Um yeah,.\n\nSo this talk like hey um I was up till like the like four o'clock last night making making these slides. Um this is this is um this is sort of a new talk for me. This is the first time you're the you're the first guys hearing it. I've been I've been traveling around the world. And giving all these uh philosophical talks. And political talks. And point of view talks. And that's great. And I've enjoyed that. But but uh you know I really thought here I'm I'm at the RLC conference reinforcement learning I should do something uh substantive maybe even technical um.\n\nSo so I um. And then all during the week you know I went to every I went to all these talks I talked to all these people. And uh I I keep just changing my mind about what I want to say. And uh.\n\nSo anyway, this is all my way of of excusing myself or explaining to you that that this talk is new. And it's not quite polished. In in particular, I might say some things more than once. Um.\n\nSo and maybe it's okay to have some repetition if they're important things. Um okay,. But just be aware of that. And maybe maybe kick me if I if I uh need to move on to the next thing.\n\nOkay, I'll try to be quick. Runtime learning I think always wins over design time because the the world is much bigger than the agent, the big world perspective. Um design time can't cover every case. Runtime learning can customize to the part of the world actually encountered. Runtime learning scales with available compute whereas design time learning or anything done at design time scales with the available human expertise at design time. It was the only thing available. And historically scaling with compute wins in the long run. That's what the bitter lesson is is explicitly about. However, today's deep learning methods, runtime deep learning methods, continual learning, they don't work very well.\n\nOkay, this is this is a a a big bitter thing for me. I wish they could worked. Well because I'm talking all about runtime learning. And I want to use it. Um yeah, if we if we one last thing about runtime learning, it does enable metalarning. Metal metal learning is where you like try learning one way. And then you try learning another way. And you notice that oh this way works better in the future I will do this. If you were if you were doing everything in one shot you couldn't do that. This idea of becoming better at learning requires uh one time you're doing learning another time you're trying a different way of learning. And you pick the better one.\n\nSo metalarning really requires this to be done at the at the runtime.\n\nOkay.\n\nNow let's think about the problem just a little bit more. You know I like to separate things into the problem. And the solution. Really almost everything I've been talking to you about is the the problem the quest. What are our goals? What are our desitter?.\n\nSo um one more slide on that. The AI problem is to design an effective purposeful agent that acts in the world. And the classic reinforcement problem is the same thing except we add the purpose is specified by scalar reward signal. The reward. And the world is general. And incompletely known. But the world can be anything could be grid world or the human world can be stochastic, complex, nonlinear, non-marov. The state space of the big world is effectively infinite. And its dynamics are effectively non-stationary. Um let me go ahead to just talk about this one a little bit further. The purpose is specified by a scalar signal.\n\nSo that's we have a name for this idea. It's called the reward hypothesis. And I I wanted to bring this up because we have thought about it. And uh it's not like a quick choice without intention. Um.\n\nSo the reward hypothesis is this that all of what we mean by goals. And purposes can be. Well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal called reward. Um,.\n\nSo there's lots of specific things there like the expectation, like the cumulative sum. Um, yeah,. And that's been thought through. And the SC the idea of a scaler reward, I just want to say it's not just a uh something we haven't thought about. In fact, it's a it's a great thing. It's a really clear way to specify the goal. It's become popular in many different disciplines, not just AI,. But also economics. And psychology, control theory,. And forever people have been trying to modify. They've been trying to add things like constraints, multiple objectives, risk sensitivity,. And and. And I I I don't know. I I hate that. I mean, even if it was good to do, I I I I don't want it to be done. I don't want it to be true. I I you I think you've already gotten the sense. I like things to be simple. You know, that's like a really high high uh uh deserata desire. I want things to be simple. And I might even simplify them a little bit too far in order to uh to uh be clear. And uh.\n\nSo anyway, I want things to be simple. Do we need to do we need all these things to get generality? That's the real question. And Michael Bowling. And others have have written this really nice paper called settling the reward hypothesis where they go through all these cases. And and I don't know if I want to uh they they they establish that in a certain sense the reward hypothesis is correct that that you don't um add generality um uh by adding multiple objectives or risk sensitivity or any of these constraints.\n\nSo they it's one way of validating that choice. And you might also probably know the reward is enough paper where we argue that even a simple reward can lead to all the attributes of intelligence in a sufficiently complex world.\n\nOkay.\n\nSo um.\n\nNow I want to talk about the solution methods the architectures obvious starting place is model free reinforcement learning basic reinforcement learning where the agent constructs a policy. And value function at runtime both these are functions all all these are runtime RL architectures okay the model free. And then you can handle the non-markoff case if you uh construct your feature uh state representation from u uh from from your data. And I'll show you the the picture of that in a second. Uh. But still better uh would be to make a model of the world. And use that model to plan with potentially better.\n\nNow the oak architecture is along the same line um of improvement of of extension. And the the thing about the oak architecture is it it adds to those things u auxiliary problems subpros. And those sub problems are in the form of attaining individual features individual state features. And in this way uh we enable the discovery of higher. And higher uh levels of abstraction. And we we achieve this open-ended uh goal.\n\nOkay.\n\nSo, as a in a picture, this this picture on the on the left is from uh the textbook, the reinforcement learning textbook. Um. And this is the very last figure in the book. And so maybe it's familiar to some of you. Uh we have uh the world. And then the agent is everything above the world. It's the policy. And value functions. It's a model of the world. It's a planner. And it's the Ubox. The viewbox. I want you to notice because this is a is a um kind of reinforcement learning where we don't assume that the state is available to the agent. We only observations are available to the agent. This we send actions to the world. The world sends back observations. And and a reward.\n\nOkay. And then there's a a process here that that's a part of the agent uh that that computes uh something that we'll use as a state representation uh by the by the policy. And value function.\n\nOkay.\n\nSo that's the construction process. And um nowadays I draw um things uh a bit differently. Um. And this the right figure is the full oak architecture. And you see the many of the same components. This U box is.\n\nNow called perception. Perception is a better name for it because what does it do? This perception process it takes in the uh the data that's happening the actions. And the observations. And it forms a a sense of where the agent is now. That's really what perception is about. Take in your sensory input, get a sense of where you are now. Use that sense to make your decisions to to as input to your policies, your value functions. And your models.\n\nOkay,.\n\nSo the oak architecture has all those things,. But it also adds auxiliary subpros. And those each subpro will have its own value function. And its own policy. Uh.\n\nSo that's what's suggested by having uh these u shadow policies behind the the main policy. And these secondary auxiliary value functions behind the main value function. Um also each one of these subpros is going to be based on a different component of the state feature representation.\n\nSo this is the thing that acts like state. And I want you to think of it as a feature vector. And each one of these sub problems is going to be based on a different component of that feature vector.\n\nSo that's what it looks like as a picture.\n\nOkay. Uh.\n\nNow we're going to get into the a bit of the nitty-gritty.\n\nWe're going to look at it. I've given all these these quick introductions to the idea tell you I've told you various properties of the oak architecture.\n\nNow I want to tell you exactly what it is or it is these these what eight steps done in parallel at runtime.\n\nOkay, it's a lot of steps. Um we're I'm going to come back to this slide a bunch of times uh. And develop each part of it.\n\nSo just relax. And let's get started. Let's just learn what some of the the the characters are here. And how they interrelate.\n\nSo we might start with the first line. Um. And this line uh is learning the policy. And the value function for maximizing rewards. That's like normal reinforcement learning. And I think that is is is almost done. If it was done, there would be a green check mark,. But it's blue. And blue means it would be done if we could do this continual deep reinforcement deep learning thing with metalarning. You know if we really could do continual learning that would be uh that would be done. And so we can do this with in very simple simplified cases. We can deal it with uh some of the algorithms like continual backdrop. We can deal with um the linear case.\n\nSo this is gets a blue check mark conceptually done. But um really it's waiting to be done well. It's waiting to uh solve this problem of continual learning for deep learning. Um.\n\nNow the next one uh is red because we don't really have a solution. We have lots of ideas. But we don't have a specific proposal. And so I'm going to come back to this later. The second one is generating new state features from the existing features. And let me go over this next bit uh quicker.\n\nLet me just run all the way down verbally through all eight steps. We we we're going to have some features.\n\nWe're going to order the features.\n\nWe're going to take the highest ranked features, the most important features according to our estimation,. And we're going to create subpros of of achieving them.\n\nSo if I decide that being in this lecture room is a is an important sub goal, I would make a a sub problem for that would be rewarded when I when I when I not would would be would would succeed when I am here. If I think uh holding this microphone up sufficiently close to my mouth is a good sub goal. Um, I would I would uh I would I would make that feature into a a good subpro. And so on for finding the restroom. And finding the the the uh the coffee. Coffee is a really good you know feature for that flowing into your mouth. And getting all the sensations involved in that.\n\nSo that feature becomes a subpro for attaining it. And then you learn solutions.\n\nSo this is the heart of of um the heart of the oak architecture is to have sub problems. You learn the solutions of the sub problems. The sub problems are the options from the from from the O. And oak. And uh we also have to learn the value functions that are associated with the sub problem.\n\nOkay. And then we're going to have these options. And the next step is we're going to have we're going to learn models of the option. We want to know uh what will happen if you if you were to execute any any one of them. This will be part of your model of the world. But it will be a high level model of the world because it'll be about uh an extended way of behaving rather than about a single action. These models will enable you to plan. And then. And then you.\n\nSo those are all the the full main steps.\n\nOkay.\n\nSo you've seen it once. Uh you're going to have to maintain metadata on on the utility of everything. And and curate uh throw some things out um. And propose new ones.\n\nOkay.\n\nSo now we're going to go through these steps. And for a while I'm going to spend a lot of time um actually on the fourth one. But yeah the the ordering the features it seems kind of easy. But but we can't do it until we have all the rest all the other pieces done that a couple cases that will h that will happen.\n\nOkay let's spend some time about the creation of the subpros one for each highly ranked feature.\n\nOkay.\n\nSo on acknowledge there's a long history of looking at subpros that are distinct from the main problem. People talk about curiosity, intrinsic motivation, auxiliary tasks.\n\nSome things are settled, some things are unsettled. You don't have to read all this.\n\nI want to direct your question to the red part. The key open questions about subpros, which are what should the subpros be? Where do they come from? How can the agent or can the agent generate its own subpros. And how do the sub problems help on the main problem?.\n\nSo the contribution of oak is to an is to propose answers to all these questions. And to really uh answer questions like uh the third one how can the agent make its own subpros in the affirmative. And thus get open-ended abstraction. Um.\n\nSo I like to think of it very basically that we have problems. And solutions. And these interact with each other. We propose a problem to work on. We work on it. We solve it. As as a part of solving it we will make new features. And those features will. Then be the basis for new sub problems. And and. And then the sub problems will have to be solved new features. And so on in an endless cycle. Roughly. That's what I'm talking about. And I wanted to give some examples from nature. Uh here's an orang ba a young orangutang um playing swinging. And so I What is he doing? Like he's not getting food. He's just interested in what it feels like when he swings. Yeah.\n\nOkay. That's what I think he's doing. I think it's that sensation is interesting. And and he got it once. And now he's trying to get it again. And understand how to control it. Um yeah,.\n\nSo here's. And also on the other slide we have an orca uh who somebody threw this big uh I don't know what to call it right now. A buoy into his into his pen. And he's decided trying to figure out what he could do with it. And he's managed to get it up on his back.\n\nSo, that was not not random. Um, he got this idea. And now he's perfecting it. Yeah.\n\nSo, animals play, people's play, uh, infants play, young people play. Um,.\n\nSo this is a sped up video of a infant playing. And, uh, this is what we want. We want the the way the the child goes from object to object, learns a little bit about it, gets bored, moves on to the the next object,. And just gradually develops a better. And better understanding. Maybe next time when he comes back to the to an object, he'll he'll he'll uh have an increased ability, be able to do new things with it. Um, this is what we want. And so I'm trying to think about them as posing sub problems for themselves, things to learn about, things to understand, things to predict,. And uh things to control. And and figure out where it can make progress in in learning solving the sub problems.\n\nOkay,.\n\nSo maybe you're ready to accept this statement. The agent must create its own sub problems. Sub problems can't be given to you. Can't be given to you at design time. You've got to create your own that's far too various. And world dependent to have been built in. We have to give the responsibility of the the questions the problems not the solutions not the features the questions okay what is the pro what is the how can we how can we um do this I mean we have much of the machinery the machine of options general value functions off policy learning planning methods these are machinery to help us in this process. But we want to create them in a domain independent way. And that's challenging Okay.\n\nSo I want to offer this this possible way to make subpros in a totally domain independent way which is um when you come across a feature when you make up a new feature or you experience a new feature you can make it to be uh the basis of a sub problem uh I call it a reward respecting subpros of feature attainment.\n\nSo let me show you exactly what that is. Um how do we create a subpro from a feature?.\n\nSo a feature is like yeah feature eye. It's it's a bright light that you saw once. It's a it's an interesting sound that happened. It's you you're a baby. And you heard the rattle make a sound. You'd like to reproduce that sound.\n\nSo you have a feature I a feature index I. And you have kappa which is how intensely you want that feature. You have to express that uh. And you'll get different um subpros if you if you want it at all costs or if you just kind of want it a little bit.\n\nSo the sub problem is to drive the world to a state where the feature is high without losing too much reward because you will lose some reward if you're not doing what you normally do because what you normally do is uh maximize reward. There is just one reward by the way. And so I don't have to qualify that is the real reward. And the sub problem is to achieve a state where the feature is high uh without losing too much reward without having to go through something that's painful or having lost opportunities to get something uh pleasurable.\n\nOkay,.\n\nSo we're trying to find an option. An option is a pair. It's a policy pi termination function gamma that maximizes the value of the i feature at termination while respecting the rewards. And values.\n\nSo here's the equation. Maybe we can understand the equation. Uh in each state you're trying to choose you're trying to maximize. And you're going to choose pi. And gamma to maximize. And it's the sum the sum is conditional on starting uh the world in in the indicated state because um yeah for each for each state we say if we started there have a policy pi that gets you rewards um from t+1 to t. T is is the time of termination. You're going to follow the option pi. And you're going to terminate when gamma says terminate. And so that will fix establish the random variable which is the time which you terminate capital t. And if you look at all the rewards you receive while you are following the option summing them up. Those are the things that you want to be as big as possible or not as least negative as possible. And then you want to um reward yourself for achieving feature I's feature I at time of termination S sub capital T. And you know there's a there's a waiting by kappa.\n\nSo you want lots of rewards. You want to be the feature to be true. But you know it's got to be traded off the rewards. And you also care about the state that you're in at the time of termination. You don't want to like find a really good way to to uh I don't know get some coffee. But has the consequence that you have to break your leg.\n\nOkay. Actually, that would be a reward. That would be a bad reward. You don't want a way to get coffee that would um uh leave you in a bad state. Like let's say you got coffee. But uh you know you're gonna get arrested or you're going to fall down the stairs.\n\nOkay. Um those are bad states. You know I often you know the walk along the edge of a cliff. But uh don't fall off. If you fall off, it actually doesn't hurt very much to fall off because, you know, if you can say just terminate while you're in the air, uh the rewards are fine,. But the value the value is, you know, bad rewards are coming up.\n\nSo, your value will be will be poor.\n\nOkay, that's how we create a sub problem. And now let's really we're getting into the heart. Uh we have these processes. Um the f the first one is we form the uh the problem just as we just talked about you know given a feature form a problem. We do that with all the high highly ranked features.\n\nSo we have.\n\nNow we have you know dozens of pro of problems. Each one we work on it to produce an option. The solution to sub problem is an option.\n\nNow you've got this options. Well that defines a correct transition model for the option. Is it. Well defined? Um what we know what the model should be. If you give me the way of behaving. And the way of terminating, we know what the model should be. And so this is something you work on. You work on computing this model, approximating that model. Once you have the model. And that you have a models of all the different uh options for all the different subpros for all the different features. And once you have the model, you use the model of course to plan. And to improve your behavior.\n\nOkay.\n\nSo we got these three steps. And there's one fourth step which is that uh you have to come up with features right we we we started with a good set uh with the highly ranked features.\n\nSo we have to have a way of ranking the features. And um. And I just want to point out that we have that because all of these these these three uh pillars the the later three all use features to to do their job right if you're going to find the option the option is a function of state. And so you have to look at state features when when should you do the option when when it when is the value function of the option uh when should it have which values it's going to look at the state features to make those decisions when you uh learn the models of the options you're going to look at the state you start in. And you're going to look at the state features of that state. And you're going to say oh that feature I found useful that other feature was useless to me um. And so. And and. Then when you use the models you will find some models are useful. And that will sort of trickle back to evaluate the choice of the of the options. And that will also trickle back to evaluate the choice of the feature attainment problems. And at least uh all these learning processes, predictive learning processes will use the features. And they will provide feedback to the uh to the features saying these are the ones that have proven useful to us. These ones have not.\n\nOkay.\n\nSo let's draw that the same idea with a different picture. I'm going to have many pictures about the same idea. And I'm going to say it a few times.\n\nSo maybe you'll you'll get it. Um this way of talking. And thinking.\n\nSo the perception process is going to is responsible for constructing interesting state features. Um the play process or the problem posing process problem posing. And solving. That's where you do the sort of core reinforcement learning things of figuring out your value functions. And your policies. And you produce the options. And then you have to predict the consequences of those options to form a transition model. And then of course you plan with the transition model to get improved policies. And values. And the feed we close the the cycle is we have feedback from the later steps back to the construction of features. And that feedback is mainly say saying I have I have found that feature useful or I have not found that feature useful.\n\nOkay.\n\nSo here's we're back to our eight steps. U we.\n\nNow understand what it means to create the subpros one for each feature. And we also know what it means we've talked about how you learn the solutions um. And the transition models. Um maybe there's I'll say one more slide about that about this this topic uh how we learn those things. Oh,. But also notice they're they're in blue because although we know how to do these things, we don't really know how to do them with continual deep learning or maybe we have to use Shbanch's continual backdrop. You know that this is this is this is a topic we'll come back to. This is incompletely understood.\n\nSo we kind of know how to do them. But we definitely think we can do better.\n\nOkay, one one short slide more about that is it to a large extent we could use just standard off-the-shelf algorithms standard offtheshelf usually off policy algorithms for learning general value functions like GTD. And emphatic TDD. And retrace ABQ uh these are prediction learning methods for generic uh GVFs. And so we can use that to learn the main problem how to get reward we can learn that for learning diagrams for the sub problems. We can find the transition models of the options with these methods. And the planning can also be done with standard algorithms applicable to all GDFs. And this enables us to say that anything that can be learned can also be planned. That's I just wanted to get to that slogan for you because it's it's a it's a good one. It's a it's a um it's a bit advanced. But almost it's almost a next step.\n\nOkay,.\n\nSo we got those things. And now the other big step. And uh I'm going to have to do it a little bit uh not in full detail,. But we have to talk about how the planning works.\n\nOkay, how does the planning going to work?. And I'm going to give that a green check mark because I think we do understand this.\n\nSo planning, why do we plan? Why do we want these jumpy uh temporally extended models of the world, the option models?. And we we. But basically why do we want to plan at all? We want to plan because the world changed. And the correct values change. And it's easier in many cases not in every case. But in many cases it's easier to get the model of the world right than to get the values right.\n\nSo you get the model right. And then you do the planning uh to make the values consistent with your model. Um. And uh uh.\n\nSo in this big world setting it's it's the world changes or appears to change. Uh most of the world's dynamics or in many cases the world's dynamics including the reward parts don't really change. But the values nevertheless change. Like it's always true that I can walk over there. And find the restroom. But it's not always true that I want to go to the restroom. Um it's not always true that I want to get coffee. It's not always true that I want to go to the library all the things or go to Edmonton.\n\nSo u to prepare for these later wants um these these different values you plan. And u this also has some implications for which sub problem is useful.\n\nOkay.\n\nNow how does planning work? Um I like to think that planning is is by approximations to value iteration.\n\nSo,.\n\nSo this this equation is value iteration. You may already know it,. But I think maybe you should look first um here. What is the model? Model is something that that takes a state a low-level model takes a state. And an action gives you um a probability distribution over next states. And the expected reward along the way. And so value iteration. Then says I'm trying to improve the values of some states. I look at the possible actions. And I'm going to maximize over them. I'm going to look at the immediate reward. And I'm going to discount. And then take the probability or the really the expected value of of the value of the next state.\n\nSo this is the probability of each next state. You wait by that you take the value of the next state.\n\nSo you know you probably are are familiar value iteration works like that. Um. And really all all planning methods in some sense work like this just apply to different states in in the search tree. And you know order which states are updated in such a way uh has is is is varied.\n\nOkay. And um the interesting thing about planning with option models is that it's the same. It's really it's really the same. Although life is lived one step at a time, we have to plan it at a higher level.\n\nSo our knowledge of the world should be about the large scale dynamics. It should not be conditional on single actions. But on a sustained way of acting that is on an option.\n\nSo if we look at the conventional model it's we receives an action an option model you receive an option still you get a probability distribution perhaps over the next states. And expected reward not not a onestep reward. But some reward while you're following the option. And then. Then value duration is almost unchanged. We just change the actions into options. And we still talk about the reward for following that option. And the probability of each next state under the option.\n\nOkay,.\n\nSo that's good. Um that's how that gives you a flavor of how planning would be done. You you would you would you you basically you say oh here's some state. What are the things I could do there? What's the best I could do? I'll update my value. And here's I could imagine another state. Maybe it's the state I'm in. Maybe it's not the state I'm in. But I go through this outer loop of of considering various states. And then maxing over the possibilities. And uh doing this equation. But I'm sure you are concerned because I've been talking about via s which is the value of an individual state. And uh we can't do that of course we have to have function approximation.\n\nSo I don't know I don't think it's that helpful to go through these equations. But yeah the value V of S will become the approximate value of a state given a parameter vector W. And also your model of the world will become R hat. And P hat they also become parametric. And then after you've done that um things are much the same there is more complications which I will skip having to do with what's computationally expensive. You can ask me about that if you want. Um, if you're interested, I'll say some summary things. Um, so,.\n\nSo we we're maybe we're almost done,. But remember I said we would come back to some things.\n\nSo, I said we would come back to um the first two learning how we're going to learn these things. And what's we the problems we have.\n\nI want to say explicitly what what the the situation is there.\n\nSo two more slides. Um.\n\nSo the oak architecture requires reliable continual learning, continual deep learning. And so this is one of those things that I said this would be we'd be all done with step one uh if we uh could do continual learning. And but we can't do we can't can we do this yet? Okay. Can we do this? Okay. Do we have reliable continual learning? Well, we do have reliable continual learning for the linear case, for the tabular case,. But for the nonlinear case, for the deep learning case, we can't have reliable we don't yet. Do we? No, we sort of do. We have uh we have we have these catastrophic failures like u like uh catastrophic forgetting. And the catastrophic loss of plasticity uh that have been figured out long ago. And also very recently. Um.\n\nSo we have these catastrophic problems. But we also looks like there's a range of solution methods.\n\nSo I guess this is an area that's that's right.\n\nNow in flux. And uh people are figuring things out. And you know we're not there I can't give it a green check okay. But um. But there are lots of ideas continual backrop is one of them the metalarning of new features I think can also help. And related to that it's the other the other problematic uh or the other step that I wanted to talk about where we having to do with um creating generating new state features. This is this is also a super old problems going back to the 1960s like Minsky. And Selfridge would talk about this. They would talk about representation learning. They would talk about the new terms problem. And anyway I like to talk about metalarning nowadays.\n\nSo backrop back in ' 86 was supposed to solve this. We were supposed to you know learning representations by gradient descent. Um. But it really just doesn't. And um I think we're we we accept we recognize that unless we are still in love. And think that gradient descent is enough for everything. Um most of the other methods other than gradient descent are based on generate. And test ideas where you like generate a bunch of features. And then you test them to see if they're useful. And so you could generate them randomly. And you could test by their utility. And u continual backdrop is an instance of that. It also has a real old history. Leslie Keelbing did her uh did some of this work in her PhD thesis in 1993. Uh Rupam Mahmud. And I did some work a decade ago. Um there's lots of ideas. But not yet a specific proposal for a whole network based on grad descent or any other uh there aren't I anyway don't have a specific proposal uh for solve this problem. I think it's a really really important problem. And I think it's like maybe it will be worked out in the next couple of years. And then it will literally um take over everything that people have done with with deep learning. If we had a deep learning method that was can do everything we're doing.\n\nNow but can also learn continually that would just um be a really big thing. And I think there's no reason why it couldn't happen. Um. And so I think it will I also think that something like uh my algorithm called IDBID or IDBD. And it's really old will be a key part of that uh solving this problem.\n\nOkay. Um I'm sort of done. This this figure was just to um remind you one more time of the cyclical nature how we have state features that produce sub problems that are solved produce options that are that are used to form models. And and this is not doesn't look like a cycle. But remembering that um there's feedback being sent from each user of the features uh information information on which features are are useful which ones are that informs the features. And so it is in fact a cycle. Um.\n\nSo in my the quest uh have we uh succeeded we have something that's doesn't it's totally domain general there's nothing in it specific to any world it's totally experential. And has the claim or the hope that it will be uh you know able to find unlimited open-ended abstractions uh limited only by the computational resources. And so we might ask you know what do you what what should one think about this. And so I think arguably reinforce Enforcement learning. And oak offer the first plausible mechanistic answer to several important questions. How can high level knowledge be learned from low-level experience? Where do concepts come from? How do we reason? What is reason? Perhaps reason is just planning in this way. What is the purpose of play to find these these uh subpros which structure our our cognitive um our cognition?. And what is the purpose of perception? We're answering the question of how we perception can operate without reference to a a human label or to an external world. Perception um can be concepts that have been formed to solve problems that are the basis of uh subpros. And if you know about cognitive uh David Maher,. Then we would say that oak is a computational theory of intelligence.\n\nOkay.\n\nNow, what about someone like yourself, a reinforcement learning AI scientist? I I would hope that you would think that Oak provides a way to think about the parts of AI AI. And their interaction. And this can guide future research. It's a vision for how to do planning with a learned model which is a key missing ability for today's AIS. Uh it offers a view of perception that's grounded in experience rather than in human labels. It offers incomplete admittedly incomplete. But schematic answers to the discovery problem. Where do the subpros, options. And features come from?.\n\nSo it's a vision of how we can obtain an open-ended super intelligence entirely grown from experience. Uh even if it's not yet fully specified. And e even if there are things that I'm saying we don't really know how to do. But we should know how to do such as solve continual learning. And metalarning um it's a vision of how to grow a super intelligence from experience at runtime his most important capabilities. And does it act learn plan model learning sub problems the options with um all the rest. And the discovery of of the state features. And thereby of the problems, options. And models leading into this virtuous open-ended cycle of discovery. And is completely general. And is thus scalable. And a potentially lasting --- *Converted using Multi-Format Converter (Tank Building Stage 2)*