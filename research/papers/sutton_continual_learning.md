---

# ELIAS ULM Learning Content
ulm_metadata:
  source_file: "/Users/mikesimka/elias_garden_elixir/apps/mfc/learning_sandbox/ulm_inbox/Sutton_continual_learning.rtf"
  content_type: "paper"
  language: "en"
  converted_date: "2025-08-31T01:16:29.399562Z"
  converter_version: "mfc-1.0"
  ready_for_learning: true
---
# Converted Document **Source Format:** unknown **Extraction Method:** native_elixir_rtf_parser **Extracted At:** 2025-08-31 01:16:29.396510Z --- [Music] hello everyone hello sorry the mics don't work too. Well I think for here okay no work fine.\n\nSo we're gonna get started thank you very much for coming welcome these are the Aral seminars which uh today are as always sponsored by uh instadp uh Google Deep Mind. And ionic. And this is a very special seminar uh for us because this was a seminar series that pop up from a reinforcement learning reading group. And as a bunch of reinforcement learning researchers uh we all in the group uh initiated our adventures with a very nice book from the person who is.\n\nNow presenting today.\n\nSo I think there is no need of any presentation we're very happy to have today with us Professor Richard salon who is going to be talking about Dynamic deep learning Richard thank you [Applause] [Music] thank you Bo it's really a pleasure to to be here today. And have a chance to talk to you share some ideas with you um I was I've never been to Imperial before. And uh I've just learned about all kinds of interesting things going on here it's all very exciting. And uh it's uh impressive in many ways.\n\nSo today uh let's start with some general remarks before I get into really what I'm going to cover today um AI artifici intell is really ambitious.\n\nSo what I would say I would say that AI researchers seek to understand intelligence. Well enough to create beings of Greater intelligence than current humans pretty awesome pretty awesome goal uh I think reaching this profound intellectual Milestone will enrich our economies challenge our societal institutions it's going to be unprecedent it's going to be transformational. But also a continuation of trends that are thousands of years old people have always created tools technology. And Been Changed by it um this is what we do it's what humans do the next big step is to understand ourselves this is a quest Grand. And glorious. And essentially human yeah of course it's also totally hyped up uh. But I don't think I'm hyping it here the things I'm saying here are just true this is a this is a grand Quest um. But it is hyped up it's it's not a good climate for science you know science likes Things To Be steady. And calm. And focus on the important fundamental issues whereas AI is like it's all there's a lot of money going into it there is.\n\nSo many companies uh there is people you know arguing that what I'm doing is is enough you don't need to do anything any else um. And uh it's it's not it doesn't feel I don't know if you guys sense it. But but I sense that it's not a really good climate for science.\n\nSo for example large language models really hop into the thing really important thing really powerful thing. But they can't just accept that they're going to be really important. And really valuable they have to say oh this is all that the Mind does um the mind they have to claim the largest language models can do reasoning. And and maybe they're conscious. And all kinds of crazy stuff instead of you know because the real scientific attitude you find out you figure out what you can do. And try to understand what you can't do. And uh. And quietly make progress I think we're not really that's not really the the the culture that I feel in our fi at the moment.\n\nSo I think I just a little bit disappointed by that. But I don't doubt this that it's an important question that we're trying to address okay.\n\nNow some others of my perspective.\n\nSo I'm one of these guys that are trying to understand. And create maximally intelligent agents. And intelligent an agent is defined to be intelligent to the extent that's able to predict. And control its input stream particularly its reward this is the way I would Define intelligence not the way everyone would find intelligence not to pick on the large language models. But they don't actually try to predict control their input stream Technically when they're running uh they don't even have an input stream um.\n\nSo we want to control we want to affect our input. And uh the creation of these super intelligent agents. And super intelligent augmented humans I think will be an unallied good for the world I'm not going to say that the world is going to be an unallied good thoroughly good. But because the feature I think is going to be tough because we're in a what's called a fourth turn turning if you don't know what that is you might check it out sometime it's kind of an interesting set of uh ways of viewing societal change um. Well the path to intelligent agents runs through reinforcement learning.\n\nNow through the lmm um. But the current biggest current bottleneck to ambitious reinforcement learning based AI is that our deep learning methods that we rely on are inadequate to the task they're not. Well suited to the task. And that's what I'm going to talk about today that they're they fail when we ask them to be we put them in a dynamic situation where they have to continue to learn as you know I think all deep learning methods uh when when they're deployed when they're inter when they're acting on a robot or or um interacting with humans they they no longer learn they learn only in advance they are sort of transient Learning Systems.\n\nNow one more sort of introductory slide just to tell you where I am where my thoughts are. And I I just put this in like like two minutes ago actually um this is the standard architecture of reinforcement learning. And if you see all these components of a modelbased reinforcement learning agent that's what MBR is model reinforcement learning agent takes in observations adits actions. And it also takes in a reward um. And it basically composes of four parts perception takes in the Stream of events. And admits a representation of of the current situation or the state probably it's a feature vector. And that passed to the policy reactive policy to pick an action. And those two right in the middle those two these two perception an action uh those two together form a complete agent. But if you want the agent to be adaptive. And if we want it to be intelligent it's going to be have to be adaptive as I have defined it. Then you're going to need a value function to say how. Well things are going. And then use the the TD from the value function to adjust the policy.\n\nSo this diagonal line through the policy if you can see that represents um not an informational transfer like the state comes into the policy adits the action. But the diagonal line means you're changing the function implemented by the Box okay.\n\nSo value function changes the policy.\n\nNow at the same time the full agent a full modelbased agent you also learn a transition model which is a model of the world uh. And its state transitions observing the states here here you see this the state at a particular point in time. And you might uh notice the state at a particular point in time. And then you take an action. And then you find yourself in a new state after amount of time. And that's a transition. And your model transition model would model those transitions by observing the data. And uh. Then you can ask questions of your model what if I did this action instead what would happen. Then oh you to this state. Then you could apply the value function to that state. And you get a TD error. And then you could use that to adjust the policy again. And we call that planning because whenever you're using a model. And not experience to adjust your anything we call it plan just these just definitions of how I'm going going to use the words.\n\nNow in the full um architecture we also we have not just one policy. And one body function. But a whole set of them. And that's meant to be suggested by these these that are stacked up behind the the true policy. And and stacked up behind the value function we more more value function.\n\nSo that's where you would posee subtasks for yourself. And you learn skills how to achieve achieve other things other than the uh reward while always being cognizant of the effect of those other things on the reward okay this is just on the side right I hope it's interesting. But it's on the side. And um a lot of this is developed in these papers you might want to it could be checked out um okay.\n\nNow let's get to today today we're going to talk about Dynamic deep learning. But really it's about uh uh paper that my colleagues uh. And I wrote. And the title is helpful for clarifying the topic the topic is loss of plasticity. And deep continual learning. And I'll Define all those things in a minute. But now is my chance to talk about me a little bit more um I acknowledge I have many jobs keing Technologies the University of Alberta Amy the Alberta machine intelligence Institute my own reinforcement learning lab. And open mind research I want to tell you about this a little bit this is our our logo open mind is a a new Institute that we've created um that's um devoted to doing fundamental research in along the lines of reinforcing the Alberta plan um for people who have already graduated obtained their degree uh. But want to uh do research in this area. And it's distributed it's we have kind of a focus in Alberta. But it's distributed. And people can be all over the world. And uh if you might want to do fundamental research in this area. And you uh want to focus on that rather than become a professor um you uh may want to apply to be a fellow of the our Institute okay.\n\nNow here's my main message main message is that deep learn learning doesn't work for continual learning um. And by not work I mean that learning slows. And eventually goes to a very low level of learning we' it's lost you've lost plasticity. And by Deep learning I mean this all the standard uh artificial neural network methods specialize as they are for this non-c continual or transient learning. And I also mean without replay buffers because I consider themselves an acknowledgement that deep learning doesn't work.\n\nNow uh Better Learning algorithms that are specialized or. Well suited to continual learning they're not hard to find. And they can apply to deep learning uh.\n\nSo I don't think there's anything wrong with artificial neural networks we just need to find better algorithms. And we only need to to find we have to start looking for them. And uh.\n\nSo that's my main message. And then this work um details. And substantiates this claim okay.\n\nSo I'm going to have demonstrations of loss of plasticity. And deep learning um. And this. And and. And the attempts to maintain it. And this will be in classic supervisor learning problems like imet. And CFR 100 with residual networks um. And also in reinforcement learning. But primarily in these classic supervised learning problem domains you know.\n\nNow already that I'm not really a supervised learning guy I'm aiming at reinforcement. But I need all those four boxes that I showed you they all need to learn continually because you live in your world. And you keep adapting to whatever it throws at you. And you you have to model the world. And the world changes the world is infinitely complex you'll never be able to anticipate all the ways that might change. And so you have to continually learn. And then hopefully I'll have just a a little bit on some goals. And ideas for Next Generation of deep learning that would be. Well suited for continual learning settings.\n\nNow this work has just been published in nature.\n\nSo I got I'm really excited about that the first time ever been published in in nature. And I just. But I want you to realize that that like uh even when you have a publication success it's often proceeded by a large time where you have publication failure uh.\n\nSo this happens all the time it's just if something is fashionable you can get it published in nature right if it's not fashionable you can't get it published at all sometimes I I've experienced this many times all my best Publications were were really hard to get published you know yeah temporal difference learning when I published that really hard options all all almost almost it's almost always true that the more novel. And and. And groundbreaking your results are the harder it will be to get them published.\n\nSo if you're having trouble you know take heart a little bit. And realize you'll have to persevere. And if you persevere you may eventually get published. But it's not guaranteed it may never become fashionable um.\n\nSo we we in particular tried to publish this over like four. And a half years. And we're unsuccessful you can see it in archive archive is good um plasticity just means the uh the ability to learn. And so loss of plasticity just means means loss of the ability to learn or not being able to learn continually not continual learning hope you're getting comfortable with the way I'm using these words maintaining plasticity is maintaining the ability to learn. And so in AI since we want all the four boxes to learn we should prioritize maintaining plasticity. And we're not the first to see this problem see at least hints of the problem uh it's very closely related to the idea of catastrophic forgetting where in deep learning you learn some things. And you learn some more things you often totally forget the old ones.\n\nNow loss clity you learn some things you learn some new things maybe you learn some new things after that. And eventually you can't learn any more new things not the same as loss of as catastrophic forgetting it's sort of cat catastrophic loss of plasticity.\n\nSo this there are also hints of this in the early neural networks work in the sitech literature. And there a few works like uh Ashen Adams that showed the failure of warm starting warm starting was like you thought oh I'll teach this this neural network on on part of the data. And that'll warm it up.\n\nSo that it'll you I give it the second half of the data it will be faster. But the fact they found the opposite if they trained it on first on the first bit. Then it was slower on the second bit. And and they did much better if they just put the two parts into one big thing. And trained it all once from scratch was kind of surprise it was surprising seen as a failure um that was one of the earliest demonstrations um. And Primacy bias Nic Ain capacity loss CLA ly. And others these were shown in in reinforcement learning okay. But no one really did a thorough demonstration in supervised learning uh of this phenomenon. And and because you have to do it thorough it's a Nega sort of a negative result you if you get hints it doesn't change people's minds. And so the significance of this piece of work was that we did it really thoroughly. And and dotted all the eyes. And crossed all the tees. And and considered all the possible ways you can get around it. And and uh have have something that's pretty incontrovertible okay.\n\nSo let me show you the first one of these demonstrations law of plasticity in supervised learning. And we're going to use the classic uh domain of imag net um.\n\nSo image n you have this database of millions of images. And there like this is an image this is a this is a a shark hope you hope that's obvious to you this is somewhat down sample. And we have to use somewh down sample images uh. But there a thousand glasses each with 700 or more images. And it's widely used um. But we need to change it because it it's why Lees as a pure supervised learning problem like they take all data they they they they they train on all the data at once. And and. Then they gradually freeze it they train. And train. And train multiple presentations. And then finally they freeze it. And actually they gradually freeze it. And slow it slow down learning until they have a nice stable State we don't want to do that we want to have something that we continue to learn.\n\nSo how are we going to do that.\n\nSo we're going to make a A variation of the problem which called continual imet.\n\nSo and we see it as a minimal change to the classic one.\n\nSo we do the usual thing we take there's 700 examples remember for each class.\n\nSo we separate that. And just training 600 training. And 100 test ones. And then we take them in pairs.\n\nSo like this might be the first pair we'll show you crocodiles versus guitars. And you have to learn to distinguish those two okay. And then when you're doing. Well we've seen we've seen all the examples of those pairs we go on to a second pair like.\n\nSo think about how that that's done we we've uh the network only has two options two outlooks a or b or yes or no uh crocodile or guitar. And so. Then when you want to ask a new question you add. Well since I'm never going to ask you again about crocodiles. And guitars I can actually throw out the heads the final uh two neurons of the neural network. And and uh. And replace them with two new ones two new ones for the new task the new task is distinguish game controllers from fish okay. And then once you've learned that I I erase the heads. And give you a new problem both ties versus oxin I guess okay. And this.\n\nSo This continues. And you can do many pairs because remember there's a thousand pairs.\n\nSo you might think I could make 500 pairs. But you can actually make many more because you can reuse ones as long as you don't re reuse the pair.\n\nSo how many pairs of uh of thousands of things it's it's it's like it's like almost th squared.\n\nSo you have lots of pairs we can do this forever um performance measure is I'm just going to measure the percent that are correct on the test set remember I present all the examples for the two classes. And then I have some saved out ones I measure performance on them. And then I average this over many many runs. And in the many runs I vary the the pairings of the class I don't you don't have to worry that maybe the first class is hard. But the first class is easy since they're all averaged over all the possibilities okay that's sort of the problem. And then there's a whole another set of details for the Learning Network. And um talked about how the heads are reset the final heads uh the structure of the network is pretty standard it's a little bit narrow because we only ask you to classify two things at once we we don't want it uh to be infinite in its size compared to the things we ask of it um.\n\nSo it has some structure of convolution layers um. And interconnected layers. And notice the last one is just two those are the last two heads okay. And then we use batches we use epics. And all the usual ways um.\n\nNow it's important that the weights are initialized in the standard way. And that it's only done once it's only for the very the very beginning uh on the first tab before the first task we we randomize all the weights small initial values this is how this is a standard practice uh. But it's only done once you know when you get a new start to get new data from the new task you're not told it's a new task. And so there's nothing no opportunity to reinitialize or do anything. And you maybe you don't want to reinitialize think about it maybe you've learned one thing. And now you're going to learn something new maybe what you learned on the first one will maybe the features you learned on the first one will help you um get those last St heads the for the new task um they maybe they'll be able to much better because they have better features okay.\n\nSo we start with just plain backdrop momentum cross entry loss Rel activations the usual things. And we of course varied all these things. But the the standard case is is this one. And yeah many variations get representative results. And then we I want to ask you how do you think what what do you think will happen over over the sequence of tasks will performance be better on the first task or the second task were you voting for first Al I think it'll improve andur you think it will improve. And since that's that would make sense that it should maybe the second task will be better third task maybe will be better at some point you're not get any more Advantage from Having learned good features that would be good that's the way a learning system should work okay okay uh this is the beginning I'm going to show you just the beginning first. And obviously it's going to depend it's going to depend on on your. Well let me do the axes first the axes are task number along the bottom the first pair the fifth pair um. And this is percent correct okay.\n\nSo if you. And these are. Well what are you tuned for this red line was tuned for doing as. Well as you could on the very first task.\n\nSo it's sort of fast right. And so that gets up to about 89% correct on the first task um this is slower the backrop alpha alpha is the step size.\n\nSo um this Red Alpha is is 10 times faster than the orange Alpha okay with orange Alpha you at least after a while maybe get up to a higher level. But uh this one's the fastest at the very beginning okay does this all make sense okay.\n\nNow the linear Baseline is. Well what if you didn't have all this network stuff you just took the the pixels. And you learned a linear map from the pixels to the class that's actually not you can do much better than than random what's random random is two classes.\n\nSo it's 5050 50% would be random um yeah. And I what else I the shade region is one standard error linear Baseline is importance of the L of the linear heads there's no nonlinearity there's there just a single. Well two units. And so this doesn't have to be run actually with with uh multiple tasks because you know there's nothing to save from one task to the next the two heads are replac the two linear heads are replaced that's the whole network.\n\nSo you don't have to run it many many times. Well you have to we to run it many times. And we establish that you could get 77% approximately correct with with you uh with a linear system okay.\n\nSo we don't that's that's really poor performance right that would be bad because you're not even using all that Network to do anything okay.\n\nSo maybe we're improving at the beginning this guy seems to be improving. But what happens in the long run okay that's the real question.\n\nSo this is what happens in the long run. And so these are this red. And the. And the orange are the same curves you saw previously just extended.\n\nSo let's go with the red one which started out at 89% which is this point right. But this first data point is an a is a bin an average over 50 Ex because there's yeah just for graphing purposes to get rid of all the jaggies it's good to average over the first 50 tasks. And so over that first 50 tasks you really can only do about 84% correct because you're already starting to decrease in performance. And and. Then you see that playing out uh as you go more. And more tasks you get worse. And worse until you're performing worse than just a linear Network. And if you use a different set of parameters uh if you're slower you also get worse. And worse. And then level out at just a little bit better than the linear Network this is an even slower step size. And it also does poorly okay.\n\nSo this is this is this is one uh setup. But it's representative we've seen this pattern over. And over again uh if you vary the details you can get small variations in different different shapes. But um this is this is this is the pattern that you always that you will always get um. And in the long run you can't do better than the much significantly better much better than the linear Baseline. And also if you introduce variations like bring in atom atom actually makes it worse Dropout makes it worse any of these things just makes performance worse um okay.\n\nSo what's the summing for good hyper parameters plus decreases across tasks nearing the performance of the one layer line your network or worse.\n\nSo this is what I'm calling catastrophic loss of plasticity question have any explanation for why is that an al01 you keep seeing this city. But that smaller Alphas of level out. And retain some Bas um we haven't thought about arguing that the orange line is good you think the orange line is a failure uh um I've never asked the students who did this all the students I listed Shaban doare. And Fernando uh all these about that particular question I'm not sure I can answer that any other questions thank you how do we explain uh how the like the the the brown line is definitely increasing. And as we saw the orange line also if we looked at detail at the first 10 tasks it also was increasing.\n\nSo like uh that's that's what Alex called for that we should get some improvement in the beginning. And so that makes sense to Alex. And the The Way We Were understanding it was that it was you learn features during the early task. And those features are later are are useful on on the subsequent tasks uh. And if you're particularly if you're learning slowly. Then then. Then uh. Well yeah empirically we're seeing that if you learn slowly. Then you're getting a much greater effect of that nature you're getting savings across the tasks what you learn maybe you maybe takes a while to find good features. And if you can only learn really slowly um that you get more accumulation over over over over uh over tasks okay um.\n\nSo there are better algorithms as I said things like Dropout. And standard variations make things worse um uh. But there are a few this is this red line is the same line we saw before we've just rescaled it you know he's going way down low uh. But but if you have a good algorithm you can you can show Improvement.\n\nSo um this is shrink. And perturb. And L2 regularization L2 regularization just means you something you add something that encourages the weights from getting too big prefer small weights. And so this uh keeps them in a in a in a a labile range.\n\nSo they can keep training keep learning from more experience um. And I I'll show you some details on that in a minute shrinking perturb does L2 regularization. But it also does a random perturbation of the out of the of the weights. And so these two are good we're going to repeatably see these are good at partially solving the problem. And continual backrop is our own algorithm um. And it's basically just like backrop it's falling gradi descent. But in addition uh we select just a very small proportion of the units. And we the very the proportion that are least useful to the network. And we reinitialize them.\n\nSo like less than one unit per example very small fraction of them. And we have a utility measure.\n\nSo we can measure their util how much they're contributing to the Network's uh behavior. And if the ones that are used least uh will be candidates for being reinitialized. And that small change uh solves this problem you have a question online about how would you select yes it's a I'll talk about that in a minute let me let me just postpone that um. But it is it's a measure of its utility. And as you'll see many of the units are not useful at all this is what this is a phenomenon of backrop okay okay. And maybe I have it here continual back propop scas grading descent with Selective reinitialization just like backrop except reinitialization this this are only parameter with the raid which we reinitialized I said it's very small very slow um that's what that is a hyperparameter. And it's it's it's we show that it's better to reinitialize selectively using a measure of utility.\n\nSo for example shrink. And perb also random makes random changes um. But it doesn't do it selectively it's always just taking all the weights. And wiggling around a little bit. And this is an old idea uh it's really sort of a kind of generated in test we we're keeping the units that are contributing to the network. And we are taking ones that are not contributing. And we're going to randomly vary them um. And this is this this idea existed before uh rupan mmud. And I did some work on it it goes back to the uh 60s like Oliver Selfridge did pandemonium. And it also was like this generating. And units in a network. And but the thing about continual backdrop is it extends the idea to General multi-layer networks okay.\n\nSo um I I want to show you in some sense convolution networks are old school old style uh.\n\nNow we want to do residual networks which also has many layers like 18 layers. But we also have has cut shortcut connections or cut through connections.\n\nSo you don't only it's not strictly layered anymore this is more more modern architecture. And we're also going to change the problem a little bit we're going to use uh CFR 100. And we're going to present five classes five classes. And then we're going to progressively add more.\n\nSo we're not going to like like before we had pairs another pair a different pair just in different pairs same number.\n\nNow we're going to add we start with five. Then we'll add five more.\n\nSo we'll have 10.\n\nSo so this is the the old five we're still going to present those yeah. And we're also going to present the new five. And the next we'll present the old this 10 will keep presenting all that 10. And we'll get five more.\n\nSo there'll be 15. And we just keep going until you get all 100 of them okay.\n\nNow if you think about that the second task will be harder than the first one right the first one just has five classes this one has 10 classes next one have 15 classes.\n\nSo you expect overall accuracy to go down over time. And see can't measure overall accuracy if you want to see a loss of performance what we want we want what we use is we show performance relative to a network that's trained from scratch with the same number of classes.\n\nSo say you had 50 classes you could you could just reinitialize your network give it 50 in the all way the 50 are all presented at once. And that'll perform a certain. Well and. Then you could do it in this increment mental way you get 5 10 15 20.\n\nSo farth. And and. And what what what will it be will be will it be uh after you get to the 50 will you be performing will you be performing worse or or better than if you see them all all all all at once does the progressivity help you or hurt you okay. And what we find is quite.\n\nSo this the y- axis here is the difference between these two did you do better or worse because you're presenting them incrementally got it okay.\n\nSo what we find is that yeah there's definite effect of of you do better uh by doing it incrementally at the beginning you you you gain a couple percentage points of accuracy which is quite a significant effect. And then it decreases. And if you do usual back poop after about I don't know 40 45 uh it's neutral. And then. And then you get this loss of plasticity effect where you do much much worse than uh. And presenting them all together at once shrink of a turb isn't quite as bad. But it's bad you're you're losing you're losing plasticity. And continual backdrop you're able to maintain the same level you get exactly what Alex asked for get an improvement at the beginning. And then doesn't you plateau okay.\n\nSo that's that's the Z pattern yeah good because you you said you keep the same performance in the long run with Contin backr. But at least are you able to reach that performance sooner than with a Network that has been trained from a scratch with with all of them at once.\n\nSo is a learning time at least faster with continual back Pro than when you because we're trying to see the advantage right of just throwing all the data at once to a network learn all the methods are are getting an advantage in in learning time at the beginning right that's what we see yeah that's F. But at at the end at the end there there is. And then we're measuring performance only on on the full set yeah. But my question. Then is like since performance is the same did you notice that uh continual backrop was reaching that performance sooner than that Network learning from a scratch I don't think that I don't think there is any measure of of sooner um you mean once you're say presenting all 50 of them yeah I mean we.\n\nSo so in the Progressive case yeah you've been you've presenting you've been presenting them in smaller groups all along. And so for most of the classes by the time you get to all 50 of them you've already learned a lot that will to be faster. But I wanted to confirm if you. Well you'll you'll get you know 45 of them pretty pretty. Well at the beginning remember the performance measure is at the at the actually I didn't I didn't say what the performance measure exactly is here. And maybe I'm not sure. But I am sure for the previous example previous example. And I think we're doing the same sort of thing is we present things. And and. Then and. Then we have this test set.\n\nSo we we we finish learning. And then we measure performance there isn't there isn't a speed within within the group you know what I mean is asking is for like the number of classes 50 for example when you train the network from Ranch 50 classes you present as many examples to that Network as you present to the network that doing yeah definitely the same number of examples that's is slower it has seen 49 times the train set sence no no no the total is the same the total is the same yeah. Well the one that is trained yeah yeah is that right yeah question maybe I'm not sure. But this I think you'll find interesting here we're looking at we're looking inside the network trying to figure out what's happening okay.\n\nSo as we increase the number of classes this this this graph is showing the percentage of units that are inactive that are dormant or dead they're which means they're defining that as active less than 1% of the time. And so you start out with small random weights everyone's active oh I guess I guess active means um they're they're reu remember.\n\nSo if the activity is less than less than uh zero they're they're capped at zero B floored at zero. And so active means uh their their activity is greater than zero. And uh at the beginning they're all they're going to be greater than zero half the time. And so none are active uh less than 0% of the time. And then as as we as we train just for standard backdrop um yeah actually I should have said that in in this in this experiment if you do standard back poop you don't even you you you you don't perform it doesn't work at all you have to already use L2 regularization.\n\nSo so that's why in these experiments we don't show you results for L2 regularization because everything is using L2 regularization that's that's needed okay.\n\nSo that anyway that based learning base Learning System um gains up or as increasing numbers of of dormant units more than more than half by the end of 100 tasks um 100 classes 100 classes. And um shrink. And perturb does much better in terms of many fewer dormant units. And continual backrop has hardly any. And we also measured this the diversity of the representation uh uh which means often often you have fures that that become similar to one another. And they they don't add anything to the uh linear rank of the uh the you have you measure the the number of uh I don't even know how to say it uh number of independent components of in in the uh in the in the features in in the network um. And so we scale this between zero. And one. And we we can see quite quite clearly that just based Learning System uh the uh we lose diversity we lose rank of the representation. And with some of the other methods we can U preserve diversity okay.\n\nNow I tried to say here's here's where I emphasize the robustness. And in these problems. And we've done other problems we've done idealized problems we've done amnest. And we're shortly I'll be doing reinforcement learning.\n\nSo across network architectures uh across activation functions not just just ru. But a whole variety of them. And across the hyper parameters we see that just plain the Deep supervised learning loses P SE dramatically. And continual setting L2 regularization improves it. But just a little bit uh or make. And and shrink. And perturb often will help a little bit further weight with the weight randomizing continual back propop does the best in maintaining the plasticity it has one hyperparameter. But as long as it's uh it's it's we're in sensitive to that it's just can be set very small.\n\nSo now let's go on. And do finally reinforcement learning.\n\nSo we're going to do ant Locomotion um maybe you've seen this problem we have this ant uh I thought I have some videos here let me scoop ahead yeah some videos.\n\nSo here's the ant on the right side um moving rapidly forward the object is to move forward as rapidly as you can this is an ant that's not moving forward at all it's doing very poorly this one's doing very. Well and we're doing this we measure the the the reward is forward motion. And the uh actions is to control these eight joints where they are red marks. And this is a standard task. And uh here the performance we're showing the time steps we're training for rather a long time often you will only change train for a million or two time steps we're going to go 20 million. And we're going to measure the rewards per episode which is one trip of running forward as fast as you can. Then you're brought back to the beginning to to run some more. And here we have a non-stationary problem. And that means that um we're going to vary the friction between the feet. And the ground.\n\nSo every two million steps we're going to vary friction we're vary it don't I have a I don't have a figure for that. But sometimes it's fast sometimes it's the feet are sticky. And sometimes they're slippery. And um.\n\nSo it changes the nature of the problem every time you you uh vary the friction with the ground.\n\nSo these um sort of actually I call this an alligator graph because it looks like the back of an alligator's tail you uh you're doing. Well and. Then there's a switch in the problem. And you your performance Falls. And then it recovers s. And recovers. And um.\n\nSo let's first look at the standard reinforcement learning algorithm po um. And uh it it does. Well at the beginning. But then at a certain point it plateaus. And if you just keep training keep training um it degenerates. And your ant that used to look like like this one oh think I must have skipped the slide yeah this was supposed to show the uh the slipperiness uh we're going to vary that. And um. And uh here's our standard poo performing. Well but if we just keep going it starts to end up looking like that second one starts looking like this guy. And uh performance is actually uh deten.\n\nSo it's worse than how you started. And you can.\n\nSo this is this.\n\nSo people have seen this before. But you know when this happens they tend to just uh stop they they stop training early on. And if you stop training you can you can avoid this this this this fall. But the real Learning System would be faced with that okay this Ali online asking could a similar effort be seen if Mass was changed instead of friction excuse me could a similar effect be seen if Mass was changed instead of friction yeah we believe.\n\nSo have every reason to believe.\n\nSo as long as there's something. Well actually I guess I should move a little more quickly because I'll answer that in the next in the next slide um. But yes the quick answer is yes okay.\n\nNow if we tune the learning algorithm we can do better. But we still see a a drop uh a loss of plasticity over time. And as before L2 L2. And U continual backrop can largely solve the problem largely solve the problem.\n\nNow I want to show you um this one this is the same thing except there's no changing in friction there's just just regular ant okay. And um. But reinforcement learning does involve changes it always involves changes because the policy gradually changes. And um we it involves temporal difference learning temporal difference learning the targets are always changing.\n\nSo really even without changes even in a stationary problem you should get this we get this effect we we realize later we get the same effect. And performance just drops. And this is where I say. Well why didn't why doesn't everyone see this. Well they do see this. And they just they just stop training after a few million. And they don't they they cuz you arguably it's okay you're just interested in a good policy you can stop. And use that policy. But if you want to keep learning it's a big problem.\n\nNow if you tune your parameters you last longer. And you get higher. But still you you suffer. And you lose all your plasticity regularization uh can maintain it. And continual back bout can maintain it better okay.\n\nSo here.\n\nNow we'll take a closer look inside this algorithm. And again we look at the percentage of dormant units. And those are increasing to more than half L2. And continual backrop keeping it low stable rank stable rank is the same pattern. And here's a new one the average weight magnitude you just looking at the units. And look at how big the weights are. And with standard backrop the weights are getting bigger. And bigger. And bigger there's no there's nothing that that forces them to be small. And maybe that's the full explanation just the weights get bigger. And so they're harder to change um L2 regularization makes them forces them to be small. And maybe that will interfere with performance um. And so they give some insight into what's what's going on.\n\nSo conclusions deep learning networks are optimized for onetime learning in a sense they totally fail for continual learning. And these simple changes like continual backrop can make them effective for continual learning we going to rank units by the utility to the network preserves the most useful.\n\nSo I guess I never got into details of utility. And I guess that's because I don't think the specific way we rank them we don't think it's necessarily the very best uh um in fact that's one thing we're working on.\n\nNow how to make how to do it uh better in a better way um you know what if your what if your network is recurrent or what if you're right.\n\nNow we're using a fairly local measure we have the units each unit is looking at its outgoing weights. And if the the immediately outgoing weights if they're all zero Z for example you know you're not useful uh. But they may some of them may be large um. But you don't know that you're connecting to someone who's your large weight is to a unit that is useful which which would be necessary in order for you to be useful.\n\nSo um that we're still experimenting with that um I think there's a large exciting World ahead of deep learning networks can learn continually. And and particularly it opens up possibil new possibilities for reinforcement learning which is inherently continual. And modelbased reinforcement we have all these different components that that are interacting. And all of which are learning simultaneously. And continually okay.\n\nSo that's my completion of the demonstration part. And now let me just say a few I could I could say a few words about uh ways we're going to try to solve these problems further. But it's a good time for questions right.\n\nNow there are a few online actually yeah good.\n\nSo one very link to what you were presenting was what will happen for continual backprop without the L2 regularization. Well it is it is necessary uh in these later in later problems to make it work. Well so.\n\nSo it wasn't in the first problem um.\n\nSo I I don't know just doesn't perform as. Well in in some cases yeah kis who as. Well ask what kind of Advantage continual learning can gain over the normal deep learning. Well it's it's it's it's really a change in the problem right we're asking a continual problem where. Well I guess backrop uh uh reinforcement learning is always continual um let see I guess didn't I just show that didn't I just show that if you have a changing problem. And you need to do continual backrop or regular back poop totally okay let's just go to uh in Chris.\n\nSo just to wrap my head around a bit more. And just kind of talking back yeah their queries on efficiencies of like Baseline training say on like you know. And number of tasks versus like n minus 5 to end tasks um I understand that in the supervised setting the Advantage may be more for like fine tuning or something like this where you have like some existing model. And you want to addal tasks or something like this yeah.\n\nSo like if say we're doing large language models. And we we train them up. And then like another uh another week's information comes out on the internet new news stories. And You' like to have your large language model be up to date with the new news stories that. Then you it is that's a big problem nowadays what they actually do nowadays is when they they they update the miles infrequently because every time they update them they have to uh update they have to clear out all the whole network the old Network they clear it to scratch start over from small weights. And they train all the data at once they have to start all over again you'd like to be able to just add the new data. And like the reinforcement context seems like a lot more it's even it's even from like uh from initial training it seems like it's better because reinforcement learning. Well more continuous in nature.\n\nSo it seems like in the super bu setting fine tuning is like like sort of this key application because that way you don't have to waste old models whereas in reinforcement learning it's kind of a new paradigm training does that make sense got to say it more concisely sure.\n\nSo in the supervis setting it seems that fine-tuning sort of to me it's seems it's more of the main application because if it's both Cas you need continue learning really uh.\n\nSo just going back to their question if the uh if it's accuracy is the same as the Baseline uh with continual learning. And with you know that Baseline number of tasks what's the sort of notable Advantage unless you use like less compute in the continual learning setting number increase the number the other one is one for one fixed number sure life you will always have more numbers coming in.\n\nSo yeah. But if you're adding more numbers that's a fine tuning task right if you're adding new tasks later on after initially training a base model are you suggesting that fine tuning is an alternative to continual learning. Well no I'm just suggesting that for supervised learning it seems like this the continual back propagation seems like more of the uh the usefulness because if the if you're getting the same level of accuracy um getting the same as a as like you know a baseline number of tasks if you go back to the graph right when you have say 50 tasks right when you're comparing it to the continual back propagation the the accuracy is about the same which I showed you're not getting the same level performance um in in sort of the the prior chart right on the supervised supervised task that's sort of what it looks like this was just sort of like you know we'll talk later I'm obviously confused than you hello uh excuse me if this is a bit of a critical question. But in the in the evaluation of the where you add classes uh continuously um like you start out with five classes. And then add another five. And so on uh. And you said you mentioned that you you trained on uh equal amount of epoch I assume it could the deterioration of the normal back propagation not just be explained by the fact that it sees fewer examples of the new classes yeah I guess I I was UN unclear about that I. And and I I am unclear in my own mind exactly how that's done. But uh absolutely the the intent was to do a fair comparison neither one would have more data. And yeah I guess I'm not clear how that was done sorry about that no wor sorry what happened to the utility of the recently reinitialized weights you said you reinitialize them based on lowest utility the recently reinitialized ones jump in or or say low this one I can answer when they're reinitialized when I said they reinitialized in the standard way. But it's a little bit subtle just a tiny bit subtle um we reize the incoming weights to the new unit to be random in the in the ordinary way. But the outgoing weights are re initialized to zero because we don't want them to interfere with the Uno the current performance of the system if the outgoing weights were nonzero they would mess things up a little bit um. And then.\n\nSo then um this new this newly initialized unit what will its utility be its utility will be zero because it's hard going with all zero okay.\n\nSo if we really um ranked by strictly by utility when you add a new one that's the one you're G to throw away. And reinitialize the next time.\n\nSo we do need to to keep track of the of the age of a of a unit. And uh prevent a a newly created unit that has that's very very young from being uh placed good job okay um whether do you see um research in dip learning. And reinforcement learning going do you think the focus will be in modifying the backprop algorithm for the continual learning. And reinforcement learning setting like in this paper or do you see it going somewhere else. Well what do we consider it's continual back propop modifying the algorithm backrop algorithm you know it's it sounds like it is it's an adjective onto back propop yeah I think I think I think it it is it's the continual backrop is almost like uh uh more more backrop what is backrop backrop is you do initializing. And then you degrad in descent we're just going to do initializing all the time okay as you go along.\n\nSo it's is very much in the same Spirit um. And and. And and um maybe it's not a new algorithm. And what will happen in the future uh do we envision algorithms that are very different from from continual back propop or is that is that about it. And I think I think we need to go significantly Beyond backrop. And continual backrop uh for for the best um to to achieve all of our goals what we would like from continual learning as I want from continual learning I not only want this bare ability to not lose all your plasticity. But I also want to do things like um uh meta learn improve the way I generalize special thing about continual learning is you you learn some. And then you learn more. And then you learn more.\n\nSo you have this experience with learning. And you can see the way I learned at this time that worked out as.\n\nSo so.\n\nNow the way I learn this way it back a little bit better you can evaluate how. Well your learning has G on this something you can't do if you learn in one shot you learn in one shot just done. But if you learn. And then you learn more. And learn more. Then you can you can learn to learn say this way of learning works better than that way.\n\nSo if that seems too abstract for you let maybe concrete um uh gen how would you like to would you generalize more on this feature or generalize more on that feature we'd like to to learn that we'd like to learn how to generalize modern deep BL systems don't really learn how to generalize they are structured to generalize in a particular way. And maybe they were lucky. And they've generalized pretty pretty. Well or maybe that's the skill of their designers. But there definitely is no algorithm that's causing them to learn. Well and I think we want to have our algorithms learn to generalize. Well um. And so.\n\nSo that's what that's part of the ambition for continual learning it's not going to come just from tweaking back poop we have to add more things. And let me just use that we can still have questions. But let me uh say a few of these things about the new ideas uh we want methods that we'll learn continually of course we want we want to be able to learn nonlinear functions of course we want them to be very efficient. But the last point is we want them to meta learn to generalize better. And yeah. And this is the slide I want to do ideally a streaming deep learning or or uh what you call Dynamic deep learning should adapt to three levels you should adapt the weights that's the usual one you should also adapt the step sizes every weight should have its own Step At least every weight should have its own step size which a step size just it's sometimes called a learning rate. But it's it's a it's the amount by which you move each weight. And so you want you want the I I'm claiming we want the ability to to learn at different rates learn faster in some weights. And slower in some weights. And that this is is how we will get to sculpt our generalization is that confusing like if you have a feature that has uh a large weight. Then I mean a large step size. Then you will change its weight a lot when you when that feature is uh present. And that means you'll generalize a lot along that feature or the other way if the if that feature has a a zero step size or very small step size. Then you will not generalize based on that does that sound good does that sound bad do you don't generalize no in a continual learning in in an agent like like me or you there are lots of things that we know that we don't want to learn we don't want to change them anymore because they're just very reliable we've learned over large parts of our life we don't want to change those. And we don't want to change those because there are other things we do want to change we are we are actually very skilled at assigning credit to features that are most likely to be relevant. And one way to do that is by having different step sizes uh for different weights in principle I'm thinking in principle if we could do that we will get we'd get a whole lot of desirable things including generalization um okay. And then the third area we'd like to adapt is the connection between the units this is sort of a a a starting view of the goal for a better A Better Learning algorithm than regular back poop or continuing back poop okay there are lots of questions we we are running out like we're already above the time.\n\nSo true there's going to be an extended QA.\n\nNow we can keep on these questions if you guys want to ask. But I want to fre people who might have their commitments.\n\nSo quickly um.\n\nSo our next seminar will be Maxim you knew better the date was it 18 or when was that of November uh.\n\nSo we'll be with um G Kath head of Quant technology at Bloomberg uh from New York um we hope to see many of you there as. Well know that we also have a reading group if you want to uh dive more into especially in enforcement learning that's what we're focus on uh please come to say hi. And also one of sponsors ionic is um having some opening for stent internships.\n\nSo is on video games. And Ai. And enforcement learning stuff.\n\nSo if you're interested also come to speak to us. And that's all go. Well --- *Converted using Multi-Format Converter (Tank Building Stage 2)*