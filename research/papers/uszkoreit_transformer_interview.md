---

# ELIAS ULM Learning Content
ulm_metadata:
  source_file: "/Users/mikesimka/elias_garden_elixir/apps/mfc/learning_sandbox/ulm_inbox/jakob_uzkoreit_comp_history_interview.md.rtfd/TXT.rtf"
  content_type: "transcript"
  language: "en"
  converted_date: "2025-08-31T01:16:41.150058Z"
  converter_version: "mfc-1.0"
  ready_for_learning: true
---
# Converted Document **Source Format:** unknown **Extraction Method:** native_elixir_rtf_parser **Extracted At:** 2025-08-31 01:16:41.147594Z --- I am Hansen Sue. And the date is April 24th M. And can you introduce yourself I'm Jakob today I'm the CEO of inceptive. And before that I'm one of the co-authors of attention is all you need. And a bunch of other scientific Publications thank you.\n\nSo can you tell us how you got into the field of artificial intelligence in many ways that was actually something that I guess as a problem runs in the family.\n\nSo my dad is a computational linguist used to teach that. And computer science in Germany. And universities before that worked at SRI in Meno Park. And spent quite some time in Stanford. And so in a certain sense it's something that yeah started to enter my life over dinner conversations when I was a young child. And then later on I started to going to University in the early O's back in Germany again actually to a certain extent tried to avoid artificial intelligence SL machine learning certainly when it comes to Applications around language in order to you know at least stray a little bit from from the family business. And then in 2006 I stumbled Upon A Google internship not too far from here. And they offered me a few different research projects that I could join. And machine translation by far had the most interesting large scale machine learning problems. And so with some chrin I bit the bullet. And actually did end up in the family business of sorts interesting.\n\nSo when your father was working in computational linguistics what were the techniques. Then and how would you contrast you know the techniques that he was using versus the techniques that you were using.\n\nSo in those days in computational linguistics the techniques were still they were becoming more data driven there was more machine learning being applied at least in certain specific parts of larger systems. But there were still certainly heavily informed by. And often times really driven by linguistic insights in a certain sense in the broadest sense theory of language or theories of language that were used as again in the at least underpinnings in how those systems were designed. But often times really were used as the backbone of how they actually operated in stark contrast right what we started doing already in 2006. And then we 2004 2005 2006 at places like Google translate was much less informed by basically our conceptualized understanding of language or a conceptualized approximation of an understanding of language. And was trying to be more black box yet at the same time often still constrained by say combinatorial optimization Al algorithms that made assumptions strong assump iions about how often times the outputs of such systems such as Google translate would be assembled as functions of the input. And so for example at the time the leading approach was statistical phrase-based machine translation where you make the assumption that you can chop the input sentence or input string into phrases each of which have a small number of candidate translations. And then what you're actually doing is within a certain search cones.\n\nSo to speak you're allowing yourself to reorder those not embracing the full comunal explosion of all possible reorderings. But you're allowing you know limited amount of reordering. And choice among those phrases. But that means you're still making a strong assumption when it comes to basically what the how the equivalence can be expressed between two sentences in two different languages what happened. Then with basically the Renaissance or Resurgence of De learning. And applying that increasingly to language was really to in many ways reduce the reduce the assumptions or limit the assumptions that had to be made about any such correspondences to an absolute minimum. And treat really as a blackbox problem right.\n\nSo then the time that you were describing your father working on it what era was what years were those I mean those were basically 80s '90s. And then the O's. And the O's I guess was when really the transition happened from systems that were largely rule-based largely based on again conceptual conceptualized theories if you wish understandings of phenomena like language towards things that were at least where at least you had larger numbers of parameters that were estimated using machine learning methods. And where the constraints were still informed in a certain sense by linguistic understanding. But not anymore basically forming the actual backbone of those systems. But yeah those were basically those three decades where that shift happened or over which of the course of which that shift also happened.\n\nSo there's been over the three decades there's been a progression an increasing progression towards removing linguistic Knowledge from the machine learning systems removing it from the machine learning systems or maybe from the perspective of some of the practitioners at the time moving it into from the from the actual implementation. And the algorithms into constraints on statistical models that were. Then used to capture those phenomena. But I would say also in a certain sense weakened right.\n\nSo making fewer or B ing it less on ultimately theory if you wish yeah that's right okay.\n\nSo in.\n\nSo I read that like around 2012 one of the one of the.\n\nSo wait.\n\nSo when you first started in 2006 you were you immediately put on Google translate or yeah that's right okay yeah. And then by 2012 what were you working on at that point by 2012 I had actually recently transitioned out of Google translate. And into a new team that at the time internally was called aqua that we had dubbed Aqua didn't stand for anything in particular. But what it was really is the language understanding team of the product that later became known as the Google Assistant it wasn't called that yet neither internally nor externally. But had basically started working on systems that effectively by in the broadest sense semantically parsing queries would enable Google search especially on mobile especially in certain interfaces to use a if you wish deeper understanding. And more explicit formalization of an understanding of a query that may have been formulated more in natural language than you know in Google e as we would call in order to immediately give answers or execute actions that the user may have may have desired M. And was that motivated by comp competition that's right that's right.\n\nSo basically that was motivated ultimately by Apple Siri. And the the concerns that or the concern that Siri could change the way people expect to search. And and obtain answers. But also execute actions in particular through when dealing with their mobile devices MH. And what were the limits of the technology at the time I would say I mean compared to what we're able to do today it was incredibly limited. And one of the issues was that the statistical methods.\n\nSo we were still applying methods that were if few squinted very reminiscent of statistical machine translation systems. And what that implied was that we actually had to fabricate training data that was remarkably similar all the way to say what type of question was being asked or what precise action was was requested of a device to schedule a timer or an alarm or some such. And we had to gather quite a bit of training data from humans or fabricate training data using using humans that was remarkably similar to what we would actually expect to see in the query stream. And we were only to an extremely limited extent able to use very large amounts of language in different domains. And of different types in improving our understanding of queries. And you know from such a distribution. And so really we couldn't really bring to bear say you know the majority of the web or.\n\nSo right it basically was all purpose built manufactured in fact we used at the time large numbers of what I guess we would.\n\nNow or one would maybe call gig workers in order to to fabricate that training data because there basically was no such data there was no data in existence that was sufficiently similar to what we expected that query stream to look like. And so can you describe to a someone of a high school level what is an RNN. And an lstm. And explain the full acronym okay.\n\nSo rnns are recurrent neural networks lstms are long short-term memory recurrent neural networks basically what they do is they evolve in step that are taken in a recurrent fashion we'll get to that in a second they evolve a vector space representation of some something that is seen as a sequence of elements. And so let's say in language the sequence of elements could be sequences of words each of those words is represented by a vector at first it could be something like an indicator Vector that you know for any word in your dictionary has exactly one dimension. And there that Vector has the value one. And has zeros elsewhere. But then that Vector is mapped through a lookup table if you wish onto some Vector in some higher dimensional space.\n\nSo now we have a sequence not of words anymore. But vectors. And some higher dimensional space of real numbers. And this RNN.\n\nNow evolves a representation of that entire sequence or basically series of prefixes of that entire sequence by mapping the next or that the current Vector that it is the current word Vector that it is consuming through some mathematical mapping. And then merging that into an existing State vector. And so you have some State Vector that starts with some initialization initial value. And now in every step you basically take in a new word Vector in the case of a sequence of words you send that through some mapping. And then either in the course of doing that mapping or with a separate mapping you.\n\nNow merge that into basically added with some waiting into your current state. And then you proceed to the next word vector. And again add that into your current state. And you also at every given step potentially transform your state in a in a way that is independent of of the next word Vector.\n\nNow in lstms what you do is you actually lstms are a special version of all of these mappings basically the mappings that control how you integrate the next word Vector how you integrate the previous state Vector. And with some additional bells. And whistles around the ability to basically in a certain sense forget your previous State as a function of the new word to retain a certain amount of the previous state to ignore the current word as a function of your state. And as a function of the new word. And so on. And so forth. And so basically it allows you it allows the model how should I say a sharp control to a certain extent to learn to control sharply how much of these various state or new word vectors to use in the computation of the subsequent State Vector okay if that makes sense I'm not sure I fully got that. But that's okay. Well we'll move on okay.\n\nSo talk about what led to the invention of the Transformer. And what was the key Insight okay.\n\nSo ultimately the situation we found ourselves in at the time was that these recurrent neural networks that again coming back basically had to evolve some kind of state vector or some kind of state representation over the course of of you know some sequence of vectors such as vectors representing words that when training those models on very large amounts of data as you would find in some problems say in Google search you would actually never reach the performance of much simpler neural network models trained on the same data. And the reason was that these much simpler neural networks trained orders of magnitude faster. And so even though they were much less expressive say for example simplistic feedforward neural networks multi-layer perceptrons for instance that you could easily show were not as expressive as rnns or lstms because for example they couldn't actually iterate through these sequences they had to chop those sequences into engrams. And then map those onto vectors. And just add those basically forgetting about the global ordering of them. And so on. And so forth they couldn't be that expressive. But because they were training.\n\nSo much faster you could actually they would actually see much more orders of magnitude more training data in any given amount of time than say an RNN or an lstm. And would as a result still perform better in practice. Then these much more expressive models that we knew could actually handle language in a much more holistic manner or phenomena in language such as longdistance dependencies in a much more holistic. And much more appropriate Manner. And so at the end of the day. And this wasn't true for all problems at the time this was really pronounced only in cases where you really were able to generate massive amounts of training data such as for dealing with queries in the regular web search query stream of the regular web search query stream at Google. And it also it basically made it clear that if we could find a way of making such feed forward models train much faster on whatever Hardware we had we could potentially actually get the best of both worlds right if we could make them oh sorry if we if we I think that came out wrong I think if we found a way of making these feedforward models more expressive akin to rnms or lstms yet somehow retain their fast training speed that we would actually get the best of both worlds basically have something have models that could really be expressive enough to capture language as a phenomenon fully or holistically while also benefiting from these vast amounts of training data that we were able to manufacture. And one of the fundamental insights from a linguistic perspective or just from a statistical perspective this doesn't just apply to language this applies to many different signal types one could argue all signal types that we can make any sense of in fact one maybe philosophical Insight here is that this must be POS possible by exploiting the fact that there are hierarchical there is a hierarchical nature to at least our understanding of pretty much every stimulus that we get meaning we basically in language for example right if you in grade school or later High School you parse sentences right what you're really doing is you're expressing the fact that there is hierarchical substructure where you have individual phrases that basically where you can in a certain sense arrive at some understanding of in quotes most of their meaning just looking at that phrase. And then maybe you have a different another phrase in that in a sentence where the same is true. And then the third one. And now in another past you don't actually need to go through every single word anymore in sequence you can aggregate these what you've basically already in isolation determined as the meaning of these individual phrases. And put those together to the meaning of a bigger phrase. And so on. And so forth.\n\nNow why is that Insight interesting it's interesting because the hardware that we had at the time is actually still remarkably similar to the accelerator Hardware that's being used in deep learning today excels at parallel processing. And so if you're able to identify what are some of these chunks that you can in isolation make some sense of already without looking at everything. Then you've just enabled parallel processing.\n\nSo basically you've said okay fine. Then we look at this phrase this phrase this phrase in isolation get to some extent an understanding of what they mean some Vector space representation of what they mean. And then in the next step we don't only need to actually aggregate those basically representations. And so on. And so forth. And that's in direct contrast to what rnn's do right because they walk through the entire signal step by step one after the other. And they have to do that in order to. Then arrive at some state representation in the very end after having run through that entire signal. And so basically what motivated these. Then self attentional models was this Insight that by basically paying what looks like a performance penalty initially namely by having every piece of the signal interact. But in a very shallow way with every other piece of the signal that doing that would enable it to actually in a certain sense build soft tree structures over your entire signal that would. Then ultimately exploit the ability of these accelerators to do parallel Computing as opposed to sequential Computing.\n\nSo you basically parallel compute the interactions between all pieces of the signal or every piece of the signal with every other piece pairwise if you wish. And then you would just repeat that step. And it turns out that you don't need to repeat that nearly as many times as the signal is long. And that is ultimately what makes this type of parallel processing way faster Master on Hardware that is very good at performing these this large number of. But more simple kind of pairwise comparisons in contrast to you know a sequence length many or sequence length many harder operations if you wish that. Then tried to basically keep that Global State up to date at every given at every given step okay.\n\nSo it sounds like in computational terms you have you're going from a linear algorithm to something that logarithmic logarith exactly that's exactly right okay. But you can't directly you don't know the tree. And so as a result you can't directly. And this is what you know what if we knew the tree or if there was really truly a tree I don't believe there truly is a tree right because it's only approximately correct that these signals are Tre structured or truly hierarchical. But if we knew a tree. And if it existed we would just draw the tree. And then we would basically just aggregate the individual representations along that tree. And then we would actually end up at something that is logarithmic if we process it in parallel right because we only need logarithmic many steps in terms of tree depth in order to actually process the signal of a given length they don't know the tree. And so as a result how do you approximate the tree you don't know which basically branches let's assume it's a binary tree.\n\nSo at every given level two things are paired up we don't know what that tree looks like okay seems silly. But why don't we iterate over all possible pairings. And now this at first looks counterintuitive from a computational efficiency perspective. But if you make that very simple. And in case of these self attentional models what usually this operation looks like is just an inner product if you make that really simple. Then you can do many of these inner products in parallel there's no sequential dependency whatsoever that's what these accelerators love. And then effectively have iterated over all possible branchings at that level. And then you take another step. And you iterate over all possible branchings again. And again. And so basically for if you happen to have Hardware that looks like these accelerators like gpus tpus Etc. Then that is a much better allocation of compute than requiring it to be more complex basically matrix multiplication steps many few or.\n\nSo per step. But then times the length of the system overall length of the signal overall I see.\n\nSo what is self- attention. And how does it work self attention is a way of imbuing feed forward models that are that don't require recurrent steps along the length of a signal with a effectively Global receptive field with a way of looking at or of considering all possible pieces of an input signal at any given moment at any given step. And so the way in the Transformer. And in its predecessors decomposable tension model Etc that we published before the way that worked was by effectively doing pairwise comparisons exactly like I just explained right.\n\nSo instead of instead of aggregating a certain kind of state by doing a set of Matrix multiplications for each new element. And doing that in order you.\n\nNow basically process you compare every single pair of representations that gives you a waiting of for any given element of all other elements.\n\nSo if we want to go into more detail here what you do is you these pairwise comparisons give you scalers for each pair of representations in fact Maybe even for every given position they give you a scaler for every other position.\n\nNow you could normalize those into some something that sums up to one you can call that a distribution over all other positions. And now you can just add those up weighted by exactly that distribution. And so in a certain sense you can conceptualize this as for every given position in your signal let's say it's a word you're getting at every given step a waiting of all other words. And you could say even though that's taking it a little bit too far. But slightly oversimplifying you could say that tells you how important at this moment each of these other words is for determining the meaning of that word. Then you basically aggregate those in an operation that is totally simplistic you just add them up. And then you send that through one basically feed forward through a small MLP. But that small M MLP.\n\nNow operates only on this aggregation.\n\nSo you basically what's an MLP multi-layer perceptron.\n\nSo a very simple feed forward Network again no recurence you know no iterative process basically you just you have this very simplistic addition that is in a certain sense independent of it's not truly independent of the length of the signal right. But it's it's very simple you can apply it to very large signals. And then you just end up with one vector again. And that you send through through this simple multi layer perceptron in fact usually a pretty small one. And you do that again in parallel independently for each of your positions for each of your words.\n\nSo in every step what you do is for each word you attend to every other word meaning you get a distribution over all the other words weighted by that distribution you sum them all up.\n\nSo say if you want something that looks like a tree. Then your distribution is very pequ only on one other word. And it's almost zero for all the others right. And so that.\n\nNow means you're just getting the vector from just that one other word you add it to your own you send that through your feed forward Network through your multi-layer perceptron. And that. Then forms the representation for the word in question that we're talking about in the next layer. And that's the process that gets repeated layer after layer. But in every layer you do it not just for one word. But for all of them in parallel. But they're all independent right because you don't need to know the new representation of any other word in order to compute your new representation. And that is what again shapes the workload into something that is much more manageable. And much more efficient ultimately on today's very parallel accelerator H hardar okay.\n\nSo could you maybe Define because I'm not sure the audience understands the difference Define recurrent. And feed forward okay.\n\nSo basically a recurrent neural network process processes a given signal in a fashion that is iterative.\n\nSo it steps through every element of that signal. And if that signal is say a sentence as it would be commonly represented. Then for every word in it every token you have one vector. And you.\n\nNow basically process the first vector. And then you get a state. Then you somehow process the next word vector. And your state. And you integrate them into some new state. And you keep doing that problem with that is it takes you at the very least as many steps as your sentence is long a feed forward neural network or MLP basically takes some fixed length input. And sends that through a series of such Transformations the same that you apply as you recur through your signal in a recurrent neural network very similar. But only in a in a fashion that is that only applies to one fixed length. And hence also has a fixed number of operations.\n\nNow that means if you want to process signals of variable size with something that is feed forward you TR additionally had to somehow aggregate that into something of fixed size. And the way that would often times happen is by just saying okay if we take our sentence with a word Vector per word. And now we have moving Windows of say three words or.\n\nSo and we basically for each of these moving for each of the three words under this moving window we map them onto one vector. And then we just add them all.\n\nNow we have one vector. And that we can send through a feedforward structure Network structure if you wish. But now you've lost all ordering information right. And so what self attention coming back to to what we talked about before it does is it gives you a way of processing something of variable size despite the fact that what you're doing is you're just applying feedforward elements there's an important detail which I for which I which I didn't mention.\n\nSo far which is that in the way I just described self attention you would have still forgotten the ordering in order to retain the ordering you need to basically Infuse each word Vector with some representation of where the word was in the signal in the sentence. But once you have that you can actually retain the ordering in a way that is not recurrent that doesn't require you to do a number of steps that is a function of the length of your signal okay.\n\nSo why did you call it a Transformer. And who came up with the name.\n\nSo why do we call it a Transformer ultimately it makes a lot of sense if you think about this kind of holistic transformation of the entirety of the signal in every step right in contrast to recurrent neural networks that evolve a representation over yeah as they occur as they iterate over a signal right this thing basically has some representation of the whole thing. And transforms it in whole You could argue it is that difference isn't you know maybe that what rard neural networks do also is a transformation it certainly is. But up to that point nobody else had used the name it seemed to be pretty fitting I had some some Transformer toys when I was a kid that I really liked I came up with the name. But it was you know there was some tongue. And cheek Swagger if you wish in the choice of the name too.\n\nSo it was you know we I would say Among Us still even Among Us somewhat secretly. But we did have high hopes for this to maybe also be transformational. But interesting can you talk about coming to assemble the team that came together on this paper.\n\nSo the team came together in a bunch of different waves if you wish. And those waves were by no means planned or orchestrated what happened was that Ilia pushin who was running a team a part of my team at the time in Google research had decided to leave the company. And so he. But he had he had several months during which as far as I recall he was waiting for his co-founder to be able to leave behind what he was doing at the moment although I'm not 100% certain about that anymore. But basically there were a few months that he was still going to spend at Google. But he knew. And we knew that he was going to leave after that. And so his appetite for risk Skyrocket effectively compared to what he was doing before he was managing a larger sub team of mine. And they had some pretty clear engagements. And with search with product teams. And so once his transition out began he basically was yeah with a much increased appetite for risk looking for something crazy to do. And this is in the context of a group that had just published a paper on a model called decomposable attention model that was also a fully self attentional model the first one we believe of that kind. And we were playing with self attention mechanisms in a bunch of different applications. And in a bunch of different shapes. And forms was that the first publication of attention at all no.\n\nSo attention actually not in the self attention manner was applied in sequence to sequence or seek to seek RNN LST models routinely at the time okay.\n\nSo you would basically the mechanism that I described where for any given word you get a distribution over the other words you would do that. But for every given word you would get a distribution over say for every given output word in the translation system you would get a distribution over input words. And so that mechanism attending from one signal into another was being used routinely was part of the state-of-the-art. And machine translation at the time there were there was another group in Edinburgh actually that was also looking at a self- attention mechanism in order to improve rnns.\n\nSo they're basically as an RNN would generate output they would attend to the previously generated output. But aside from that work we believe we were actually the first to publish back in 2016 on self attention. And now that publication actually we were working on that already in 2015. And before just took us a while to get around writing up a small short paper for emlp publish that there. But basically that was the context of a group.\n\nSo we had already applied mechanisms like that although implemented in a much less scalable way to be clear to a bunch of different types of signals including language. But on much smaller tasks than large scale machine translation. And you know I had high hopes for basically replacing recurent neural networks Wholesale in seekto seek style models that were applied to say all sorts of things from machine translation to Gmail autocomplete. And lots of other problems at the time. And so you know Ilia said why not sounds crazy. But that could actually work he was also quite hopeful that there would be practical applications also for his soon to be former team because it could potentially speed up question answering models that they had been working on in Google search where latency requirements were actually very tight. And very quickly asish vasani who was at brinan at the time was also looking for something new to do he had worked on a few different projects. But you know nothing really with the potential at least that he was looking for he was also looking for something new they had talked I talked to sh around the time that he started a whole bunch. And so they started hacking away on this. And we wrote up some initial design doc that basically just said we're going to apply this to all sorts of different problems iterative self attention for all sorts of different problems including machine translation. And then we had a meme there of a bunch of Transformers zapping lasers. And and they got going on actually trying it out. And actually. And now I don't remember the exact sequence of events at some point in time Nikki parar who was also on my team at the time joined forces. And started running experiments. And we saw first signs of life. But also saw those to Plateau after some time. And that's not uncommon in in these kinds of efforts because turns out that even if your intuitive or conceptual idea for a mechanism is has lots of promise or holds lots of promise you really need to get all the implementation right you need to really optimize it because a lot has to do everything has to do with computational efficiency. And we hadn't we were building Bas we I shouldn't say we actually it was really a shish. And Ilia were. And Nikki they were building this codebase from scratch as a new research codebase it didn't they simply didn't have time hadn't had time to implement all the bells. And whistles of what. Then was basically super fast moving modern day deep learning. And around that time when we joined through happen stance ultimately some folks in the brain team no shazir lukash Kaiser. And Aiden Gomez saw what we were doing there. And decided to experiment with the self- attention mechanism in a code base that they had been building at the time actually in order to build convolutional seekto seek models.\n\nSo a different attempt of moving away from recurrent models in seek to seek. But based on convolutions. And they. Then added the self attention mechanism to it. And it really improved their results. And they had a code base that actually had all the bells. And whistles. And then came a crazy period of time when basically Noam. And no Lucas. And Aiden ripped out what they had put in the other mechanisms that they had put in before. And in the end we're left with something that was purely self- intentional it looked lot like the architecture that asish had. And Ilia had left the company by.\n\nNow but that asish had put together or that asish had at the time. But it was.\n\nNow implemented in this other framework with all the black magic tricks. And and in a way that really you know Noam who's one of the most legendary deep learning magicians there are right he. Then ultimately also implemented that mechanism in it very efficient way. And so what started. Then was actually the end of that plateau. And that turned into just rapid improvements in performance of these models for basically almost every day for months. And yeah was just an incredible incredible ride.\n\nSo there's very interesting thing that you just talked about which is that the Insight that was discovered was that by taking out these things these quote bells. And whistles that you mentioned. Well the bells. And whistles were important in terms of training tricks etc. But it was other mechanisms convolutional mechanisms etc those were the ones that we removed. And that's would actually you know reduced it back to this very pure self- attentional architecture that. Then also worked really. Well once in that other code base that had all the tricks of the trade if that makes sense.\n\nSo we took out architectures. But it.\n\nNow was implemented very efficiently by Noah in this code base that had all the tricks of the trade.\n\nSo basically the differentiation here is the tricks of the trade the Deep learning black magic that was crucial to have. But what they had before was something that had some convolutions. And some other stuff. And you know. And then they had added the self- attention mechanism on top of that. And what they. Then did was remove all the other stuff they had before they added self attention being left with just self attention. But now newly implemented super efficiently in this code base that had all the tricks of the trade all the Blackmagic Wizardry. And that's really what. Then in a certain sense really started to show not just promise. But started to just improve rapidly. And very quickly Way Beyond the state-of-the-art okay.\n\nSo okay yeah.\n\nSo that.\n\nSo that means like you it kind of went back to a pure self attention model exactly.\n\nSo by tricks of the trade the black magic you mentioned like is this sort of like like I've I.\n\nSo I've read that like designing neural networks is in a way more of an art than a science is that's correct that's right.\n\nSo basically let me give you a few examples.\n\nSo there are things like what was called layer normalization or layer Norm at the time this is a at Best turistic Way statistically motivated in some sense. But really at best turistic way of making sure that none of your activations go through the roof. And just in terms of their absolute magnitude which. Then makes optimization incredibly challenging. And the way you implement that actually makes a difference you can Implement a bunch of different ways in fact one could argue the very first Transformer paper had it in a weird way in a funky way. But an in effective one. But you basically need to have fast. And working verified implementations of such a trick if you wish if you want your architecture to really behave optimally another one is label smoothing where you don't actually train your model to predict the very pequ empirical distribution of it is exactly that next word that will follow these previous words. And so basically all other words are zero probability. And that word has all the probability Mass. But it turns out it's easier to train these things. And you get in a certain sense maybe more robust parameters more robust Solutions in the end if you smooth that distribution a little bit. And you actually allocate reallocate some probability Mass to the words that you don't actually observe. And take it away from the word that you do observe in your training data that you train your model towards why we could talk about with a length. And nobody really actually knows. And so it's basically tricks like that you not only have to have. But you need to know how to use them you need to they're kind of these magic constants that some folks such as Noom for example they just have a feel for it they've done it a million times they know okay yeah label smoothing with this kind of parameter or here is how you apply layer norm. And here's how you implement it. Well and that list of such tricks is pretty long. And so you need to have those in order for your say new architecture to even in C in a certain sense stand a chance right your conceptual mechanism can be as good as it wants it needs to be implemented super efficiently. And you need to have overall a setup in which you have these tricks of the trade at your disposal because not all of them are going to be useful for all versions of all architectures right that you need to start experimenting with those. And playing around with it. And then you look at how your models perform. And how they react to some of these tricks. And then Alchemy. And then you add some others. And then you modify them a little bit right in that kind of tinkering that is really this kind of this kind of art. And to drive that point home maybe it's.\n\nNow been over six years. And you still have you still find in open source open source competitive open source code bases implementations of the Transformers that use some of the same magic numbers that the original paper had right.\n\nSo there really there clearly is some art to this right. And you need ultimately the art at least as much if not more. Then you need these conceptual insights of hey wouldn't it be more effective if we did this pairwise comparison even though it looks kind of silly. But then iterate that only a few times in contrast to iterating over the signal purely sequentially right yeah.\n\nSo how did the how did the name of the paper come to be to tell you the truth I don't remember okay.\n\nSo there's the story Leon who also joined the project actually somewhere around the time that Nikki joined a little bit later as far as I recall he at some point recalls he recalls at some point suggesting this as a relation in relation you know. Well first of all making it clear. And emphasizing that we're really going against the grain. And propos a new. Well it wasn't entirely new the mechanism at the time right. But we had published on it before. But to take this new mechanism. And really dump everything else out. And just build with that. And that rapid departure from basically all other established mechanisms was something that we should emphasize. And do.\n\nSo in a in a in a you know in a strong way. But also this nod to The Beatles. And I may not actually have been in that meeting although I thought I was. But yeah basically at some point he proposed that you know isn't aren't we actually saying that attention is all you need. And that stuck. But yeah again I don't remember any of the any of the details actually okay.\n\nSo it's a reference to the beetle song it's also a reference to The Beatles song. But it's also it also certainly as far as I recall it was the way he said it at the time was probably also driven by this kind of in a certain sense defiant phrasing of ah we don't need all that stuff blah is all you need right that you know sometimes Engineers would throw around.\n\nSo certainly also in part of reference to that I would say Okay.\n\nSo then describe their reaction to the paper both at the time immediately. And also in the years since I mean immediately actually people were skeptical. And I think rightly.\n\nSo so we evaluated on two tasks dependency parsing in a spe very specific instance. But primarily machine translation. And really the focus was on machine translation. And if something that different is evaluated only on one major task. Then I do think that actually skepticism is Justified more than Justified. And so our initial reviews coming back from nurs were actually kind of lukewarm I would say they or they were at least the there were some lukewarm reviews among them somebody was excited too. But then what happened fairly quickly is that folks in a variety of different places open AI is one of them. But not the only one Sasha Rush actually who hugging face didn't exist at the time yet. But who. Then later joined hugging face. And at the time was I believe at is he at Harvard I think he was at Harvard at the time him open Folks at open AI Alec. And ilaser they very quickly started playing around with this. And because they did see the promise that if this worked. Then you really could have something that computationally is much is a much better fit for accelerator Hardware of the day. And of the present day too. And so as they started playing around with it there were some initial I remember actually having a conversation with Sasha in New York very quickly after the paper hit archive. And he had reimplemented the whole thing from scratch. And there were some issues around learning raid schedules. And so forth. But by. And large it was working out of the box. And that was unusual because it wasn't based on an open source implementation we did release one later. But he had done it from scratch actually in a I believe he actually even documented that project as the annotated Transformer there was a blog post that he had written up. But he was really he got super excited. And again others as. Well by the fact that they could very quickly get something that actually trains Ed least as. Well if not better. And an order of magnitude or more faster than lstms. And that as that kind of moved through the community it took maybe I would say half a year or.\n\nSo by the time we presented the paper in a poster session at nurs 2017 we basically had tons. And tons of interested people. And mostly still practitioners.\n\nSo practitioners or somewhat more senior people who still were very Hands-On. And then the next major event or events actually two of them were the release of the GPT before it became a series the generative pre-train Transformer coming out of open ey. And Bert which was for bidirectional encoder representations from Transformers. And both of those showed that you can use these.\n\nNow more efficient models training them in one way or another either as language models in the case of GPT or as basically infilling models in the case of Bert that you could use those models trained unsupervised just on text curated at the time it was books that they used at the time. And then they broadened into other types of data. But that you could use these unsupervised pre-trained models to actually. Then either tune or adapt those models to standard tasks in language understanding very rapidly. And just get off the charts good performance. And so that. Then really kicked off the next major wave of interest. And excitement especially as. Then both of those in fact actually maybe Bert in a in a in a in a very crisp way they had different model sizes in even the original publication. And they showed that just making it much bigger would massively improved performance across a pretty broad range a shockingly broad range actually of language understanding tests. And that. Then formed this in.\n\nSo many ways basically the initial spark that led to these massive infrastructure efforts that we're seeing.\n\nNow you know it's not even culminating there's still very much going on around gp4 Claude. And whatnot all the very large models built by you know the coheres open eyes. And anthropics of this world that just keep improving actually as we make them bigger. And bigger. And that the community. Then grasp pretty fast.\n\nSo basically around I would say 2019 2020 that was in full swing. And you know we were seeing that these what is called maybe somewhat misleadingly emerging capabilities are manifesting Etc. And so at that stage it kind of maybe transitioned into a mode where I was starting to get a little bit what's the right term I'm really looking forward to seeing new architectures that address some of the pretty obvious problems the Transformers still has. And inefficiencies. And issues. And so.\n\nSo basically.\n\nNow we're in a state when there was a lot of energy. And a lot of effort spent on scaling these things up applying them also in increasingly to other modalities. And people getting super excited about that by other modalities you mean all sorts of stuff.\n\nSo basically it took a while we can this is maybe a different slightly different question. But genomic data images videos time series all sorts of stuff like that audio. But basically you know it had it had at that stage.\n\nNow in you know maybe 2020 21 22 become kind of the default building block. And in my opinion maybe people maybe the amount of effort we invest invested in replacing it. And improving it actually started to be too little that's that's gotten better this year maybe 23 24. But it's still not quite I still think we really yeah it's about time we do away with it or at least dramatically improve it the multimodality aspect was very interesting because one of the other things that happened basically I would say around 19 2019 2020 I had moved back to Berlin we started working on applying in this new group rain Berlin on applying Transformers to Vision there was also a group doing this here. But ultimately what we did in Berlin showed around that time 2020 21 that you could actually take a pretty much vanilla Transformer. And do image understanding with it in at a level of quality that certainly rivaled. And maybe superseded the cnns of the time. And that was. And the interesting thing is Not only was it at least as good or maybe better. But it was the same architecture that you could also apply directly to language. And that is along with other such new modalities that other groups start applying this to maybe most prominently Alpha full 2 actually with protein sequences or ultimately also protein 3D representations of proteins that showed that.\n\nNow you know the prior on how successful would it be if you tried to apply this architecture or deep learning basically to some entirely different problem the prior just shifted before it was okay maybe we can do this people in 2012 had done it with computer vision with cnns with convolutional n networks. Then you know in 2015 seek to seek. And 2016 the first major launch of gnmt of the Google neural machine translation system people had. Then applied lsdm seek to seek lsdm to machine translation. But there were years between these major successful new applications right speech recognition. And so forth was also somewhere in there. But by 2020 effectively through things like the vision Transformer Alpha full 2 Etc the community had realized.\n\nNow there seems to be this thing that works for everything right. And even though that's not actually true. And even though it does require did certainly still does require a lot of work to make it work people really started to be optimistic about projects like that succeeding. And so as a result they started succeeding. And right it's it's it's really remarkable actually the one of the biggest impact one of the biggest causes for accelerating impact of this paper was that it got people really optimistic about applying that stuff to some new problem that nobody had applied it to before. And about scale. And scaling it up being one of the Surefire ways of. Then also making it better.\n\nSo that's something you can throw money at that's something you can throw time at. And so suddenly the went from this mode of oh my God there's a new thing we can do with this to yeah of course it's going to work. And so we're just going to do this. And this. And this. And this. And now we have models I can do it all together. And now we can learn from Vision about language. And from language about vision. And actually the lines are starting to blur. And that's really I think what we need to happen even more than it happens today with you know modeling video modeling processes over time in order to really get to the next seismic shift that deep learning will cause which is to really have models that have a holistic grasp of our sensory experience of our sensory environment. And are able to really get what we how we perceive the world right. And that's separate to certain extent get ways of or are able to perceive the world. And to effect to intervene with the world in ways that we can never right.\n\nSo on one hand it's about really getting our reality. And being able to not just generate stimuli that we generally consume videos audio all this sort of stuff. And understanding our Audio Visual. And sensory environment. And mesing that with our conceptual understanding as captured in language. But also allowing us to suddenly learn from electromicroscopy data in ways that no human could ever make any sense of right as is increasingly the case with things like structural areas like structural biology or design molecules in ways that we don't even understand not only do we not understand the rules or the you know the mechanisms at play we don't even understand how those design choices. Then have the effects that they have. But ultimately you know enabling this kind of observation. And discovery of aspects of our a world that are completely inaccessible to us humans.\n\nSo I wanted to clarify.\n\nSo it seems like what's happened is that previously in deep learning you would have specific architectures for specific problems. And now the Transformer is like a universal architecture being applied to everything y That's right exactly. And that just right it gets people more optimistic about these new applications. But it also enables these multimodal models that cut across many applications that bring to bear data in one modality or modalities bring to bear data in one modality in order to improve our understanding of another allow us to map things from one to the other. And ultimately right if you think about it language is a way for us to codify. And communicate usually only.\n\nSo far only between humans a sliver of conceptual understanding of our world right. And so if I one way of conceptualizing Lang as a phenomenon is I give you a piece of language I give you a piece of how of a world. And then you can actually run that world to a certain extent right. And that involves all the stimuli. And all the modalities if you want to look at it like that that we can perceive. But also that we can interact with in the world. And for the first time it's within reach to build a computer that can actually that has any chance of getting close to our ability not only to capture a piece of a perceived world. And communicate say in language. But also take some piece of language. And then in a certain sense instantiate this kind of sliver of if you wish encapsulated world or.\n\nSo are large language models just stochastic parrots piles of Statistics or do you believe they are emergent Learners that encode knowledge. And understanding of the world are we just stochastic parrots or are we emergent Learners I don't know.\n\nSo basically I think these distinctions are almost meaningless as long as we don't specifically Define what we mean by intelligence what we mean by learning what we mean by parting I don't think it's a particularly useful or many of those if not most of those distinctions. And dividing lines are particularly actionable or insightful because we do not know how we learn we don't know if what we call understanding is in any fundamental way truly foundationally different from just grocking the statistics or just being a bit more a bit better at being predicting at predicting certain phenomena right for all we know our you know oh.\n\nSo sacred. And oh.\n\nSo different learning abilities are just that. And if that's the case. Then you know surely they are learning. And they're Learners. And so in a certain sense right I'm I'm of the opinion that I don't think there's a ghost in that machine. But I highly doubt there's one in us too.\n\nSo or either or whichever it is. But basically I would on one hand I don't believe in the mechanistic interpretation of life because it's not a machine that was designed. But evolved. And as a result structurally it's very different. But on the other hand I find it very difficult believe to believe any kind of you know Duality or dualistic view that implies that there is something other than something mechanistic that makes us that differentiates us from you know other machinery. And as a result right it could just be an experimential SL data problem right to get to get them to our level of in quotes understanding. And no matter how you define understanding by the way I find it very difficult to believe that there is no understanding in these in these machines it's not the same as ours it can't be because they're their the breadth of stimuli that they're exposed to is just incre extremely limited even compared to ours. But ours is also extremely limited compared to the world right. And you see that very clearly when you look at applications of deep learning to problems such as protein structure prediction no human not even a field of scientific discipline biology over decades was able to produce an understanding of how protein folding happens how protein structure emerges that was anywhere close to being good enough to being applied in the real world. And then you come along with this blackbox thing. And enough data. And enough pre-training. And it basically just works.\n\nNow how. Well it works I think we still don't fully know. But it works certainly better than anything humans ever did by hand that's for sure. And so it is not anymore the case that basically their the breadth of data of stimuli that they perceive that the machines actually are able to tap into is just smaller than ours it's basically just different at this stage I would say both what we see what we're you know able to learn from. And what the machines are able to learn from both of both of those distributions a few are still incredibly limited when you compare it to the wealth of information that's that's out there are llms a path towards artificial general intelligence I have no idea what artificial general intelligence is. And so I do not know I can't for the life of me give you a good answer to that question actually that's a great answer you mentioned you know you think you you're almost ready to move on to the next thing.\n\nSo where do you think the next thing in AI is going to be.\n\nSo let's see I mean for me personally the next thing in AI is actually more applications of AI if you wish. And I think in the past. And in the future it is different applications that provide good forcing function or that yeah ultimately steer where new ideas where you know we realize that the needs are unmet. And where we'll. Then have to improve our tools I think there's if you think about basically neural network architectures there's some obvious shortcomings of what we have today one is the fact that you still have to take these signals chop them into pieces. And then basically learn representations in some Vector space for each of these pieces. But the problem is this chopping into pieces is something that we can do fairly. Well for signals that have been in a certain sense engineered or have evolved to have certain statistical properties such as language for genomic data that have not evolved you know genetics there was no reason for evolution to basically evolve genetics in such a way that license that kind of tokenization or that chopping into pieces. And then representing them. And so. And what I mean by that is that these pieces you can't have too many of them for our current methods our current methods like it when there's tens or hundreds of thousands of pieces not more than that they don't like it when they're too few they also don't like it when there's still too many of these tokens although we're getting a lot better at that.\n\nNow we can do like you know hundreds of thousands or millions or.\n\nSo but we still have constraints around those things. And the fact that we need this chopping into pieces. And that we can't learn that in a data driven manner is a massive impediment to applying these Technologies effectively another one is that basically today we would all agree there are some problems that are harder than others right for usum I mean if I ask you what's 2 plus two you don't have any issue giving me a response to that if I ask you to you know I don't know find solutions to the shorting equation for some even simplistic system I way more way harder to do even though we actually understand how to do it like we understand the rules. But we you know to really do execute the computation extremely difficult to do yet you can formulate both of those queries in roughly the same length. And the output might also even be okay maybe in 2 plus two the output is also incred incredibly simplistic. But you can imagine situations in which the inputs. And outputs have very similar shapes. And yet the compute that something like you know gp4 or command r or whatever would allocate in order to execute to in order to try to solve this problem in order to give you a good answer is the same because the input. And output sizes are roughly the same. And that just makes no sense right.\n\nSo every time a human will say oh I need some time to think about that's not something our models can currently do. And that's obviously broken.\n\nSo right.\n\nNow basically we. And we are making progress there's actually there's just very recently been a few really interesting pieces of work mixtures of depths actually an interesting one I think in particular. But there is still scratching just scratching the surface when it comes to Dynamic allocation of compute as a function of a the difficulty of the problem as opposed to the obvious apparent size of the input of the query. And the answer that's generated. And I think actually already those two things if we really made substantial progress on them in connection or in the context of course of architectures that. Then really support that. And are efficient in that context. Then we would already we would already have made massive of progress. And who knows I mean it seems like seems like we're getting there. But it's you know these things are usually not revolutions they look like them in hindsight. And storytelling narrative wise you know lots of folks want to make them into kind of these singular events that somehow look like a revolution. But the reality is as it happens it's just a ton of hard work building on a ton of hard work building on a ton of hard work. And lots of people really trying their best. And trying to be creative. And so when you look at it like that. Then we're probably still in the beginning of really solving some of those or addressing some of those taking a step back this entire conversation we've assumed that the hardware. And I've said this a few times actually looks the same. And if you think about it right.\n\nNow because this's kind of a co-evolution that's happening right we were joking the other day with Jensen wrong the CEO of Nvidia that you know we were saying. Well we built the Transformer to fit your gpus. And he's like yeah we're building the gpus to fit your Transformers. And both are true. And so basically there is this co-evolution. But it is not clear that this specific instance of co-evolution is going is or rather I would say it's almost guaranteed that this specific co-evolution process is in a local Optimum. And meaning it is very difficult at the moment to come up with a very different model Paradigm because the existing model Paradigm has been engineered already.\n\nNow over some period of time to fit very. Well to this Hardware.\n\nSo even though it could be a departure from Transformers. And God knows which ways dumping self attention replacing something with something that doesn't need tokenization would all be awesome it is actually not going to be that fundamentally different. But also it is extremely difficult to come along. And build a completely different chip completely different compute substrate for machine learning deep learning AI because. Well all the competitive models they're all you know based on. And tailored to this Hardware why would it be interesting to come up with a different chip there is this if you look at the history of computing. And actually this is the perfect place to do this in the world I would say if you look at the history of computing there was a switch somewhere basically leading up to the 20s. And 30s of the 20th century the switch from analog to digital Computing. And right if you look at later Lord kelvin's tied computers. And similar super powerful analog computers that were exactly single purpose. And they had to be almost by definition in a certain sense what is it here analog precisions some of these some of these old analog Computing devices their issue was that you couldn't.\n\nNow apply them to even closely related other problems because these other problems would have to be addressed with sufficiently different algorithms or sufficiently different approaches that you actually had to dump out the entire computer. And build it from scratch. And that's really difficult to do. And so digital Computing offered the promise of being General right.\n\nNow with you know in a single way we can actually Express. And potentially address all computable everything that's computable we can actually do everything that's computable. But what's the price that we pay the price that we pay is that we invest the majority of the energy that goes into computation into maintaining discrete States.\n\nSo if you if you just squint at a current computer at a digital computer today the vast majority of energy it consumes the vast majority of heat that comes out of it has to go. And that energy has to be expended in order to maintain that some something that actually could be a scalar between zero. And one actually or more that.\n\nNow it stays either at zero or at one. But now with deep learning. And maybe with you know things like the transformer I'm not going to say the Transformer is it that would kind of be sad I would I would think. But with something like you know the successor the successor of the Transformer we seem to be in a position where actually we.\n\nNow have one algorithm that applies to many different relevant problems. And it's always the same algorithm.\n\nSo why couldn't we.\n\nNow go. And build an analog computer just for that one problem. And that one problem is just run a Transformer or just run the Transformer successor. Then we train such a model that can actually train new such models we build this one analog computer for doing that. And we're done we basically implemented an analog computer that can be way more power efficient for something like you know a complete algorithm an algorithm to learn new algorithms of the same computational shape that.\n\nNow are able to handle a very broad variety of problems that we really care about not all problems right not everything there's no completeness here or not anytime soon. But at least that way I think we could increase thermodynamic effic by orders of magnitude not just one or two. But many orders of magnitude potentially. And so I do think that there's taking this big step back enormous potential for for very different ways of doing all this that are that are building on still what we've.\n\nNow learned through you know this round of deep learning if you wish anyway yeah okay what do you see is the greatest their greatest potential to benefit humanity. And their greatest potential harm superlatives are always difficult.\n\nSo let's see chatbots you mean systems that or some kind of yeah systems that have language input. And output. Well I think in general. And this may sound like a toy. But in general the biggest potential is that they obviate the need to formalize or to structure information in a way that is palatable to human design machines you do not need to make a table in some way that is computer readable whatever that means you can basically just dump out the data. And if it's a table or tabular in a general way. Well you know maybe there's an exception maybe you have some place where in some row whatever the nth field is contains a note that says ah yeah that was the value. But actually I didn't really measure it that. Well and. Then take it with a grain of salt or some such. And these things that were previously in a certain sense from a data perspective incomprehensible to machines have.\n\nNow become totally manageable. And this doesn't hold only for data that is data. But it also holds for data that is effectively algorithms.\n\nSo you can.\n\nNow basically have llms or what have you whatever you want to call these things that generate code. And alongside with it natural language or sorry human language reasoning or plans that describe what the model in quotes thinks this code should be doing. And now you can go. And you edit the code. And your English description changes you can edit your English description the code changes you can basically you know comment in totality whatever was generated there. And that really marks a departure from all of these previously formulaic languages formula data description descriptions or data formats if you wish that is in a certain sense setting us free from the shackles of our limited of the machines we of our previous machines limited generality if you wish. And I think that's you know in the broadest sense the biggest potential the biggest harm or again I think yeah superlatives here are really tricky. But one of the maybe interesting downsides that this technology yeah comes with. And it's I think inevitable we just have to think about ways of mitigating this that's maybe underemphasized that's often underemphasized I believe is that we as societies as cultures have grown accustomed to the ability to use the difficulty of consuming. But primarily also generating text as security tool right.\n\nSo for example appeals in traffic court why doesn't everybody file appeal. Well because it's annoying it's difficult okay why is it difficult. Well you need to fill out a form okay that has become a push of a button. Then you need to mail it somewhere okay fine that has become a push of a button a long time ago.\n\nSo how about appealing some other decision how about affecting anything in any kind of basis Democratic fashion right these things are rate limited by the ability of generating or by our individual ability of generating language. But that rate limitation is gone right you can.\n\nNow generate language even conditioned on loads. And loads of information at hundreds of tokens per second on a single GPU right or faster. And so with some of these super large models. And as a result we really need to rethink a lot of mechanisms around Public Services around all sorts of white collar crime. And it's not it's not at all that these that these machines or these systems you know will take initiative. And anytime soon. And will harm us. But they are amazing tools in the hands of malevolent actors.\n\nSo I think that actually as one of a variety. But as an often underemphasized example I think is one of the Practical dangers or downsides of the technology that we need to look into. And that we need to mitigate a lot of the times people talk about content identification or generation you know identifying machine generated content I think that ship has sailed a long time ago we published a paper on water marking contents of generative models we will never go back to any system like that to be effective for a variety of different reasons. But I think that's also something we need to confront there's a very simplistic alternative or very obvious alternative which is we need to authenticate content or basically certify that content was generated by human individual that's maybe another area related closely related area. But and it's not the same problem actually if you think it through. But yeah. And the list goes on. But it's it's problems like that that we need to think about. And take care of.\n\nSo H how is how important are issues of trust to future chatbot technology. Well I mean it matters. But at the end of the day it matters in a way. And it also manifests increasingly in ways that are similar to how it manifests in humans.\n\nSo we've built as again societies. And cultures we've we've built lots. And lots of tools that help us build trust in other individuals. And we need to build a similar Suite of tools. And actually I think it's they're going to look very similar that allow us to establish trust in machines. And systems we have to kiss goodbye once. And for all the notion that we can do this in a way that is basically by Design or by construction there will not be as far as I'm concerned a training objective of neural network architecture a data set of whatever combination of all those things that by Construction in an engineering manner we can engineer to be trustworthy just like I can't engineer a child to be trustworthy or a human. Then you know a person to be trustworthy we have to educate them certify them recertify them on an ongoing basis. And so forth. And we have to understand better what situations are in which they might not be as trustworthy. And then just deal with that right this I find it I find it sometimes almost comical when there are articles about hallucinations of large language models. And they in the Press it's often described as this I should I say almost this very surprising perplexing phenomenon when I go to my three-year-old. And I ask her a question that she factually cannot possibly know she is going to totally hallucinate a great answer often times right in a in a shape or form that if I didn't actually know the fact I had no way of telling whether this was hallucination or not as long as it's as the context of the question is close enough to you know something that her world model already covers. Well enough right I can ask her who the president of the US is she's going to give me alth although she mostly speaks German she's going to give me an American sounding name right U maybe actually the current one she already knows. But like the last one she's still going to give me an American sounding name. And so it's not at all surprising that these that these models do that there are certainly things we can do to improve that to mitigate some of it. But there is not going to be a correct by construction kind of a way of engineering them to be safe in any way comparable to say how we're trying to engineer planes to be safe right even though I guess that also has a bit for. But for similar reasons. But yeah thank you.\n\nSo you know we touched on this you touched on this a little bit in your early answer. But there a lot there's a lot of discourse about both the AI might lead us to apocalypse or Extinction there's also on the other side utopian discourse what do you think of these conversations I think there there is a time. And a place for everything we should be having some of those conversations for sure I feel the extent the amount of energy. And time. And that's that's expended on some of them the amount of attention that some of them get is totally blown out of proportion. And often actually fanned by players very often in the industry in the sector even to further completely other objectives. And you know say for example I do believe that. And this is completely this is actually an emergent phenomenon I don't think there's necessarily any malevolence here that's happening. But I do think that it is quite convenient to just given psychology of people in the media the masses Etc it's quite convenient to be able to focus the discourse on say you know things like the risk of Extinction when you actually don't yet have an idea how to deal with some of these much more mundane problems like disruption potential disruption or a potential for disruption of civil services Public Services due to the fact that.\n\nNow everybody can produce like Tech at you know rapid rates or.\n\nSo and.\n\nSo yeah sure you know people are going to do what they learn you know removes resistance or eases resistance that they might otherwise encounter. And I think that's what we're seeing to a large extent. And so again don't get me wrong I think a lot of these conversations need to be had. But there are many more potentially often more mundane. And more pressing topics that I feel deserve some of that if not most of that airtime attention energy Etc. And that is a little bit unfortunate because that's really what actually could impede progress here. And could harm people on the way. And so i' I you know in a certain sense I feel we should get better at yeah allocating resources when it comes to which of these. And this Cuts both ways. But it goes both in the utopian as. Well as into the dystopian direction that these extremes right low probability High magnetude sure you know need there are things we need to talk about. But I feel we need to spend much more energy on the high probability. And much lower magnitude. But High certainty space of outcomes right what excites you most about chatbots again I think it depends a little bit on you know what you call chat Bots I don't think chatbots by themselves are.\n\nSo exciting. But I feel ultimately especially if you include things that can interact with lots of digital systems it really comes back to what we talked about earlier to this the to this generalized applicability of technology to problems in which the data. And or the problem formulation SLS solution description algorithm are just difficult to squeeze into a traditional kind of machine suitable formats or format. And so it's you know the fact that you can.\n\nNow deal with say for example language. And and image content in a way that is almost algebraic right you can basically you can you can describe something in language. And then you can apply that function if you wish to another piece of language in order to you know summarize a piece of text extract certain bits. And pieces of. Then formal information rewrite some of it etc those kinds of things I feel that's it's just very. And it's less about the natural way of interacting with the machine I kind of don't care about how natural it is I care about how fast it is. And how cheap it is in a in a thermodynamic sense efficient that we have some ways to go. But really it's it's just the level of generality that you.\n\nNow have it's it's also yeah it's just offers the potential to deal with the world's information in ways that just have absolutely been impossible before do we need to explore alternative approaches to Ai. And why alternative to What alternative to LMS again I mean alternative to llms meaning or deep learning Maybe I mean if you're basically thinking of things like symbolic approaches or neuros symbolic approaches I think the answer is in my mind what has worked better than any other lens through which to look at whether or not something could be effective is does it make it faster does it make it more thermodynamically more efficient. And if the answer to that is yes. And that's that's quite conceivable right that doing certain things in a more symbolic manner locally sometimes is more efficient by all means let's do it. But if we can't basically show that's the case. Then just for the sake of doing it differently because we think it might be philosophically better or you know mesh better with certain theoretical or certain theories of something why I don't know I wouldn't really yeah wouldn't understand the motivation okay. And you mentioned this therom dnamic thing.\n\nSo yeah what are the what are the environmental you know consequences of chat Bots. And llms. And can we make it environmentally sustainable.\n\nSo there I think it's really important. And in general I think this is actually I was tempted to say this also in the to the last question I think the chatbot llm distinction is a problematic one here because what I really believe is that deep learning overall very large models applied to you know a very broad range of different problems have the potential to drive our Energy Efficiency through the roof. And so when it comes. And this even just the allocation problems. But also our ability to harness energy all sorts of sustainable all sorts of sustainable energy can be optimized in just in some cases evidently massively in some other cases it's going to be it's going to be a little more difficult with these methods in ways that pay for the many times over that actually it would be to me shocking if you know some years into the future the fraction of our overall of the overall energy that is consumed by these large models hasn't gotten very large because it is it is to a large extent exactly those models that will make it much easier. And much more effective actually. And much more sustainable to in all sorts of different ways actually yeah ultimately generate or extract if you wish the energy from from the system okay final question can you talk about your current Venture sure.\n\nSo at inceptive what we're doing is ultimately building models. And this isn't just a AI or a dry Affair this involves an enormous amount of work experimental work in a wet lab both to validate methods. But also to generate training data. But ultimately to build methods or models that allow us to design molecules that once in the context of say you're. And my cells exhibit certain pretty specific. And increasingly broad functions. And so basically I guess one way of reformulating of rephrasing that is we're designing medicines with artificial intelligence. But we're looking at a particular angle of doing this where what we're starting with is mRNA. And soon RNA molecules that actually are in a certain sense life complete if you wish right.\n\nSo there's the RNA World hypothesis that everything in life can actually be traced back or started with RNA molecules it's not clear if that's really. And we might never know how life really or where life really originated. But it's enough to know that it could have been like this to. Then basically conclude that if we could only design RNA molecules optimally which is an incredibly challenging task we could affect all sorts of different interventions in in ourselves in these very complex organisms.\n\nNow that's definitely science fiction. But maybe we can start with a with a small set of functions such as mRNA printing some protein such as you know maybe detecting the presence of some small molecule or some specific macromolecule in the vicinity. And expressing one protein or the other whether or not based on whether or not it's actually present such as self-amplifying or self-replicating itself which if you wish looks a lot like like recurrence or recursion actually if we just have some of these simple functions can we learn how to design rnas that exhibit you know any combination of such functions inside our cells in ways that also have you know manageable or the desired consequences when it comes to innate immune responses or similar phenomena. And thus really enable medicines that are programmable to an extent that goes Way Beyond what we're able to do say certainly using small molecule medicines. But also for more mundane reason reasons using protein biologics where you know in contrast to those RNA molecules are incredibly easy to manufacture we can in our lab actually synthesize basically more or less any possible RNA the same cannot def certainly cannot be said for proteins it's much harder to do much harder to do at scale much harder to do you know if you want to create a or synthesize a broad variety of them. And so at the end of the day you know basically finding or enabling one type of substrate if you wish in this case RNA as one family of macro molecules that can still ultimately when designed just right affect the kinds of interventions that constitute you know the vast majority maybe even of all existing medicines. And many medicines that we would like to exist. And enabling that using machine learning because really the or deep learning AI at scale because one thing is clear at least to us we will never even as you know if you wish as Humanity we will never learn conceptually how to do that ourselves. And this is ultimately because.\n\nSo our only our only alternative is to just learn it in the sense of or through blackbox methods such as such as deep learning. And you can think of this just as a strict generalization of the problem of language understanding or more precisely generating human language where you know in human language at least we have an existence proof of entities like you. And me that are able to do it. But we've tried for decades.\n\nNow to understand how we do it. And what the rules are that govern it we failed to understand that to conceptualize that to build a theory around that. And instead we've had to apply these datadriven blackbox mechanisms that are.\n\nNow actually able to do it to a pretty large extent both understand. And generate human languages. And now in case of you know effectively learning life's languages which is you know at least one of the at first languages say RNA that is the one that we want to learn first here we don't even have an existence proof of any kind of system or organism that is able to design these things. And to comprehend how they work. And so there's even less of an of a hope of basically being able to design them effectively using or based on Theory based on conceptual understandings of this. And as a result an even greater Reliance on doing this with methods like AI.\n\nNow we're starting in in I would say you know the humble beginnings on that almost certainly very long journey are that we would like to design mRNA molecules with much higher chemical stability than say for example the mRNA code vaccines that are actually as a result accessible to a far greater number of people right not just say two billion people. But maybe two three times that because they don't require distribution using a deep Frozen cold chain. But also mRNA molecules that have U much higher. And potentially carefully modulated also over time protein expression subpar protein expression or two insufficient proin expression is basically what prevents many mRNA medicines from actually being practical today either because you would have to administer.\n\nSo much mRNA that you're going to get unwanted side effects or adverse effects or because you ultimately end up only expressing or not triggering certain certain Pathways or certain phenomena in life if the if the protein does isn't isn't expressed at certain minimum rates.\n\nSo ultimately you could probably enable a pretty broad range of mRNA medicines that we can't actually practically produce. And manufactured today or designed today. But also democratize access to them in a in a pretty meaningful way. And then expand the range of such properties. And functions Beyond you know protein expression. And chemical stability as we go. And then eventually including things like recursion self-amplification or replication conditionals reacting on you know the environment that that in those cells that these molecules end up in etc all right thank you --- *Converted using Multi-Format Converter (Tank Building Stage 2)*