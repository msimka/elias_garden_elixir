{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 ArialMT;\f1\froman\fcharset0 Times-Roman;\f2\fnil\fcharset0 HelveticaNeue;
}
{\colortbl;\red255\green255\blue255;\red237\green237\blue237;\red255\green255\blue255;\red0\green0\blue0;
\red25\green25\blue25;\red255\green255\blue255;\red154\green154\blue154;\red13\green13\blue13;}
{\*\expandedcolortbl;;\cssrgb\c94510\c94510\c94510;\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c0\c0;
\cssrgb\c12941\c12941\c12941;\cssrgb\c100000\c100000\c100000\c10196;\cssrgb\c66667\c66667\c66667;\cssrgb\c5882\c5882\c5882;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\qc\partightenfactor0

\f0\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\pard\pardeftab720\partightenfactor0

\f1\fs24 \cf2 \cb1 \strokec2 \
\pard\pardeftab720\partightenfactor0
\cf0 \strokec4 \
\
\pard\pardeftab720\partightenfactor0

\f0\fs28\fsmilli14400 \cf2 \cb5 \strokec2 I am Hansen Sue and the date is April 24th M and can you introduce yourself uh\cb1 \
\cb5 I'm Jakob today I'm the CEO of inceptive and before that I'm one of the\cb1 \
\cb5 co-authors of attention is all you need and a bunch of other scientific Publications thank you um so can you\cb1 \
\cb6 tell us how you got into the field of artificial intelligence in many ways that was\cb1 \
\cb5 actually something that uh I guess as a problem runs in the family so my dad is\cb1 \
\cb5 a computational linguist uh used to teach that that and computer science uh\cb1 \
\cb5 in Germany and universities um before that worked at SRI in Meno Park and uh\cb1 \
\cb5 spent quite some time in Stanford and so in a certain sense it's something that um yeah started to enter my life over\cb1 \
\cb5 dinner conversations when I was a young child and uh then later on I uh started\cb1 \
\cb5 to going to University in uh the early O's uh back in Germany again um actually\cb1 \
\cb5 to a certain extent tried to avoid uh artificial intelligence SL\cb1 \
\cb5 machine learning certainly when it comes to Applications around language uh in order to you know at least stray a\cb1 \
\cb5 little bit from uh from the family business um and then in 2006 I stumbled\cb1 \
\cb5 Upon A Google internship uh not too far from here and they offered me a few different research projects that I could\cb1 \
\cb5 join um and machine translation by far had the most interesting large scale\cb1 \
\cb5 machine learning problems and so with some chrin uh I bit the bullet\cb1 \
\cb5 and actually did end up in the family business of sorts interesting so when your father was working uh in\cb1 \
\cb5 computational linguistics um what were the techniques then and how would you contrast you know\cb1 \
\cb5 the techniques that he was using versus the techniques that you were using so in those days in computational\cb1 \
\cb5 computational linguistics the techniques were still they were becoming more data\cb1 \
\cb5 driven um there was more more machine learning being applied at least in certain specific parts of larger systems\cb1 \
\cb5 but there were still certainly heavily informed by and often times really driven by uh linguistic\cb1 \
\cb5 insights in a certain sense in the broadest sense theory of language or theories of language that were used as\cb1 \
\cb5 again in the at least underpinnings in how those systems were designed but often times really were used as the\cb1 \
\cb5 backbone of of how they actually operated um in stark contrast uh right\cb1 \
\cb5 what we started doing already in 2006 and then we 2004 2005 2006 uh at places\cb1 \
\cb5 like Google translate was um much less informed by basically our conceptualized\cb1 \
\cb5 understanding of language or a conceptualized approximation of an understanding of language and was trying\cb1 \
\cb5 to be more black box yet at the same time um often still constrained by um\cb1 \
\cb5 say combinatorial optimization Al algorithms that made assumptions strong assump iions about how uh often times\cb1 \
\cb5 the outputs of such systems such as Google translate would be assembled uh as functions of the input and so for\cb1 \
\cb5 example at the time um the leading approach was statistical phrase-based machine translation where you make the\cb1 \
\cb5 assumption that you can chop the input sentence or input string into phrases\cb1 \
\cb5 each of which have a small number of candidate translations and then what you're actually doing is within a\cb1 \
\cb5 certain search cones so to speak you're allowing yourself to reorder those not embracing the full comunal explosion of\cb1 \
\cb5 all possible reorderings but you're allowing you know limited amount of reordering and choice among those\cb1 \
\cb5 phrases but that means you're still making a strong assumption um when it comes to basically what the how the\cb1 \
\cb5 equivalence uh can be expressed between uh two sentences in two different\cb1 \
\cb5 languages um what happened then with basically the the Renaissance or\cb1 \
\cb5 Resurgence of De learning uh and applying that increasingly to language was really to in many ways reduce the\cb1 \
\cb5 reduce the assumptions or or limit the assumptions that had to be made about\cb1 \
\cb5 any such correspondences to an absolute minimum and treat really as a blackbox\cb1 \
\cb5 problem right so then the the time that you were describing your father working\cb1 \
\cb5 on it what era was what years were those I mean those were basically uh 80s '90s\cb1 \
\cb5 and then the O's and and the O's I guess was when really the transition happened\cb1 \
\cb5 from systems that were largely rule-based largely based on again\cb1 \
\cb5 conceptual uh conceptualized theories if you wish understandings of of phenomena\cb1 \
\cb5 like language uh towards things that were at least where at least you had\cb1 \
\cb5 larger numbers of parameters that were estimated using machine learning methods um and where the the constraints were\cb1 \
\cb5 still informed in a certain sense by linguistic understanding uh but not anymore uh basically forming the the the\cb1 \
\cb5 actual backbone of those systems but yeah those were basically those three decades where that shift happened or\cb1 \
\cb5 over which of the course of which that shift also happened so there's been over the three decades there's been a\cb1 \
\cb5 progression an increasing progression towards removing linguistic Knowledge from the machine learning systems\cb1 \
\cb5 removing it from the machine learning systems or maybe from the perspective of some of the practitioners at the time\cb1 \
\cb5 moving it into from the from the actual implementation and the algorithms um\cb1 \
\cb5 into uh constraints on statistical models that were then used to capture\cb1 \
\cb5 those phenomena uh but I would say also in a certain sense weakened right so so\cb1 \
\cb5 making making fewer or B ing it less on on ultimately theory if you wish yeah\cb1 \
\cb5 that's right okay um so uh\cb1 \
\cb5 in uh so I read that like around 2012 um one of the one of the um so wait so when\cb1 \
\cb5 you first started um in 2006 you were were you immediately put on Google\cb1 \
\cb5 translate or yeah that's right okay yeah and then by 2012 what were you working on at that point\cb1 \
\cb5 by 2012 I had actually recently transitioned out of Google translate and into a new team that um at the time\cb1 \
\cb5 internally was called aqua uh that we had dubbed Aqua um didn't stand for\cb1 \
\cb5 anything in particular but what it was really is the language understanding team of the product that later became uh\cb1 \
\cb5 known as the Google Assistant it wasn't called that yet uh neither internally nor externally um but had basically\cb1 \
\cb5 basically started working on systems that effectively by in the broadest\cb1 \
\cb5 sense semantically parsing queries would enable Google search especially on\cb1 \
\cb5 mobile especially in certain interfaces um to uh use a if you wish deeper\cb1 \
\cb5 understanding and more explicit formalization of an understanding of a query um that may have been formulated\cb1 \
\cb5 more in natural language than you know in Google e as as as we would call uh in order to immediately give answers\cb1 \
\cb5 or uh execute actions that the user may have uh may have desired M and was that\cb1 \
\cb5 motivated by comp competition that's right that that's right so basically that was motivated ultimately by by\cb1 \
\cb5 Apple Siri um and the uh the concerns\cb1 \
\cb5 that or the concern that Siri could change the way people expect to search\cb1 \
\cb5 and uh and obtain answers but also execute actions um in particular through\cb1 \
\cb5 uh when when when dealing with their mobile devices MH and what were the limits of the\cb1 \
\cb5 technology at the time\cb1 \
\cb5 um I would say I mean compared to what we're able to do today it was incredibly\cb1 \
\cb5 limited um and one of the issues was that the statistical methods so we were\cb1 \
\cb5 still applying methods that were if few squinted very reminiscent of statistical\cb1 \
\cb5 machine translation systems um and what that implied was that we actually had to\cb1 \
\cb5 fabricate training data that was remarkably similar um all the way to\cb1 \
\cb5 say what type of question was being asked or what precise action was uh uh\cb1 \
\cb5 was was requested of a device to schedule a timer or or an alarm or some such um and we had to gather quite a bit\cb1 \
\cb5 of training data uh from humans uh or or fabricate training data using uh using\cb1 \
\cb5 humans um that that was remarkably similar to what we would actually expect to see in the query stream um and we\cb1 \
\cb5 were only to an extremely limited extent able to use very large amounts of\cb1 \
\cb5 language in different domains and of different uh types uh in in improving\cb1 \
\cb5 our understanding of of queries and you know from from such a distribution and so really we couldn't really bring to\cb1 \
\cb5 bear say uh you know the majority of the web or so right it basically was all\cb1 \
\cb5 purpose built manufactured in fact we we used uh at the time um large numbers of\cb1 \
\cb5 uh what I guess we would now or one would maybe call gig workers in order to\cb1 \
\cb5 uh to fabricate that training data because there basically was no such data there was no data in existence that was\cb1 \
\cb5 sufficiently similar to what we expected that query stream to look look like and um so can you describe to a uh\cb1 \
\cb5 someone of a high school level um what is an RNN and an lstm and explain the\cb1 \
\cb5 full acronym okay so rnns are recurrent neural networks lstms are long\cb1 \
\cb5 short-term memory recurrent neural networks um basically what they do is\cb1 \
\cb5 they evolve in step that are taken in a recurrent fashion\cb1 \
\cb5 we'll get to that in a second they evolve a vector space representation of some something that is\cb1 \
\cb5 seen as a sequence of elements and so let's say in language the sequence of\cb1 \
\cb5 elements could be sequences of words each of those words is represented by a\cb1 \
\cb5 vector at first it could be something uh like an indicator Vector that you know for any word in your dictionary has\cb1 \
\cb5 exactly one dimension and there that Vector has the value one and has zeros elsewhere um but then that Vector is\cb1 \
\cb5 mapped through a lookup table if you wish onto some Vector in some higher dimensional space so now we have a\cb1 \
\cb5 sequence not of words anymore but vectors and some higher dimensional uh space of of of real numbers um and this\cb1 \
\cb5 RNN now evolves a representation of that entire sequence um or basically series of\cb1 \
\cb5 prefixes of that entire sequence by mapping the next or that the current\cb1 \
\cb5 Vector that it is the current word Vector that it is consuming uh through some uh\cb1 \
\cb5 mathematical mapping and then merging that into an\cb1 \
\cb5 existing uh State vector and so you have some State Vector that starts with some\cb1 \
\cb5 initialization initial value and now in every step you basically take in uh a\cb1 \
\cb5 new word Vector in the case of a sequence of words you send that through some mapping and then um either in in\cb1 \
\cb5 the course of doing that mapping or with a separate mapping you now merge that into basically added with some waiting\cb1 \
\cb5 uh into your current state and then you proceed to the next word vector and again add that into your current state\cb1 \
\cb5 and you also at every given step potentially transform your state in a in a way that is independent of uh of the\cb1 \
\cb5 next word Vector now in lstms what you do is you actually lstms\cb1 \
\cb5 are a special version of all of these mappings basically the mappings that uh\cb1 \
\cb5 control how you integrate the next word Vector how you integrate the previous\cb1 \
\cb5 state Vector um and uh with with some additional bells and whistles around the\cb1 \
\cb5 ability to basically in a certain sense forget your previous State as a function of the new word um to retain a certain\cb1 \
\cb5 amount of the previous state to ignore the current word as a function of your state and as a function of the new word\cb1 \
\cb5 and so on and so forth and so basically it uh allows you uh it allows the model\cb1 \
\cb5 uh how should I say um a sharp control uh to a certain extent to learn to\cb1 \
\cb5 control sharply um how much of these various um state or new word vectors to\cb1 \
\cb5 use in the computation of the subsequent State Vector\cb1 \
\cb5 okay if that makes sense I'm not sure I fully got that but that's okay well we'll move on okay um so talk about what\cb1 \
\cb5 led to the invention of the Transformer and what was the key Insight okay so\cb1 \
\cb5 um ultimately uh the the situation we found ourselves in at the time was that\cb1 \
\cb5 that these recurrent neural networks um that again coming back basically had\cb1 \
\cb5 to evolve some kind of state vector or some kind of state representation over\cb1 \
\cb5 the course of uh of you know some sequence of vectors such as vectors\cb1 \
\cb5 representing words um that that when training those models on very large\cb1 \
\cb5 amounts of data uh as you would find in some problems uh say in Google\cb1 \
\cb5 search you would actually never reach the performance uh of much simpler neural\cb1 \
\cb5 network models um trained on the same data and the reason was that these much simpler neural networks trained orders\cb1 \
\cb5 of magnitude faster and so even though they were much less expressive uh say for example simplistic\cb1 \
\cb5 feedforward neural networks um uh multi-layer perceptrons for\cb1 \
\cb5 instance that you could easily show were not not as expressive as rnns or\cb1 \
\cb5 lstms um because for example they couldn't actually iterate through these sequences they had to chop those\cb1 \
\cb5 sequences into engrams and then map those onto vectors and just add those basically forgetting about the global\cb1 \
\cb5 ordering of them and so on and so forth they couldn't be that expressive but because they were training so much\cb1 \
\cb5 faster you could actually uh they would actually see much more orders of\cb1 \
\cb5 magnitude more training data uh in any given amount of time than say an RNN or an lstm and would as a result still\cb1 \
\cb5 perform better in practice then these much more expressive models that we knew could actually handle language in a much\cb1 \
\cb5 more holistic manner or phenomena in language such as longdistance dependencies in in a much more holistic\cb1 \
\cb5 and and much more appropriate Manner and so at the end of the day and this this\cb1 \
\cb5 wasn't true for all problems at the time this was really pronounced only in cases\cb1 \
\cb5 where um you really were able to generate massive amounts of training\cb1 \
\cb5 data such as for dealing with queries in in the regular web search query stream\cb1 \
\cb5 of the regular web search query stream uh at Google and it also it it\cb1 \
\cb5 basically made it clear that if we could find a way of making such feed forward\cb1 \
\cb5 models train much faster on whatever Hardware we had we could potentially\cb1 \
\cb5 actually get the best of both worlds right if we could make them oh sorry if we if we uh I think that that came out\cb1 \
\cb5 wrong I think if we found a way of making these feedforward models more expressive akin to rnms or lstms yet\cb1 \
\cb5 uh somehow retain their fast training speed that we would actually get the best of both worlds basically have\cb1 \
\cb5 something have models that could really be expressive enough to capture language\cb1 \
\cb5 as a phenomenon fully or holistically while also benefiting from these vast\cb1 \
\cb5 amounts of training data that that we were able to manufacture and one of the fundamental insights from\cb1 \
\cb5 a linguistic perspective or just from a statistical perspective this doesn't just apply to language this applies to\cb1 \
\cb5 to many different signal types uh one could argue all signal types that we can make any sense of in fact one maybe\cb1 \
\cb5 philosophical Insight here is that this must be POS possible by exploiting the fact\cb1 \
\cb5 that there are hierarchical there is a hierarchical nature to at least our\cb1 \
\cb5 understanding of pretty much every stimulus that we get meaning we\cb1 \
\cb5 basically um in in language for example right if if you uh in in in grade school\cb1 \
\cb5 or later High School you you parse sentences right what you're really doing is you're expressing the fact that there\cb1 \
\cb5 is hierarchical substructure where you have individual phrases that basically where you can in a certain\cb1 \
\cb5 sense arrive at some understanding of in quotes most of their meaning just looking at that phrase and then maybe\cb1 \
\cb5 you have a different another phrase in that in a sentence where the same is true and then the third one and now in\cb1 \
\cb5 another past you don't actually need to go through every single word anymore in sequence you can aggregate these what\cb1 \
\cb5 what you've basically already in isolation determined as the meaning of these individual phrases and put those\cb1 \
\cb5 together to the meaning of a bigger phrase and so on and so forth now why is is that Insight interesting\cb1 \
\cb5 it's interesting because the hardware that we had at the time is actually still remarkably similar to the accelerator Hardware that's being used\cb1 \
\cb5 in deep learning today excels at parallel processing and so if you're able to identify what are some of these\cb1 \
\cb5 chunks that you can in isolation uh make some sense of already without looking at everything then\cb1 \
\cb5 you've just enabled parallel processing so basically you've said okay fine then we look at this phrase this phrase this\cb1 \
\cb5 phrase in isolation get to some extent an understanding of what they mean some Vector space representation of what they\cb1 \
\cb5 mean and then in the next step we don't only need to actually aggregate those basically representations and so on and\cb1 \
\cb5 so forth and that's in direct contrast to\cb1 \
\cb5 what rnn's do right because they they walk through the entire signal step by step one after the other and they have\cb1 \
\cb5 to do that in order to then arrive at some state representation in the very end after having run through that entire\cb1 \
\cb5 signal and so basically um what motivated these uh then self\cb1 \
\cb5 attentional models was this Insight that by basically paying what looks like a\cb1 \
\cb5 performance penalty initially namely by having every piece of the signal interact but in a very shallow way with\cb1 \
\cb5 every other piece of the signal that doing that would enable it to actually in a certain sense build soft tree\cb1 \
\cb5 structures over your entire signal that would then ultimately exploit the\cb1 \
\cb5 ability of these accelerators to do parallel Computing as opposed to sequential Computing so you basically\cb1 \
\cb5 parallel compute the interactions between all pieces of the signal or every piece of the signal with every\cb1 \
\cb5 other piece pairwise if you wish and then you would just repeat that step and it turns out that you don't need to\cb1 \
\cb5 repeat that um nearly as many times as the signal is long and that is\cb1 \
\cb5 ultimately what makes this type of parallel processing way faster Master on Hardware that is very good at performing\cb1 \
\cb5 these this large number of but more simple kind of pairwise comparisons in\cb1 \
\cb5 contrast to uh you know a sequence length many or sequence length many\cb1 \
\cb5 harder operations if you wish uh that then tried to basically keep that Global State up to date at every given at every\cb1 \
\cb5 given step okay so it sounds like in computational terms\cb1 \
\cb5 you have you're going from a linear algorithm to something that logarithmic\cb1 \
\cb5 logarith exactly that's exactly right okay but but you can't directly you don't know the tree and so as a result\cb1 \
\cb5 you can't directly and this is what you know what if if we knew the tree or if there was really truly a tree I don't\cb1 \
\cb5 believe there truly is a tree right because because it's only approximately correct that these signals are Tre structured or truly hierarchical but if\cb1 \
\cb5 we knew a tree and if it existed we would just draw the tree and then we would basically just aggregate the the\cb1 \
\cb5 individual representations along that tree and then we would actually end up at something that is logarithmic if we\cb1 \
\cb5 process it in parallel right because we only need logarithmic many steps in terms of tree depth in order to actually\cb1 \
\cb5 process the signal of a given length they don't know the tree and so as a result how do you\cb1 \
\cb5 approximate the tree you don't know which basically branches let's assume it's a binary tree so at every given\cb1 \
\cb5 level two things are paired up we don't know what that tree looks like okay seems silly but why don't we iterate\cb1 \
\cb5 over all possible pairings and now this at first looks counterintuitive uh from\cb1 \
\cb5 a computational efficiency perspective but if you make that very simple and in case of these self attentional models\cb1 \
\cb5 what usually this operation looks like is just um an inner product if you make that really simple then you can do many\cb1 \
\cb5 of these inner products in parallel there's no sequential dependency whatsoever that's what these accelerators love and then effectively\cb1 \
\cb5 have iterated over all possible branchings at that level and then you take another step and you iterate over\cb1 \
\cb5 all possible branchings again and again and so basically for if you happen to\cb1 \
\cb5 have Hardware that looks like these accelerators like gpus tpus Etc um then that is a much better allocation of\cb1 \
\cb6 compute than requiring it to be more complex basically matrix multiplication steps many few or so per step but then\cb1 \
\cb5 times the length of the system overall uh length of the signal overall I see um so what is self-\cb1 \
\cb5 attention and how does it work self attention is a way of imbuing feed\cb1 \
\cb5 forward models models that are that don't require recurrent steps um along\cb1 \
\cb5 the length of a signal with a um effectively Global\cb1 \
\cb5 receptive field with a way of looking at or of considering\cb1 \
\cb5 all possible pieces of an input signal at any given moment at any given step\cb1 \
\cb5 and so the way uh in the Transformer and in its its its predecessors decomposable\cb1 \
\cb5 tension model uh Etc that we published before the way that worked was by\cb1 \
\cb5 effectively doing pairwise comparisons uh exactly like I just explained right so instead\cb1 \
\cb5 of um instead of aggregating a certain kind of state by doing uh a set of\cb1 \
\cb5 Matrix multiplications for each new element and doing that in order you now\cb1 \
\cb5 basically process uh you compare um every single pair uh of representations\cb1 \
\cb5 that gives you a waiting of uh for any given element of all other elements\cb1 \
\cb5 so if we want to go into more detail here what you do is you these pairwise comparisons give you scalers for each\cb1 \
\cb5 pair of representations in fact Maybe even for every given position they give\cb1 \
\cb5 you a scaler for every other position now you could normalize those into some something that sums up to one you can\cb1 \
\cb5 call that a distribution over all other positions and now you can just add those up weighted by exactly that distribution\cb1 \
\cb5 um and so in a certain sense you can conceptualize this as for every given\cb1 \
\cb5 position in your signal let's say it's a word you're getting at every given step\cb1 \
\cb5 a waiting of all other words and you could say even though that's taking it a little bit too far um but but slightly\cb1 \
\cb5 oversimplifying you could say that tells you how important at this moment each of these other words is for\cb1 \
\cb5 determining the meaning of that word then you basically aggregate those in an\cb1 \
\cb5 operation that is totally simplistic you just add them up and then you send that through one basically feed forward\cb1 \
\cb5 through a small MLP but that small M MLP now operates only on this aggregation so\cb1 \
\cb5 you basically what's an MLP uh multi-layer perceptron so a very simple feed forward Network again no no\cb1 \
\cb5 recurence uh uh you know no iterative process basically you just you you you\cb1 \
\cb5 have this very simplistic addition that is in a certain sense independent of it's not truly independent of the length\cb1 \
\cb5 of the signal right but it's it's very very simple you can apply it to to very large signals and then you just end up\cb1 \
\cb5 with one vector again and that you send through uh uh through through this simple uh multi layer perceptron uh in\cb1 \
\cb5 fact usually a pretty small one and you do that again in parallel independently for each of your positions for each of\cb1 \
\cb5 your words so in every step what you do is for each word you attend to every\cb1 \
\cb5 other word meaning you get a distribution over all the other words weighted by that distribution you sum\cb1 \
\cb5 them all up so say if if you want something that looks like a tree then then your distribution is very pequ only\cb1 \
\cb5 on one other word and it's almost zero for all the others right and and so that now means you're just getting the vector\cb1 \
\cb5 from just that one other word you add it to your own you send that through uh\cb1 \
\cb5 your your um feed forward Network through your multi-layer perceptron and\cb1 \
\cb5 that then forms the representation for the word in question that we're talking about in the next layer and that's the\cb1 \
\cb5 process that gets repeated layer after layer but in every layer you do it not just for one word but for all of them in\cb1 \
\cb5 parallel but they're all independent right because you you don't need to know\cb1 \
\cb5 the new representation of any other word in order to compute your new representation and that is what again\cb1 \
\cb5 shapes the workload into something that is much more manageable and much more\cb1 \
\cb5 efficient ultimately uh on today's very parallel accelerator H hardar okay\cb1 \
\cb5 um so uh could you maybe um Define um because\cb1 \
\cb5 I'm not sure the audience understands the difference um Define recurrent and feed forward okay\cb1 \
\cb5 so basically a recurrent neural network um process processes uh a given signal\cb1 \
\cb5 in a fashion uh that is iterative so it steps through every element of that signal and if that signal is\cb1 \
\cb5 say a sentence as it would be commonly represented then for every word in it every token um uh you have one vector\cb1 \
\cb5 and you now basically process the first vector and then you get a state then you somehow process the next word vector and\cb1 \
\cb5 your state and you integrate them into some new state and you keep doing that problem with that is it takes you at the\cb1 \
\cb5 very least as many steps as your sentence is long a feed forward uh um neural network\cb1 \
\cb5 or um MLP basically takes some fixed length\cb1 \
\cb5 input and sends that through a series of such Transformations the same that you\cb1 \
\cb5 apply as you recur through your signal in a recurrent neural network very similar but only in a um in a fashion\cb1 \
\cb5 that that is uh that only applies to one fixed length um and hence also has a a\cb1 \
\cb5 fixed number of operations now that means if you want to process signals of\cb1 \
\cb5 variable size with something that is feed forward you TR additionally had to\cb1 \
\cb5 somehow aggregate that into something of fixed size and the way that would often times\cb1 \
\cb5 happen is by just saying okay if we take our sentence with a word Vector per word\cb1 \
\cb5 and now we have moving Windows of say three words or so and we basically for\cb1 \
\cb5 each of these moving for each of the three words under this moving window we map them onto one vector and then we just add them all now we have one vector\cb1 \
\cb5 and that we can send through a feedforward uh um structure Network structure\cb1 \
\cb5 um if you wish but now you've lost all ordering information right and so what what self\cb1 \
\cb5 attention coming back to uh to what we talked about before it does is it gives you a way of processing something of\cb1 \
\cb5 variable size uh despite the fact that what you're doing is you're just applying\cb1 \
\cb5 feedforward elements there's an important detail which I for which I\cb1 \
\cb5 which which I didn't mention so far um uh which is that\cb1 \
\cb5 in the way I just described self attention you would have still forgotten the ordering in order to retain the\cb1 \
\cb5 ordering you need to basically Infuse each word Vector with some\cb1 \
\cb5 representation of where the word was in in the signal in the sentence um but\cb1 \
\cb5 once you have that you can actually retain the ordering um in a way that uh\cb1 \
\cb5 is not recurrent that doesn't require you to do a number of steps that is a function of the length of of your signal\cb1 \
\cb5 okay um so why did you call it a Transformer um and who came up with the\cb1 \
\cb5 name um so why do we call it a Transformer um ultimately it makes a lot\cb1 \
\cb5 of sense if you think about this kind of holistic transformation of the entirety\cb1 \
\cb5 of the signal in every step right uh in contrast to recurrent neural networks\cb1 \
\cb5 that evolve a representation over yeah as they occur as they iterate over a\cb1 \
\cb5 signal right this thing basically has some representation of the whole thing and transforms it in whole You could\cb1 \
\cb5 argue it is that that difference isn't you know maybe that that that what rard\cb1 \
\cb5 neural networks do also is a transformation it certainly is but up to that point nobody else had used the name\cb1 \
\cb5 it seemed to be pretty fitting um I had some uh some Transformer toys when I was a kid that I really liked uh I came up\cb1 \
\cb5 with the name um but it was uh you know there was some tongue and\cb1 \
\cb5 cheek um Swagger if you wish uh in the choice of the name too um so it was you\cb1 \
\cb5 know we we I would say Among Us still even Among\cb1 \
\cb5 Us somewhat secretly but we did have high hopes for this to maybe also be transformational\cb1 \
\cb5 but interesting um can you talk about\cb1 \
\cb5 coming to assemble the team that um came together on this paper so\cb1 \
\cb5 the team came together in a bunch of different waves if you wish and and\cb1 \
\cb5 those waves were by no means um planned or or uh\cb1 \
\cb5 orchestrated what happened was that um Ilia pushin uh who was uh running a team\cb1 \
\cb5 uh a part of my team at the time um in Google research um had decided to leave\cb1 \
\cb5 the company and so he but he had he had several months\cb1 \
\cb5 uh during which uh as far as I recall he was waiting for his co-founder to be\cb1 \
\cb5 able to um leave behind what he was doing at the moment although I'm not 100% certain\cb1 \
\cb5 about that anymore but basically there were a few months uh that he was still going to spend at Google but he knew and\cb1 \
\cb5 we knew that he was going to leave after that and so his appetite for risk um\cb1 \
\cb5 Skyrocket effectively compared to what he was doing before before he was managing a larger sub team of mine and\cb1 \
\cb5 they had some pretty clear engagements um and uh with with search with with\cb1 \
\cb5 product teams and so once his transition out began he basically was uh yeah with\cb1 \
\cb5 a much increased appetite for risk looking for something crazy to do and this is in the context of a group\cb1 \
\cb5 that had just published um a paper on a model called decomposable attention model that was\cb1 \
\cb5 also a fully self attentional model um the the first one we believe uh of that\cb1 \
\cb5 kind and we were playing with self attention mechanisms in a bunch of different applications and in a bunch of different shapes and forms was that the\cb1 \
\cb5 first publication of attention at all no uh so attention actually uh not in the\cb1 \
\cb5 self attention manner was applied in sequence to sequence or seek to seek RNN\cb1 \
\cb5 LST models routinely at the time okay um so you would basically the mechanism that I described where for any given\cb1 \
\cb5 word you get a distribution over the other words you would do that but for every given word you would get a distribution over say for every given\cb1 \
\cb5 output word in the translation system you would get a distribution over input words and so that mechanism attending\cb1 \
\cb5 from one signal into another was being used routinely was part of the state-of-the-art and machine translation\cb1 \
\cb5 at the time uh there were there was another group in Edinburgh actually that was also looking at a self- attention\cb1 \
\cb5 mechanism in order to improve rnns so they're basically as an RNN would\cb1 \
\cb5 generate output they would attend to the previously generated output um but aside\cb1 \
\cb5 from that work we believe uh we were actually the the first to publish back in 2016 uh on self\cb1 \
\cb5 attention and now that publication actually we were working on that already\cb1 \
\cb5 in 2015 uh and and before just took us a while to get around writing up a small\cb1 \
\cb5 short paper for emlp publish that there but basically that was the context of a group so we we we had already applied\cb1 \
\cb5 mechanisms like that although implemented in a much less scalable way to be clear to a bunch of different\cb1 \
\cb5 types of signals including language but on on much smaller tasks than large scale machine translation and uh you\cb1 \
\cb5 know I had high hopes for basically replacing recurent neural networks\cb1 \
\cb5 Wholesale in seekto seek style models uh that were applied to say all sorts of\cb1 \
\cb5 things from uh machine translation to Gmail autocomplete and and lots of other\cb1 \
\cb5 problems um at the time and so you know Ilia said why not sounds crazy but uh\cb1 \
\cb5 that could actually work um he was also quite hopeful that there would be practical applications also for his soon\cb1 \
\cb5 to be former team uh because it could potentially speed up question answering\cb1 \
\cb5 models that they had been working on in Google search where latency requirements were actually uh uh very very tight and very quickly\cb1 \
\cb5 asish vasani who was at brinan at the time was also looking for something new to do he had worked on a few different\cb1 \
\cb5 projects but uh you know nothing really with the potential uh at least that he\cb1 \
\cb5 was looking for uh he was also looking for something new um they had talked I talked to sh around the time that that\cb1 \
\cb5 he started a whole bunch and so uh they started hacking away on this and and\cb1 \
\cb5 we wrote up some initial design doc uh that basically just said we're going to apply this to all sorts of different\cb1 \
\cb5 problems iterative self attention uh for all sorts of different problems including machine translation um and\cb1 \
\cb5 then we had a meme there of a bunch of Transformers zapping lasers and uh and they got going on on actually trying it\cb1 \
\cb5 out um and actually and now I don't remember the exact sequence of events at\cb1 \
\cb5 some point in time Nikki uh parar who was also uh on my team at the time uh\cb1 \
\cb5 joined forces and started running experiments and we saw first signs of life but also saw those to Plateau after\cb1 \
\cb5 some time and that's not uncommon in in uh in these kinds of efforts because\cb1 \
\cb5 turns out that even if your intuitive or conceptual idea for a mechanism is has\cb1 \
\cb5 lots of promise or holds lots of promise you really need to get all the implementation right you need to really\cb1 \
\cb5 optimize it because a lot has to do everything has to do with computational efficiency and we hadn't we were\cb1 \
\cb5 building Bas we I shouldn't say we actually it was really a shish and and Ilia uh were were and Nikki they were\cb1 \
\cb5 building this codebase from scratch as a new research codebase uh it didn't they simply didn't have time hadn't had time\cb1 \
\cb5 to implement all the bells and whistles of what then was basically super fast\cb1 \
\cb5 moving modern day deep learning um and and around that time when we joined\cb1 \
\cb5 through happen stance ultimately um some folks in the brain team no shazir lukash\cb1 \
\cb5 Kaiser and Aiden Gomez um uh saw what we were doing there and\cb1 \
\cb5 decided to uh experiment with the self- attention mechanism in a code base that they had been building uh at the time\cb1 \
\cb5 actually in order to build convolutional seekto seek models uh so so a different\cb1 \
\cb5 attempt of moving away from recurrent models in seek to seek um but but based\cb1 \
\cb5 on on convolutions and uh they then added the self attention mechanism to it and it really improved their results and\cb1 \
\cb5 they had a code base that actually had all the bells and whistles and uh then\cb1 \
\cb5 uh came a crazy period of time when uh basically Noam and and no Lucas and\cb1 \
\cb5 Aiden um ripped out what they had put in the other mechanisms that they had put\cb1 \
\cb5 in before and in the end we're left with something that was purely self- intentional it looked lot like the\cb1 \
\cb5 architecture that asish had and Ilia Ilia had left the company by now but that asish had put together or that\cb1 \
\cb5 asish had at the time but it was now implemented in this other framework uh with all the the black magic tricks and\cb1 \
\cb5 uh and in a way uh that really you know Noam who's one of the most legendary\cb1 \
\cb5 deep learning magicians uh there are uh right he he then ultimately also\cb1 \
\cb5 implemented that mechanism in it very very efficient way and so what\cb1 \
\cb5 started then was actually the end of that plateau and that turned into just rapid improvements in performance of\cb1 \
\cb5 these models for basically almost every day for months and uh yeah was just an\cb1 \
\cb5 incredible uh incredible ride so there's very interesting thing that you just\cb1 \
\cb5 talked about which is that um the Insight that that was\cb1 \
\cb5 discovered was was that by taking out these things um these quote bells and\cb1 \
\cb5 whistles that you mentioned well the bells and whistles were important in terms of training tricks etc etc um but\cb1 \
\cb5 it was other mechanisms convolutional mechanisms etc etc those were the ones that that we removed and that's would\cb1 \
\cb5 actually you know reduced it back to this very pure self- attentional architecture that then also worked really well once in that other code base\cb1 \
\cb5 that had all the the tricks of the trade if that makes sense so we took out\cb1 \
\cb5 architectures but it now was implemented very efficiently by Noah in this code\cb1 \
\cb5 base that had all the tricks of the trade so basically the differentiation here is the the tricks of the trade the\cb1 \
\cb5 the Deep learning black magic that was crucial to have but what they had had before was something that had some\cb1 \
\cb5 convolutions and some other stuff and you know and then they had added the self- attention mechanism on top of that\cb1 \
\cb5 and what they then did was remove all the other stuff they had had before they added self attention being left with just self attention but now newly\cb1 \
\cb5 implemented super efficiently in this code base that had all the tricks of the trade all the Blackmagic uh Wizardry um\cb1 \
\cb5 and that's really what then in a certain sense really started to show not just promise\cb1 \
\cb5 but started to just improve rapidly and very quickly Way Beyond the state-of-the-art okay\cb1 \
\cb5 so okay yeah so so that so that means like you it kind of went back to a pure\cb1 \
\cb5 self attention model exactly exactly exactly um so by by tricks of the trade the black magic you mentioned like is\cb1 \
\cb5 this sort of like um like I've I so I've read that like designing neural networks\cb1 \
\cb5 is is in a way more of an art than a science is that that's correct that's right so basically let me give you a few examples\cb1 \
\cb5 so um there are things like what was called layer normalization or layer Norm\cb1 \
\cb5 at the time this is a at Best turistic Way statistically motivated in in some\cb1 \
\cb5 sense but but really at best turistic way of making sure that none of your\cb1 \
\cb5 activations uh go through the roof um and just in terms of their absolute magnitude which then makes optimization\cb1 \
\cb5 incredibly challenging and the way you implement that actually makes a difference you can\cb1 \
\cb5 Implement a bunch of different different ways in fact one could argue the very first Transformer paper had it in a weird way in a funky way but but but an\cb1 \
\cb5 in effective one um but but you basically need to have fast and uh\cb1 \
\cb5 working verified implementations of such a trick if you wish uh if you want your\cb1 \
\cb5 architecture to really behave optimally another one is label smoothing where you\cb1 \
\cb5 don't actually train your model to predict the very pequ empirical\cb1 \
\cb5 distribution of it is exactly that next word that will follow these previous words and so basically all other words\cb1 \
\cb5 are zero probability and that word has all the probability Mass but it turns out it's easier to train these things\cb1 \
\cb5 and you get in a certain sense maybe more robust parameters more robust Solutions in the end if you smooth that\cb1 \
\cb5 distribution a little bit and you actually allocate reallocate some probability Mass to the words that you don't actually observe and take it away\cb1 \
\cb5 from the word that you do observe in your training data that you train your model towards why we could talk about with a length and nobody really actually\cb1 \
\cb5 knows and so it's basically tricks like that that you not only have to have but you need to know how to use them you\cb1 \
\cb5 need to they're kind of these magic constants that some folks such as Noom for example they just have a feel for it\cb1 \
\cb5 they've done it a million times they know okay yeah label smoothing with this kind of um uh parameter or here is how\cb1 \
\cb5 you apply layer norm and here's how you implement it well and that list of such tricks is pretty long and so you you\cb1 \
\cb5 need to have those in order for your say new architecture to even in C in a\cb1 \
\cb5 certain sense stand a chance right your your conceptual mechanism can be as good as it wants it needs to be implemented\cb1 \
\cb5 super efficiently and you need to have overall a setup in which you have these tricks of the trade at your disposal\cb1 \
\cb5 because not all of them are going to be useful for all versions of all architectures right that you need to\cb1 \
\cb5 start experimenting with those and playing around with it and then you look at how your models perform and how they\cb1 \
\cb5 react to some of these tricks and then Alchemy and then you add some others and\cb1 \
\cb5 then you modify them a little bit right in that kind of tinkering that is really this kind of this kind of art and to\cb1 \
\cb5 drive that point home maybe it's now been over six years and you still have\cb1 \
\cb5 you still find in open source open source competitive open source code bases implementations of the\cb1 \
\cb5 Transformers that use some of the same magic numbers that the original paper had right so there really there clearly\cb1 \
\cb5 is some art to this right uh and and uh you need\cb1 \
\cb5 ultimately the art at least as much if not more then you\cb1 \
\cb5 need these conceptual insights of hey wouldn't it be more effective if we did this pairwise comparison even though it\cb1 \
\cb5 looks kind of silly but then iterate that only a few times in contrast to\cb1 \
\cb5 iterating over the signal purely sequentially right um\cb1 \
\cb5 yeah um so how did the how did the name of the paper come to\cb1 \
\cb5 be to tell you the truth I don't remember okay so there's the the the\cb1 \
\cb5 story um Leon uh who also joined the project\cb1 \
\cb5 actually uh somewhere around the time that Nikki joined uh a little bit later\cb1 \
\cb5 as far as I recall um he at some point\cb1 \
\cb5 recalls he he recalls at some point suggesting this as a relation in\cb1 \
\cb5 relation uh uh you know well first of all making it clear and emphasizing that\cb1 \
\cb5 we're really going against the grain and propos a new well it wasn't entirely new\cb1 \
\cb5 the mechanism at the time right but we we had published on it before but uh to to take this new mechanism and really\cb1 \
\cb5 dump everything else out and just build with that and that that rapid departure from uh basically all other established\cb1 \
\cb5 mechanisms was something that we should emphasize and do so in a in a in a uh\cb1 \
\cb5 you know in a strong way but also this nod to The Beatles um and I may not\cb1 \
\cb5 actually have been in that meeting although I thought I was but yeah basically at some point uh uh he\cb1 \
\cb5 proposed that uh you know isn't aren't we actually saying that attention is all you need and that stuck but but yeah\cb1 \
\cb5 again I don't remember any of the any of the details actually okay so it's a reference to the beetle song it's also a\cb1 \
\cb5 reference to The Beatles song but it's also it it also certainly as far as I recall it was the way he said it at the\cb1 \
\cb5 time was probably also driven by this kind of uh in a certain sense defiant\cb1 \
\cb5 phrasing of ah we don't need all that stuff blah blah blah is all you need right uh that that you know sometimes\cb1 \
\cb5 Engineers would throw around um so certainly also also in part of reference to that I would say\cb1 \
\cb5 Okay um so then describe their reaction to the\cb1 \
\cb5 paper both at the time immediately and also in the years since I mean immediately actually people\cb1 \
\cb5 were skeptical and I think rightly so um so we we evaluated on two tasks um\cb1 \
\cb5 dependency parsing uh in in a spe very specific instance but primarily machine\cb1 \
\cb5 translation and and really the focus was on machine translation and if something\cb1 \
\cb5 that different uh is evaluated only on on one major task then I do think that\cb1 \
\cb5 actually skepticism is is Justified more than Justified um and so our initial\cb1 \
\cb5 reviews coming back from nurs were actually kind of lukewarm I would say they or they were at least the there\cb1 \
\cb5 were some lukewarm reviews among them somebody was excited too um but\cb1 \
\cb5 then what happened fairly quickly is that folks in a variety of different\cb1 \
\cb5 places open AI is one of them but not the only one Sasha Rush actually uh um\cb1 \
\cb5 who hugging face didn't exist at the time yet but who then later joined hugging face um and at the time was I\cb1 \
\cb5 believe at is he at Harvard I think he was at Harvard at the time him uh open\cb1 \
\cb5 Folks at open AI um uh Alec and and ilaser they very quickly started playing\cb1 \
\cb5 around with this and because they they did see the promise that if this worked then you really could could have\cb1 \
\cb5 something that computationally is much is a much better fit for accelerator Hardware of the day and and of the\cb1 \
\cb5 present day too uh and so as they started playing around with it there\cb1 \
\cb5 were some initial I remember actually having a conversation with Sasha in New York very quickly after the paper hit\cb1 \
\cb5 archive and uh he had reimplemented the whole thing uh from scratch and there\cb1 \
\cb5 were some issues around learning raid schedules and so forth but by and large it was working out of the box and that\cb1 \
\cb5 was unusual because it wasn't based on an open source implementation we did release one later uh but he had done it\cb1 \
\cb5 from scratch actually in a I believe he he actually even documented that project\cb1 \
\cb5 as the annotated Transformer there was a blog post that that he had written up but he was uh really um he got super\cb1 \
\cb5 excited and and again others as well uh by the fact that they could very quickly\cb1 \
\cb5 get something that actually trains Ed least as well if not better and an order\cb1 \
\cb5 of magnitude or more faster than lstms and that as that kind of moved through\cb1 \
\cb5 the community it took maybe I would say half a year or so so by the time we uh\cb1 \
\cb5 presented the paper uh in a poster session at nurs 2017 uh we basically had tons and tons\cb1 \
\cb5 of interested people and mostly still practitioners so practitioners or somewhat more senior people who still\cb1 \
\cb5 were very very Hands-On and then the next major event or events\cb1 \
\cb5 actually two of them were uh the release of the GPT before it became a series uh\cb1 \
\cb5 the generative pre-train Transformer coming out of open ey um and Bert uh\cb1 \
\cb5 which was for bidirectional encoder representations from Transformers and um\cb1 \
\cb5 both of those showed that you can use these now more efficient models training\cb1 \
\cb5 them in one way or another either as language models in the case of GPT uh or as basically infilling models\cb1 \
\cb5 in the case of Bert that you could use those models trained unsupervised just on text curated at the time it was books\cb1 \
\cb5 that they used at the time and then they broadened into other types of data but that you could use these unsupervised\cb1 \
\cb5 pre-trained models to actually uh uh then either tune or adapt those models\cb1 \
\cb5 to to standard tasks in uh language understanding very very rapidly and just\cb1 \
\cb5 get off the charts good performance and so that then really kicked off the next\cb1 \
\cb5 major wave of of interest and excitement uh especially as then both of those in\cb1 \
\cb5 fact actually maybe Bert in a in a in a in a very crisp way they had different model sizes in even the original\cb1 \
\cb5 publication and they showed that just just making it much bigger uh would\cb1 \
\cb5 massively improved performance across a pretty broad range a shockingly broad range actually of language understanding\cb1 \
\cb5 tests and that then formed this uh in in so many ways basically the initial spark\cb1 \
\cb5 that led to these massive infrastructure efforts that we're seeing now you know\cb1 \
\cb5 uh it's not even culminating there's still very very much going on around\cb1 \
\cb5 gp4 uh Claude and uh whatnot all the very very large models built by you know\cb1 \
\cb5 the coheres open eyes and uh anthropics of this world that uh just keep improving actually as as we make them\cb1 \
\cb5 bigger and bigger and that that the community then grasp pretty fast so basically around I would say 2019 2020\cb1 \
\cb5 that was in full swing uh and you know we were seeing that these what is called maybe somewhat misleadingly emerging\cb1 \
\cb5 capabilities are are manifesting Etc and so at that stage um it kind of maybe\cb1 \
\cb5 transitioned into a mode where I was starting to get a little bit uh\cb1 \
\cb5 what's the right term I'm really looking forward to\cb1 \
\cb5 seeing new architectures that address some of the pretty obvious problems the Transformers still has and\cb1 \
\cb5 inefficiencies and and and issues and so uh so basically now we're in a state when when there was a lot of energy uh\cb1 \
\cb5 and a lot of effort spent on scaling these things up uh applying them also uh\cb1 \
\cb5 in increasingly to other modalities uh and people getting super excited about that by other modalities you mean all\cb1 \
\cb5 sorts of stuff so basically it took a while we can this is maybe a different slightly different question but uh\cb1 \
\cb5 genomic data images videos um time series all sorts of stuff like that\cb1 \
\cb5 audio um but but basically you know it had it had\cb1 \
\cb5 at that stage now in you know maybe 2020 21 22\cb1 \
\cb5 become kind of the default building block and in my opinion uh maybe people\cb1 \
\cb5 uh maybe the amount of effort we invest invested in replacing it uh and\cb1 \
\cb5 improving it actually started to be uh too little um that's that's gotten\cb1 \
\cb5 better uh this year maybe 23 24 uh but it's still not quite I I still think we\cb1 \
\cb5 we really um yeah it's about time we do away with it or or at least dramatically improve it the multimodality aspect was\cb1 \
\cb5 very interesting because um one of the other things that happened basically I would say around 19 2019 2020 I had\cb1 \
\cb5 moved back to Berlin we started working on applying in in this new group rain Berlin uh on applying Transformers to\cb1 \
\cb5 Vision there was also a group doing this here but ultimately what we did in Berlin showed uh um around that time\cb1 \
\cb5 2020 21 that you could actually take take a pretty much vanilla Transformer\cb1 \
\cb5 and do image understanding with it in uh at a level of quality that certainly\cb1 \
\cb5 rivaled and maybe superseded uh the cnns of the time and that was and the\cb1 \
\cb5 interesting thing is uh Not only was it at least as good or maybe better but it was the same architecture that you could\cb1 \
\cb5 also apply directly to language and that is along with other such uh uh new\cb1 \
\cb5 modalities that other groups uh start applying this to um maybe maybe most\cb1 \
\cb5 prominently Alpha full 2 actually uh with with protein sequences or or\cb1 \
\cb5 ultimately also protein 3D representations of proteins um that\cb1 \
\cb5 showed that now you know the the prior on how successful would it be if you\cb1 \
\cb5 tried to apply this architecture or deep learning basically to some entirely\cb1 \
\cb5 different problem the prior just shifted before it was okay maybe we can do this people in 2012 had done it with computer\cb1 \
\cb5 vision with cnns with convolutional n networks then you know in 2015 seek to\cb1 \
\cb5 seek and 2016 the first major launch of of gnmt of the Google neural machine\cb1 \
\cb5 translation system people had then applied uh lsdm seek to seek lsdm um to machine translation but there\cb1 \
\cb5 were years between these major successful new applications right uh\cb1 \
\cb5 speech recognition and so forth was also somewhere in there but by\cb1 \
\cb5 2020 effectively through things like the vision Transformer Alpha full 2 Etc the community community had realized\cb1 \
\cb5 now there seems to be this thing that works for everything right and even though that's not actually true and even\cb1 \
\cb5 though it does require did certainly still does require a lot of work to make it work people really started to be\cb1 \
\cb5 optimistic about projects like that succeeding and so as a result they started succeeding and and right it's\cb1 \
\cb5 it's it's really remarkable actually the one of the biggest impact one of the biggest uh uh causes for uh accelerating\cb1 \
\cb5 impact of this paper was that it got people really optimistic about applying\cb1 \
\cb5 that stuff to some new problem that nobody had applied it to before and about scale um and scaling it up being\cb1 \
\cb5 one of the Surefire ways of then also making it better so that's something you can throw money at that's something you can throw time at and so suddenly the\cb1 \
\cb5 went from this mode of oh my God there's a new thing we can do with this to yeah of course it's going to work and so we're just going to do this and this and\cb1 \
\cb5 this and this and now we have models I can do it all together and now we can learn from Vision about language and from language about vision and actually\cb1 \
\cb5 the lines are starting to blur and that's really I think what we need to happen even even more than it happens\cb1 \
\cb5 today with you know modeling video modeling processes over time in order to really get to the next seismic shift\cb1 \
\cb5 that deep learning uh uh will cause which is to really have models that have\cb1 \
\cb5 a holistic grasp of our sensory experience of our sensory environment\cb1 \
\cb5 and are able to really get what we how we perceive the world right and and\cb1 \
\cb5 that's separate to certain extent get ways of or are able to perceive the\cb1 \
\cb5 world and to effect to intervene with the world in ways that we can never right so on one hand it's about really\cb1 \
\cb5 getting our reality and being able to not just generate stimuli that we\cb1 \
\cb5 generally consume videos audio all this sort of stuff and understanding our Audio Visual and sensory environment and\cb1 \
\cb5 uh mesing that with our conceptual understanding as uh captured in language but also allowing us to suddenly learn\cb1 \
\cb5 from uh electromicroscopy data in ways that no human could ever make any sense\cb1 \
\cb5 of right as as is increasingly the case with things like uh structural areas\cb1 \
\cb5 like structural biology or design molecules uh in in ways that we don't\cb1 \
\cb5 even understand not only do we not understand the rules or the the the you know the mechanisms at play we we we we\cb1 \
\cb5 don't even understand how those design choices then have the effects that they have uh but ultimately you know enabling\cb1 \
\cb5 this this um kind of observation and discovery of aspects of our a world that\cb1 \
\cb5 are completely inaccessible to us humans so I wanted to clarify so it\cb1 \
\cb5 seems like what's happened is that\cb1 \
\cb5 um previously um in deep learning you would\cb1 \
\cb5 have specific architectures for specific problems and now the Transformer is like a universal architecture being applied\cb1 \
\cb5 to everything y That's right exactly and that just uh right it gets people more\cb1 \
\cb5 optimistic about these new applications but it also enables these multimodal models that cut across many applications\cb1 \
\cb5 that bring to bear data in one modality or modalities bring to bear data in one\cb1 \
\cb5 modality in order to improve our understanding of another um allow us to map things from one to the other and\cb1 \
\cb5 ultimately right if you think about it language is a way for us to codify and\cb1 \
\cb5 communicate usually only so far only between humans a sliver of conceptual\cb1 \
\cb5 understanding of our world right and so if I one way of conceptualizing Lang as\cb1 \
\cb5 a phenomenon is I give you a piece of language I give you a piece of how of a world and then you can actually run that world to a certain extent\cb1 \
\cb5 right and that involves all the stimuli and and all the modalities uh if you\cb1 \
\cb5 want to look at it like that uh that we can perceive uh but also that we can uh\cb1 \
\cb5 interact with in the world and for the first time it's within reach to build a\cb1 \
\cb5 computer that can actually that has any chance of getting\cb1 \
\cb5 close to our ability not only to capture a piece of a perceived world and communicate say in language but also\cb1 \
\cb5 take some piece of language and then in a certain sense instantiate uh this kind of sliver of if you wish encapsulated\cb1 \
\cb5 world or so so are large language models uh just\cb1 \
\cb5 stochastic parrots piles of Statistics or do you believe they are emergent\cb1 \
\cb5 Learners that encode knowledge and understanding of the world are we just\cb1 \
\cb5 stochastic parrots or are we emergent Learners I don't know so basically I I\cb1 \
\cb5 think these distinctions are are almost meaningless as long as we don't specifically Define what we mean by\cb1 \
\cb5 intelligence what we mean by learning uh what we mean by parting um I don't think\cb1 \
\cb5 it's a particularly useful or many of those if not most of those distinctions\cb1 \
\cb5 and dividing lines are particularly actionable or insightful because we do not know how we learn we don't know if\cb1 \
\cb5 what we call understanding is in any fundamental way truly foundationally\cb1 \
\cb5 different from just grocking the statistics or just being a bit more a\cb1 \
\cb5 bit better at being predicting at predicting certain phenomena right for all we know our you\cb1 \
\cb5 know oh so sacred and oh so different learning uh abilities are just that and\cb1 \
\cb5 if that's the case then you know surely they are learning and they're Learners\cb1 \
\cb5 um and and so in a certain sense right\cb1 \
\cb5 um I'm I'm of of the opinion that I don't think there's a ghost in that\cb1 \
\cb5 machine but I highly doubt there's one in us too so or either or whichever it\cb1 \
\cb5 is but basically um I\cb1 \
\cb5 would on one hand I don't believe in the mechanistic interpretation of life\cb1 \
\cb5 because it's not a machine that was designed but evolved and as a result structurally it's very different but on\cb1 \
\cb5 the other hand I find it very difficult believe to believe any kind of you know\cb1 \
\cb5 Duality or dualistic view that implies that there is something other than something mechanistic that makes us that\cb1 \
\cb5 differentiates us from you know other machinery and uh as a result right it it\cb1 \
\cb5 could just be an experimential SL data problem right to get to get them to our\cb1 \
\cb5 level of in quotes understanding and no matter how you define understanding by the way I find it very difficult to believe that there is no understanding\cb1 \
\cb5 in these in these machines it's not the same as ours it can't be because they're their uh the breadth of stimuli that\cb1 \
\cb5 they're exposed to is just incre extremely limited even compared to ours but ours is also extremely limited\cb1 \
\cb5 compared to the world right and you see that very clearly when you look at applications of deep learning to problems such as protein structure\cb1 \
\cb5 prediction no human not even a field of scientific discipline biology over\cb1 \
\cb5 decades was able to produce uh uh an understanding of how protein folding\cb1 \
\cb5 happens how protein structure emerges that was anywhere close to being good enough uh to being applied in the real\cb1 \
\cb5 world and then you come along with this blackbox thing and enough data and enough pre-training and it basically\cb1 \
\cb5 just works now how well it works I think we still don't fully know but it works certainly better than anything humans\cb1 \
\cb5 ever did by hand that's for sure and so it is not anymore the case that\cb1 \
\cb5 basically their the breadth of uh data of stimuli that they perceive that the machines actually are able to tap into\cb1 \
\cb5 is just smaller than ours it's basically just different at this stage I would say both what we see what we're you know\cb1 \
\cb5 able to learn from and what the machines are able to learn from uh both of both of those distributions a few are still\cb1 \
\cb5 incredibly limited when you compare it to the wealth of information that's that's out\cb1 \
\cb5 there are llms a path towards artificial general intelligence I have no idea what\cb1 \
\cb5 artificial general intelligence is and so I do not know I can't for the life of me give you a good answer to that\cb1 \
\cb5 question actually that's a great answer\cb1 \
\cb5 um uh you mentioned you know you think you you you you you you um you're almost\cb1 \
\cb5 ready to move on to the next thing so where do you think uh the the next thing\cb1 \
\cb5 in AI is going to be so let's see um I mean for me\cb1 \
\cb5 personally um the next thing in AI is actually more applications of AI if you\cb1 \
\cb5 wish and I think in the past and in the future it is different applications that\cb1 \
\cb5 provide good forcing function or that yeah ultimately steer where new ideas uh\cb1 \
\cb5 uh where you know we we realize that the needs are unmet and where we'll then have to improve our our our tools um I\cb1 \
\cb5 think there's if you think about basically neural network architectures\cb1 \
\cb5 um there's some obvious shortcomings of what we have today one is the fact that you still have to take these signals\cb1 \
\cb5 chop them into pieces and then basically learn representations in some Vector space for each of these\cb1 \
\cb5 pieces but the problem is this chopping into pieces is something that we can do\cb1 \
\cb5 fairly well for signals that have been in a certain sense engineered or have evolved to have certain statistical\cb1 \
\cb5 properties such as language for genomic data that have not\cb1 \
\cb5 evolved you know genetics there was no reason for evolution to uh basically um evolve uh\cb1 \
\cb5 uh genetics in such a way uh that license that kind of tokenization or\cb1 \
\cb5 that chopping into pieces and then representing them and so and what I mean by that is that these pieces you can't have too many of them for our current uh\cb1 \
\cb5 methods our current methods like it when there's tens or hundreds of thousands of pieces not more than that um they don't\cb1 \
\cb5 like it when when they're too few um they also don't like it when there's still too many of these tokens although\cb1 \
\cb5 we're getting a lot better at that now we can do like you know hundreds of thousands or millions or so but we still\cb1 \
\cb5 have constraints around those things and and the fact that we need this chopping into pieces and that we can't learn that\cb1 \
\cb5 in a data driven manner is a massive uh uh impediment to applying these\cb1 \
\cb5 Technologies effectively another one is that basically today we would all agree\cb1 \
\cb5 there are some problems that are harder than others right for for usum I mean if I ask you what's 2 plus two uh you don't\cb1 \
\cb5 have any issue giving me a response to that if I ask you to you know I don't know find solutions to the shorting\cb1 \
\cb5 equation for some even simplistic system I way more way harder to do uh even\cb1 \
\cb5 though we actually understand how to do it like we understand the rules uh but um we you know to to really do execute\cb1 \
\cb5 the computation extremely extremely difficult to do yet you can formulate\cb1 \
\cb5 both of those queries in roughly the same length and the output might also even be okay maybe in 2 plus two the\cb1 \
\cb5 output is also incred incredibly simplistic but uh you can imagine situations in which the inputs and\cb1 \
\cb5 outputs have very similar shapes and yet the compute that something like uh you know gp4 or or uh\cb1 \
\cb5 command r or whatever would allocate in order to execute to in order to try to\cb1 \
\cb5 solve this uh problem in order to uh give you a good answer is the same\cb1 \
\cb5 because the input and output sizes are roughly the same and that just makes no sense right so every time uh a human\cb1 \
\cb5 will say oh I need some time to think about that that's not something our models can currently do and that's\cb1 \
\cb5 obviously broken so um right now basically we and we we are making\cb1 \
\cb5 progress there there's actually there's just very recently been a few really really interesting um pieces of work um\cb1 \
\cb5 mixtures of depths actually an interesting one I think in particular uh but there is still scratching just\cb1 \
\cb5 scratching the surface when it comes to Dynamic allocation of compute as a function of a the\cb1 \
\cb5 difficulty of the problem as opposed to the obvious apparent size of the input\cb1 \
\cb5 of the query and and the answer that's generated\cb1 \
\cb5 um and I think actually already those two things if if we really made\cb1 \
\cb5 substantial progress on them uh in connection or in the context of course of architectures that then really\cb1 \
\cb5 support that and are efficient in that context then we would already we would already have made massive of progress um\cb1 \
\cb5 and who knows I mean it seems like seems like we're getting there but it's uh you know\cb1 \
\cb5 these these things are usually not revolutions they look like them in hindsight and storytelling narrative\cb1 \
\cb5 wise you you know lots of folks want to make them into kind of these singular events that that somehow uh look like a\cb1 \
\cb5 revolution but the reality is as it happens it's just a ton of hard work building on a ton of hard work building\cb1 \
\cb5 on a ton of hard work and lots of people really trying their best and trying to be creative uh and so when you look at\cb1 \
\cb5 it like that then we're probably still in the beginning of really solving some of those um or addressing some of those\cb1 \
\cb5 taking a step back this entire conversation we've assumed that the\cb1 \
\cb5 hardware and I've said this a few times actually looks the same and if you think about it right now\cb1 \
\cb5 because this this's kind of a a co-evolution uh that's happening right we were joking the other day with Jensen\cb1 \
\cb5 wrong the CEO of Nvidia that you know we were saying well we built the Transformer to fit your gpus and he's\cb1 \
\cb5 like yeah we're building the gpus to fit your Transformers and both are true and so basically uh there is this\cb1 \
\cb5 co-evolution but it is not clear that this specific instance of co-evolution is going is is or rather I would say\cb1 \
\cb5 it's almost guaranteed that this specific co-evolution process is in a local Optimum and uh\cb1 \
\cb5 meaning it is very difficult at the moment to come up with a very very different model Paradigm because the\cb1 \
\cb5 existing model Paradigm has been engineered already now over some period of time to fit very well to this\cb1 \
\cb5 Hardware so even though it could be a departure from Transformers and God knows which ways dumping self attention\cb1 \
\cb5 uh replacing something with something that doesn't need tokenization would all be awesome it is actually not going to\cb1 \
\cb5 be that fundamentally different but also it is extremely difficult to come along and build a\cb1 \
\cb5 completely different chip completely different compute substrate uh for\cb1 \
\cb5 machine learning deep learning AI because well all the competitive models they're all uh you know based on and\cb1 \
\cb5 tailored to this Hardware why would it be interesting to come up with a different\cb1 \
\cb5 chip there is this if you look at the history of computing um and actually\cb1 \
\cb5 this is the perfect place to do this in the world I would say if you look at the history of computing um there was a switch\cb1 \
\cb5 somewhere basically leading up to the 20s and 30s of the 20th\cb1 \
\cb5 century the switch from analog to digital Computing and right if you look at later\cb1 \
\cb5 Lord kelvin's tied computers and and similar super powerful analog computers\cb1 \
\cb5 that were exactly single purpose and they had to be almost by definition in a certain sense um what is it here analog\cb1 \
\cb5 precisions some of these some of these old analog Computing devices\cb1 \
\cb5 um their issue was that you couldn't now apply them to even closely related other\cb1 \
\cb5 problems because these other problems would have to be uh addressed with\cb1 \
\cb5 sufficiently different algorithms or sufficiently different approaches that you actually had to dump out the entire computer and build it from scratch and\cb1 \
\cb5 that's really difficult to do and so digital Computing offered the promise of being General right now with uh you know\cb1 \
\cb5 in a single way we can actually Express and potentially address all\cb1 \
\cb5 computable uh everything that's computable we can actually do everything that's computable but what's the price that we pay the price that we pay is\cb1 \
\cb5 that we invest the majority of the energy that goes into computation into maintaining\cb1 \
\cb5 discrete States so if you if you just squint at a current computer at a digital computer today the vast majority\cb1 \
\cb5 of energy it consumes the vast majority of heat that comes out of it has to go\cb1 \
\cb5 uh and that energy has to be expended in order to maintain that some something\cb1 \
\cb5 that actually could be a scalar between zero and one actually or or more that now it stays either at zero or at\cb1 \
\cb5 one but now with deep learning and maybe with\cb1 \
\cb5 you know things like the transformer I'm not going to say the Transformer is it that would kind of be sad I would I\cb1 \
\cb5 would think but with something like you know the the successor the successor of the Transformer\cb1 \
\cb5 we seem to be in a position where actually we now have one algorithm that applies to many many many different\cb1 \
\cb5 relevant problems and it's always the same algorithm so why couldn't we now go and\cb1 \
\cb5 build an analog computer just for that one problem and that one problem is just run a Transformer or just run the\cb1 \
\cb5 Transformer successor successor then we train such a model that can actually\cb1 \
\cb5 train new such models we build this one analog computer for doing that and we're done we basically implemented an analog\cb1 \
\cb5 computer that can be way more power efficient for something like you know a\cb1 \
\cb5 complete algorithm an algorithm to learn new algorithms of the same computational\cb1 \
\cb5 shape that now are able to handle a very broad variety of problems that we really\cb1 \
\cb5 care about not all problems right not everything there's no completeness here or not anytime soon but at least that\cb1 \
\cb5 way I think we could increase thermodynamic effic by orders of magnitude not just one or two but many\cb1 \
\cb5 orders of magnitude potentially and so I do think that there's taking this big step back enormous potential for uh for\cb1 \
\cb5 for very different ways um of of of doing all this that are that are building on still what we've now learned\cb1 \
\cb5 through through uh you know this this this round of deep learning if you wish\cb1 \
\cb5 um anyway yeah okay what do you see is the greatest uh their greatest potential\cb1 \
\cb5 to benefit humanity and their greatest potential\cb1 \
\cb5 harm superlatives are always difficult um so let's see chatbots you mean uh\cb1 \
\cb5 systems that or or or some some kind of yeah systems that have language input\cb1 \
\cb5 and output um well I think in in in general and this may sound like a toy but in\cb1 \
\cb5 general the the biggest potential is that they obviate the need to formalize\cb1 \
\cb5 or to to uh structure information in a way that is palatable to human design\cb1 \
\cb5 machines you do not need to make a table in some way that is computer readable whatever that means you can basically\cb1 \
\cb5 just dump out the data uh and if it's a table or tabular in in a general way\cb1 \
\cb5 well you know maybe there's an exception maybe you have some place where in some row whatever the nth field is is uh\cb1 \
\cb5 contains a note that says ah yeah that was the value but actually I didn't really measure it that well and then take it with a grain of salt or some\cb1 \
\cb5 such and these things that were previously in a certain sense from a data perspective um incomprehensible to\cb1 \
\cb5 machines have now become totally manageable um and this doesn't hold only for data that is data but it also holds\cb1 \
\cb5 for data that is effectively algorithms so you can now basically basically have\cb1 \
\cb5 llms or what have you whatever you want to call these things that generate code and alongside with it natural language\cb1 \
\cb5 or sorry human language reasoning or plans that describe what the model in\cb1 \
\cb5 quotes thinks this code should be doing and now you can go and you edit the code and your English description changes you\cb1 \
\cb5 can edit your English description the code changes um you you can basically you know comment in totality whatever\cb1 \
\cb5 was generated there and that really marks a departure from all of these previously formulaic languages formula\cb1 \
\cb5 data description descriptions or data formats if you wish that is uh in a\cb1 \
\cb5 certain sense setting us free from the the shackles of our limited uh\cb1 \
\cb5 um of of the machines we of our previous machines limited uh uh generality if you\cb1 \
\cb5 wish and I think that's you know in the broadest sense the biggest potential the biggest harm or again I\cb1 \
\cb5 think yeah superlatives here are really tricky but one of the maybe\cb1 \
\cb5 interesting um downsides that this technology uh yeah comes with and it's I\cb1 \
\cb5 think inevitable we just have to think about ways of mitigating this um that's maybe underemphasized that's often\cb1 \
\cb5 underemphasized I believe is that we as societies as cultures have\cb1 \
\cb5 grown accustomed to the ability to use the difficulty of consuming but\cb1 \
\cb5 primarily also generating text as uh security tool right so for\cb1 \
\cb5 example um appeals in traffic court why doesn't everybody file appeal\cb1 \
\cb5 well because it's annoying it's difficult okay why is it difficult well you need to fill out a form\cb1 \
\cb5 okay that has become a push of a button then you need to mail it somewhere okay fine that has become a push of a button a long time ago so how about uh\cb1 \
\cb5 appealing some other decision how about affecting anything in any kind of basis\cb1 \
\cb5 Democratic fashion right these things are rate limited by the ability of\cb1 \
\cb5 generating or by by our individual ability of generating language but that rate limitation is gone right you can\cb1 \
\cb5 now generate language even conditioned on loads and loads of information at hundreds of tokens per\cb1 \
\cb5 second on a single GPU right uh or faster and so with with some of these super large\cb1 \
\cb5 models and as a result we really need to rethink a lot of mechanisms around\cb1 \
\cb5 Public Services um around uh all sorts of white\cb1 \
\cb5 collar crime um and it's not it's not at all that these that these machines or\cb1 \
\cb5 these systems you know will take initiative uh and and anytime soon uh\cb1 \
\cb5 and will harm us but they are amazing tools in the hands of malevolent actors\cb1 \
\cb5 so I think that actually um as as one of of a variety but as as as an often\cb1 \
\cb5 underemphasized example I think is is is one of the Practical dangers uh or downsides of the technology that we need\cb1 \
\cb5 to look into and that we need to mitigate um a lot of the times people talk about content identification or or\cb1 \
\cb5 generation you know identifying machine generated content I think that ship has sailed a long time ago we published a\cb1 \
\cb5 paper on water marking uh uh contents of of generative models we will never go\cb1 \
\cb5 back to any system like that uh uh to be effective for a variety of different reasons but um I think that's also\cb1 \
\cb5 something we need to confront there's a very simplistic alternative or very obvious alternative which is we need to authenticate content or uh basically\cb1 \
\cb5 certify that content was generated by human individual that's maybe another area uh related closely related area um\cb1 \
\cb5 but uh and it's not the same problem actually if you think it through but uh yeah and and the list goes on but it's\cb1 \
\cb5 it's problems like that uh that we need to think about and take care of so H how is how important are issues of trust\cb1 \
\cb5 to um future chatbot\cb1 \
\cb5 technology well um I mean it matters but at the end of the day it matters in a\cb1 \
\cb5 way and it also manifests increasingly in ways that are similar to how it manifests in\cb1 \
\cb5 humans so we've built as again societies and cultures we've we've built lots and lots\cb1 \
\cb5 of tools that help us build trust in other individuals and we need to build a\cb1 \
\cb5 similar Suite of tools and actually I think it's they're going to look very very similar uh that allow us to\cb1 \
\cb5 establish trust in in in machines and systems um we have to kiss goodbye once\cb1 \
\cb5 and for all the notion that we can do this in a way that is basically by\cb1 \
\cb5 Design or by construction uh there will not be as far as I'm concerned um a training objective\cb1 \
\cb5 of neural network architecture a data set of whatever combination of all those things that by Construction in an\cb1 \
\cb5 engineering manner uh we can engineer to be trustworthy just like I can't\cb1 \
\cb5 engineer a child to be trustworthy or a human then you know a person to be\cb1 \
\cb5 trustworthy we have to educate them certify them um recertify them on an\cb1 \
\cb5 ongoing basis uh and so forth and we have to understand better what\cb1 \
\cb5 situations are in which they might not be as trustworthy and then just deal with that right this I find it I find it\cb1 \
\cb5 sometimes almost comical when uh there are articles about hallucinations of\cb1 \
\cb5 large language models and uh they they uh in the Press it's often described as\cb1 \
\cb5 this I should I say almost uh this very surprising perplexing uh phenomenon when\cb1 \
\cb5 I go to my three-year-old and I ask her a question that she factually cannot possibly know she is going to totally\cb1 \
\cb5 hallucinate a great answer often times right in a in a shape or form that if I didn't actually know the fact I had no\cb1 \
\cb5 way of telling whether this was hallucination or not as long as it's as as the context of the question is close\cb1 \
\cb5 enough to uh you know something that her world model already covers well enough right I can ask her who the president of\cb1 \
\cb5 the US is she's going to give me alth although she mostly speaks German she's going to give me an American sounding\cb1 \
\cb5 name right U maybe maybe actually the current one she already knows but like the last last one she's still going to\cb1 \
\cb5 give me an American sounding name and so it's not at all surprising that these that these models do that there are\cb1 \
\cb5 certainly things we can do to improve that to mitigate some of it but there is not going to be a correct by\cb1 \
\cb5 construction um kind of a way of engineering them to be safe in any way\cb1 \
\cb5 comparable to say how we're trying to engineer planes to be safe right um even though I guess that also has a bit for\cb1 \
\cb5 but for similar reasons but uh yeah um thank you so um you know we\cb1 \
\cb5 touched on this you touched on this a little bit in your early answer but um there a lot there's a lot of discourse\cb1 \
\cb5 about about um both um the AI might lead us to apocalypse or\cb1 \
\cb5 Extinction um there's also on the other side um utopian discourse um what do you\cb1 \
\cb5 think of these\cb1 \
\cb5 conversations I think there um there is a time and a place for everything uh we\cb1 \
\cb5 should be having some of those conversations for sure I feel the extent\cb1 \
\cb5 the amount of energy and time and that's that's expended on some of them the\cb1 \
\cb5 amount of attention that some of them get is totally blown out of proportion and often actually fanned by\cb1 \
\cb5 players very often in the industry in the sector even to to further completely\cb1 \
\cb5 other objectives and you know say for example I I do believe that and this is\cb1 \
\cb5 completely this is actually an emergent phenomenon I don't think there's necessarily any malevolence here that's\cb1 \
\cb5 happening but I do think that it is quite convenient uh to just given psychology\cb1 \
\cb5 of of of of uh people in the media the masses Etc it's quite convenient to be\cb1 \
\cb5 able to focus the the discourse on say you know things like the risk of\cb1 \
\cb5 Extinction when you actually don't yet have an idea how to deal with some of these much more mundane problems like\cb1 \
\cb5 disruption potential disruption or a potential for disruption of civil services Public Services uh due to the\cb1 \
\cb5 fact that now everybody can produce like Tech at you know rapid rates or so and so yeah sure you know people are going\cb1 \
\cb5 to do what they learn uh you know removes resistance or eases\cb1 \
\cb5 resistance that they might otherwise encounter and I think that's what we're seeing to a large extent and so again\cb1 \
\cb5 don't get me wrong I think a lot of these conversations need to be had but\cb1 \
\cb5 there are many more potentially often more mundane and more pressing topics\cb1 \
\cb5 that I feel deserve some of that if not most of that airtime attention energy\cb1 \
\cb5 Etc um and that is a little bit unfortunate because that's really what\cb1 \
\cb5 actually could impede progress here and could harm people on the way um and so\cb1 \
\cb5 i' I you know in a certain sense I feel we should get better at uh uh yeah allocating resources when it\cb1 \
\cb5 comes to which of these and and this Cuts both ways but it goes both in the utopian as well as into the dystopian\cb1 \
\cb5 direction that these extremes right low probability High magnetude sure you know\cb1 \
\cb5 uh need there are things we need to talk about but I feel we need to spend much more energy on the high probability and\cb1 \
\cb5 much lower magnitude but High certainty uh space of of of outcomes right\cb1 \
\cb5 um what excites you most about\cb1 \
\cb5 chatbots again I think it depends a little bit on on you know what you call chat Bots I don't think chatbots by\cb1 \
\cb5 themselves are so exciting but uh I feel ultimately especially if you\cb1 \
\cb5 include things that can interact with lots of digital systems it it really comes back to to what we talked about\cb1 \
\cb5 earlier to this uh the to this generalized\cb1 \
\cb5 applicability of technology to problems in which the data and or the problem\cb1 \
\cb5 formulation SLS solution description algorithm uh are just difficult to\cb1 \
\cb5 squeeze into a traditional kind of machine uh uh suitable formats or format\cb1 \
\cb5 and so it's uh you know the fact that you can now deal with say for example\cb1 \
\cb5 language and uh and image content in a way that is almost algebraic right you\cb1 \
\cb5 can basically you can you can describe something in language and then you can apply that function if you wish to\cb1 \
\cb5 another piece of language in order to you know summarize a piece of text extract\cb1 \
\cb5 certain bits and pieces of then formal information um rewrite some of it etc etc those\cb1 \
\cb5 kinds of things uh I feel that's it's just\cb1 \
\cb5 very and it's less about the natural way of interacting with the machine I I I\cb1 \
\cb5 kind of don't care about how natural it is I care about how fast it is and how cheap it is uh in a in a thermodynamic\cb1 \
\cb5 sense efficient that we have some ways to go but really it's it's just the\cb1 \
\cb5 level of generality that that you now have it's it's uh um also yeah it's just\cb1 \
\cb5 offers the potential to deal with the world's information uh in in in in ways\cb1 \
\cb5 that just have absolutely been impossible before um do we need to explore\cb1 \
\cb5 alternative approaches to Ai and why alternative to What alternative to\cb1 \
\cb5 LMS again I mean alternative um to llms meaning or deep learning Maybe\cb1 \
\cb5 um I mean if you're basically thinking of things like symbolic approaches or\cb1 \
\cb5 neuros symbolic approaches I think the answer is in my mind what has worked uh better than any\cb1 \
\cb5 other uh lens through which to look at whether or not something could be effective is does it make it faster does\cb1 \
\cb5 it make it more thermodynamically more efficient and if the answer to that that is yes and that's that's quite\cb1 \
\cb5 conceivable right that doing certain things in a more symbolic manner um\cb1 \
\cb5 locally sometimes uh is is more efficient by all means let's do it\cb1 \
\cb5 but if if we can't basically show that that's the case uh then just for the\cb1 \
\cb5 sake of doing it differently because we think it might be\cb1 \
\cb5 philosophically better or you know um mesh better with certain theoretical or\cb1 \
\cb5 certain theories of of something why I don't know I wouldn't\cb1 \
\cb5 really yeah wouldn't understand the motivation okay um and you mentioned\cb1 \
\cb5 this therom dnamic thing so yeah um what are the what are the environmental you\cb1 \
\cb5 know uh consequences of of chat Bots and llms um and can we make it\cb1 \
\cb5 environmentally sustainable so there I think it's really important and in general I think this is actually I was tempted to say this uh also in the to\cb1 \
\cb5 the last question I I think the the chatbot llm distinction is a problematic one here because uh what I really\cb1 \
\cb5 believe is that deep learning overall uh very large models applied to you know a\cb1 \
\cb5 very broad range of different problems have the potential to drive our Energy Efficiency through the roof and so when\cb1 \
\cb5 it comes and this even just the allocation problems but but also um our\cb1 \
\cb5 ability to harness energy um all sorts of uh\cb1 \
\cb5 sustainable uh uh all sorts of sustainable energy um can be optimized\cb1 \
\cb5 in in just in some cases evidently massively in some other cases it's going to be uh it's going to be a little more\cb1 \
\cb5 difficult uh with these methods in ways that pay for the many many many times\cb1 \
\cb5 over that actually it would be to me shocking if you know some years into the\cb1 \
\cb5 future the fraction of our overall of the overall energy that is consumed by\cb1 \
\cb5 these large models hasn't gotten very very large because it is it is to a\cb1 \
\cb5 large extent exactly those models that will make it much much easier and much more effective actually uh and much more\cb1 \
\cb5 sustainable um to uh in in all sorts of different ways actually\cb1 \
\cb5 uh yeah ultimately generate or extract if you wish the energy from uh from the\cb1 \
\cb5 system okay um final question can you talk about your current Venture\cb1 \
\cb5 sure so uh at inceptive uh what we're doing is\cb1 \
\cb5 ultimately building models uh and this isn't just a\cb1 \
\cb5 AI or a dry Affair this involves an enormous amount of work uh experimental\cb1 \
\cb5 work in a wet lab both to validate methods uh but also to generate training data but ultimately to build methods uh\cb1 \
\cb5 or models that allow us to design molecules that uh once in the context of\cb1 \
\cb5 say uh you're and my cells um exhibit certain pretty specific and and\cb1 \
\cb5 increasingly broad functions um and so basically uh I guess one way of of\cb1 \
\cb5 reformulating of rephrasing that is uh we're designing medicines with artificial intelligence but um we're\cb1 \
\cb5 looking at a particular angle of doing this where what we're starting with is\cb1 \
\cb5 mRNA and soon RNA molecules um that\cb1 \
\cb5 actually are in a certain sense life complete if you wish right so there's the RNA World hypothesis that everything\cb1 \
\cb5 in life can actually be traced back or started with uh RNA molecules it's not\cb1 \
\cb5 clear if that's really and we might never know how life really or or where life really originated but it's enough\cb1 \
\cb5 to know that it could have been like this to then basically um\cb1 \
\cb5 conclude that if we could only design RNA molecules\cb1 \
\cb5 optimally which is an incredibly challenging task we could affect all sorts of\cb1 \
\cb5 different interventions in uh in ourselves in these very complex organisms now that's definitely science\cb1 \
\cb5 fiction but maybe we can start with a uh with a small set of functions um such as\cb1 \
\cb5 mRNA printing some protein uh such as you know maybe detecting the presence of\cb1 \
\cb5 some small molecule or some specific macromolecule in the vicinity\cb1 \
\cb5 and expressing one protein or the other whether or not based on whether or not\cb1 \
\cb5 it's actually present um such as um\cb1 \
\cb5 um self-amplifying or self-replicating itself which if you wish looks a lot\cb1 \
\cb5 like uh uh like recurrence or or recursion actually if we just have some\cb1 \
\cb5 of these simple functions can we learn how to design rnas that uh exhibit you\cb1 \
\cb5 know any combination of such functions uh inside our cells um in ways that also\cb1 \
\cb5 have uh you know manageable or the desired uh consequences when it comes to\cb1 \
\cb5 innate immune responses or or or similar phenomena and thus really um enable\cb1 \
\cb5 medicines that are programmable to an extent uh that goes Way Beyond what\cb1 \
\cb5 we're able to do uh say certainly using small molecule medicines um but also for\cb1 \
\cb5 more mundane reason uh reasons uh using protein biologics where um you know in\cb1 \
\cb5 contrast to those uh RNA molecules are incredibly easy to manufacture we can in\cb1 \
\cb5 our lab actually synthesize basically more or less any possible RNA the same\cb1 \
\cb5 cannot def certainly cannot be said for for proteins it's much harder to do much harder to do at scale um much harder to\cb1 \
\cb5 do uh you know if you want to create a or synthesize a broad variety of them um\cb1 \
\cb5 and so um at the end of the day uh you know\cb1 \
\cb5 basically finding or um enabling one\cb1 \
\cb5 type of substrate if you wish in this case RNA as as one family of macro\cb1 \
\cb5 molecules that can still ultimately when designed just right um affect the kinds\cb1 \
\cb5 of interventions that constitute you know the vast majority uh maybe even of\cb1 \
\cb5 all existing medicines and many medicines that we would like to exist um\cb1 \
\cb5 and uh enabling that using machine learning because really the or or deep learning AI at scale because one thing\cb1 \
\cb5 is clear at least to to us um we will never\cb1 \
\cb5 even as you know if you wish as Humanity we will never learn conceptually how to\cb1 \
\cb5 do that ourselves um and and this is ultimately\cb1 \
\cb5 because so our only our only alternative is to just learn it in in the sense of\cb1 \
\cb5 uh or or through blackbox methods such as such as deep learning um and you can think of this just as a strict\cb1 \
\cb5 generalization of the problem of uh language understanding or or more precisely\cb1 \
\cb5 generating human language where you know in human language at least we have an existence proof of entities like you and\cb1 \
\cb5 me that are able to do it but we've tried for decades now to understand how we do it and what the rules are that\cb1 \
\cb5 govern it we failed uh to understand that to conceptualize that to build a theory around that and instead we've had\cb1 \
\cb5 to apply these datadriven blackbox mechanisms that are now actually able to do it to a pretty large uh extent both\cb1 \
\cb5 understand and generate uh human languages\cb1 \
\cb5 um and now in case of you know effectively learning life's languages uh which is you know at least\cb1 \
\cb5 one of the at first languages uh say RNA that that that is the one that we want to learn first here we don't even have\cb1 \
\cb5 an existence proof of any kind of system or organism that is able to design these things and to comprehend how they work\cb1 \
\cb5 and so there's even less of an of a hope of of basically um being able to design\cb1 \
\cb5 them effectively using or based on Theory based on conceptual understandings of this um uh and uh as a\cb1 \
\cb5 result an even greater Reliance on doing this with methods like AI um now we're\cb1 \
\cb5 starting in uh in in I would say you know the humble beginnings on on\cb1 \
\cb5 that uh almost certainly very long journey are that we would like to design\cb1 \
\cb5 uh mRNA molecules with um much higher chemical stability than say for example\cb1 \
\cb5 the uh mRNA code vaccines um that are actually as a result uh accessible to a far greater\cb1 \
\cb5 number of people right not just say two billion people uh but but maybe two\cb1 \
\cb5 three times that because they don't require distribution using a deep Frozen cold chain um but also mRNA molecules\cb1 \
\cb5 that have U much higher and potentially carefully modulated also over time\cb1 \
\cb5 protein expression um subpar protein expression or two insufficient proin expression is\cb1 \
\cb5 basically what prevents many mRNA medicines uh from actually being practical today either because you would\cb1 \
\cb5 have to administer so much mRNA that you're going to get unwanted side effects or adverse uh effects or uh\cb1 \
\cb5 because you ultimately end up um only expressing uh or not uh triggering\cb1 \
\cb5 certain uh uh certain Pathways or certain phenomena in life uh if the if the protein does isn't isn't expressed\cb1 \
\cb5 at at certain minimum rates so ultimately you could probably enable um a a pretty broad range of um\cb1 \
\cb5 mRNA medicines that we can't actually practically uh produce and and uh\cb1 \
\cb5 manufactured today or designed today but also um democratize access to them uh in\cb1 \
\cb5 a in a pretty meaningful way and then expand the range of such properties and functions uh Beyond you know protein\cb1 \
\cb5 expression and chemical stability as as we go and then eventually including things like uh recursion uh\cb1 \
\cb5 self-amplification or replication um conditionals reacting on you know the\cb1 \
\cb5 environment that uh that in in those cells that these molecules end up in etc\cb1 \
\cb5 etc all right thank you\cb1 \
\pard\pardeftab720\partightenfactor0

\f1\fs24 \cf0 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf7 \strokec7 \
\pard\pardeftab720\partightenfactor0
\cf0 \strokec4 \
\
\
\
\
\pard\pardeftab720\partightenfactor0

\f0\fs20 \cf0 \cb8 \strokec4 \
\pard\pardeftab720\qc\partightenfactor0

\f2\fs22 \cf0 \cb1 \strokec4 \
}