{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 ArialMT;}
{\colortbl;\red255\green255\blue255;\red237\green237\blue237;\red25\green25\blue25;\red255\green255\blue255;
}
{\*\expandedcolortbl;;\cssrgb\c94510\c94510\c94510;\cssrgb\c12941\c12941\c12941;\cssrgb\c100000\c100000\c100000\c10196;
}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28\fsmilli14400 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 [Applause] I had I had so much fun talking with\cb1 \
\cb3 with so many of you here today. Um and you know we need a conference on\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 reinforcement. It's it's clear. I wasn't sure but now I think it's clear. This was the right move. Um\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 so I have prepared for you a talk uh on the oak architecture. It's a vision of\cb1 \
\cb3 super intelligence from experience. It really is my attempt to um\cb1 \
\cb3 address the issues at the center of of AI. And I want to start just by\cb1 \
\cb3 recognizing how um how difficult and important is the task\cb1 \
\cb3 of AI. Um so\cb1 \
\cb3 AI AI is a grand quest. Uh we're trying to understand how people work. We're\cb1 \
\cb3 trying to make people. We're trying to make ourselves powerful. Um and this is a profound intellectual\cb1 \
\cb3 milestone. It's going to change everything. You know this, but it's good to take a moment and pause and recognize what\cb1 \
\cb3 we're doing is incredibly hard, incredibly important. as an intellectual milestone. I think it'll be comparable\cb1 \
\cb3 to the origin of life on the earth at least when we under when we when some\cb1 \
\cb3 part of the of the of the planet understands how it works and how it\cb1 \
\cb3 thinks and how it uh can be so uh transformative to our to the to the\cb1 \
\cb3 to the planet. Okay. But it's also a continuation of things that what we've\cb1 \
\cb3 always done and it's just the next big step. So now myself I um I think this is\cb1 \
\cb3 just going to be good. Lots of people are worried about it. I think it's it's going to be good. It's an unalloyed good\cb1 \
\cb3 and I think the greatest advances are still ahead of us. It's a marathon.\cb1 \
\cb3 I think you know good for this group that the path to full AI strong AI runs\cb1 \
\cb3 through reinforcement learning and not I think through things like uh to non-experential things like large\cb1 \
\cb3 language models. The biggest bottleneck is strangely is we have inadequate learning algorithms.\cb1 \
\cb3 You may think we have our deep learning and we that's the one thing we know but I think it's not like that at all. I\cb1 \
\cb3 think it's it's more like we they are very our algorithms are very crude. they need to be they need to be better and\cb1 \
\cb3 that is what we should be working on. Okay. So, and then myself\cb1 \
\cb3 uh I've tried to think deeply about this intelligence for half a century. Every\cb1 \
\cb3 day I'm sort of in the trenches designing algorithms, trying to design algorithms, seeking better algorithms\cb1 \
\cb3 for reinforcement learning, for learning from experience. And I follow this Alberta plan for AI research which you\cb1 \
\cb3 may know about. Mike and Patrick and I did it a couple years ago. And today I'm going to talk about the vision of an\cb1 \
\cb3 overall agent architecture AI agent architecture called oak and I think it provides a line of sight towards towards\cb1 \
\cb3 our grand prize of understanding mind. Okay. So those are my my introductory\cb1 \
\cb3 comments. Now let's talk about oak. I think it's fun just to start with the name so you which may be a mystery oak.\cb1 \
\cb3 It comes from the idea of options and knowledge. Now, as many of you are very\cb1 \
\cb3 familiar with, an option is a pair. Well, actually, you may think it's a triple. I sort of have dropped the\cb1 \
\cb3 initiation set in in in my for many last couple decades. Um, so for me, it's a\cb1 \
\cb3 pair of a policy, a way of behaving, and a way of deciding to stop behaving in\cb1 \
\cb3 the Okay, so just those two, just a pair. And in oak the agent has lots of\cb1 \
\cb3 options and it's going to learn its knowledge will be about what happens when you follow the option. So in this\cb1 \
\cb3 way the agent is meant to learn a high level transition model of the world that enables planning with larger jumps and\cb1 \
\cb3 hopefully carves the world at its joints. Um yeah so that's that's uh where the\cb1 \
\cb3 name is from. I think it's is a grand challenge and a grand quest. And so I\cb1 \
\cb3 show it like this that we are seeking the holy grail. The holy grail of AI.\cb1 \
\cb3 Let me put that up in a way it's easier to read. Uh we want this\cb1 \
\cb3 we want this AI design that is domain general contains I'm going to say\cb1 \
\cb3 nothing specific to the world. We want a general idea. So I'll talk about that\cb1 \
\cb3 more in a minute. But let me just put down my three main uh design goals. It\cb1 \
\cb3 should be domain general. It should be experential. That is the mind should grow from runtime experience not from a\cb1 \
\cb3 special training phase. And third it should be open-ended in its sophistication in its abstractions.\cb1 \
\cb3 So that um it's a it can it can form any any concepts in its mind that are needed\cb1 \
\cb3 to deal with whatever world it's connected to limited only by its computational resources. So those are\cb1 \
\cb3 the three main deterata and we'll talk about them. I guess first\cb1 \
\cb3 I want to establish some words. I want to talk about I'm going to talk about design time and runtime. Design time and\cb1 \
\cb3 runtime. when you're in the factory being designed then your robot goes out and lives its world that's its runtime\cb1 \
\cb3 sort of a lot a lot the way Leslie spoke um so the age is designed s at at design\cb1 \
\cb3 time we would be building in any domain knowledge that we might have\cb1 \
\cb3 farther away from the chest might be\cb1 \
\cb3 just pardon me.\cb1 \
\cb3 Okay, cool. At design time is when you're building\cb1 \
\cb3 in any of your domain knowledge and then at runtime is when you're actually interacting with the world. Learning\cb1 \
\cb3 from experience, making plans that are specific to the part of the world you're in. So like uh I don't know a large\cb1 \
\cb3 language model everything is done as design time and when it goes out to be\cb1 \
\cb3 used in the world it doesn't do anything. My emphasis is going to be the other way around. We want I want to do\cb1 \
\cb3 all the important things at runtime online on the job.\cb1 \
\cb3 And in a minute I'll be talking about a big world but a big the idea of a big world a big complex world is that you're\cb1 \
\cb3 you're not going to be able to build things in it. You're not going to be have your agent know everything about\cb1 \
\cb3 the world in the factory because the world is huge and you can't know everything. Your a your robot is small\cb1 \
\cb3 compared to the world. And uh so if you want to be able to learn arbitrary\cb1 \
\cb3 open-ended abstractions um you need the world to find out whether the right abstractions for for the part of the\cb1 \
\cb3 world you're uh running into. So you got to do it at runtime. So I don't know. So\cb1 \
\cb3 I'm going to talk about this a lot. runtime. Everything has to be done at runtime and why that's actually a good\cb1 \
\cb3 way to think about it. Okay, so let's just ask the question. Yeah, should an\cb1 \
\cb3 agent's design this is a question for you guys, right?\cb1 \
\cb3 Should the agents design reflect the world in which it's expected to be used?\cb1 \
\cb3 Good thing about this question is both answers are\cb1 \
\cb3 wrong. I mean both answers are right. So you\cb1 \
\cb3 can um yeah if you want something to uh perform well and you want to put it out\cb1 \
\cb3 there and have it do something good right away, you know, you want to you want to put design domain knowledge in\cb1 \
\cb3 there. You want it to reflect the world. But if you want a good design, so I'm\cb1 \
\cb3 gonna say no. My quest, my quest is that the design should not depend on the\cb1 \
\cb3 world at all. Okay. Uh it should be domain general. Now\cb1 \
\cb3 this is really just you know there there are multiple questions and there are multiple uses and like you know I I I\cb1 \
\cb3 have to respect totally you know someone who wants to do an application make something that's useful make something\cb1 \
\cb3 that performs well sure that's important but also important is let's understand\cb1 \
\cb3 the mind in a simple way let's let's what we what we would want is a a\cb1 \
\cb3 conceptually simple understanding of what's going on inside a mind. That's\cb1 \
\cb3 that is in some sense the grand quest of of AI is to understand what it means to\cb1 \
\cb3 achieve goals, what it means to understand an arbitrary world. It's it should be simple and if you the world\cb1 \
\cb3 the actual domain that you're going to interact with is arbitrarily complex and so your goal your job your agent's job\cb1 \
\cb3 is to go out there and learn all the fiddly wonky little details and highle\cb1 \
\cb3 structures of the world that it's going to encounter. But you really don't want to understand what it's doing at that in\cb1 \
\cb3 terms of all those domain specific details. You want a level of understanding that is at a higher level\cb1 \
\cb3 and is based on principles and not the intricate complex details that that are\cb1 \
\cb3 in the world. So it's sort of different purposes. Someone who wants to go have something\cb1 \
\cb3 that will perform really well right away or someone who wants to have a conceptual understanding of what a mind\cb1 \
\cb3 is and how intelligence works. So my quest is to do the latter. And so it's only natural that uh that we would want\cb1 \
\cb3 to exclude those things. So we of course I have to reference the bitter lesson at\cb1 \
\cb3 this point. Um and I'll just read this. The actual contents of minds\cb1 \
\cb3 are part of the arbitrary intrinsically complex outside world.\cb1 \
\cb3 They are not what should be built in as their complexity is endless genuinely endless. our our agent has a\cb1 \
\cb3 big computer and we don't want to understand everything going on in there. Instead, we should build in only the\cb1 \
\cb3 meta methods that can find and capture this arbitrary complexity.\cb1 \
\cb3 In short, we want agents that can discover like we can, not which contain what we have already discovered.\cb1 \
\cb3 So that's the idea for the purpose here in a scientific conference trying to understand what a mind is, how it should\cb1 \
\cb3 work. We want we want a level of description that is above all those domain specific uh endless details.\cb1 \
\cb3 Okay. Now second question, should the agent learn from special training data\cb1 \
\cb3 or should it learn only from runtime experience?\cb1 \
\cb3 only. You must be triggering a little bit on the word only.\cb1 \
\cb3 Only from runtime experience.\cb1 \
\cb3 Well, for me the agent should learn only from runtime experience\cb1 \
\cb3 should be entirely experential. And the reason is again um\cb1 \
\cb3 we want a conceptually simple design and if it's possible uh to learn only at\cb1 \
\cb3 runtime that would be a simpler a simpler understanding and um\cb1 \
\cb3 so we should seek this. So the form of the argument is going to be there's certain things that that have to be done\cb1 \
\cb3 at runtime and and but and could be done at design time and uh so basically\cb1 \
\cb3 everything has to be done at runtime maybe also at design time but it ha at\cb1 \
\cb3 at runtime it ha you have to be able to learn you have to be able to change your abstractions you have to um be able to\cb1 \
\cb3 make a model of the world you have to be able to plan with that model every all those things have to be done at runtime\cb1 \
\cb3 because you're going to encounter the world. The world might not be like what you expected and certainly you won't\cb1 \
\cb3 know all the intricate details and all the abstractions that are needed for for the part of the world that you're interacting with. So all those things\cb1 \
\cb3 have to be done at runtime. Now you could also do some of them at design time but it's sort of in some sense that\cb1 \
\cb3 would be optional. You know, that might speed you up, but since they all have to you have to be capable of being done at\cb1 \
\cb3 at runtime, why not make a conceptually simple design and just doesn't worry about trying to get a head start on the\cb1 \
\cb3 problem, but just has the uh the u runtime aspects. And so that's what\cb1 \
\cb3 that's what my quest is. That's the that's the the uh journey I'm the quest I'm setting out on. And I'm hoping\cb1 \
\cb3 you're going to join me. Okay. So I've talked about the big world perspective.\cb1 \
\cb3 So let's let's do that. Let's talk let's make let's gain a common knowledge amongst us\cb1 \
\cb3 of what that means. The big world perspective or the big world hypothesis something that's been floating around at\cb1 \
\cb3 least in Alberta for like five years and we've all come comfortable with it. Um and it's really influences all of our\cb1 \
\cb3 thoughts and our designs. So the idea is simply that the world is bigger, more\cb1 \
\cb3 complex than the agent. Uh and it's much bigger really. It's bigger than this. It's it's big, you know, it's really\cb1 \
\cb3 big. And it's got to be much much bigger than the agent because the world contains,\cb1 \
\cb3 you know, billions of other agents and um and all of course all the atoms and\cb1 \
\cb3 all the intricacies of uh the objects spread around. um the world is much more\cb1 \
\cb3 big and and the world contains what what is happening in all those other agents\cb1 \
\cb3 matters to you. It matters to you what's going on in the minds of your your friends and your loved ones and your\cb1 \
\cb3 enemies. Um all all those things are important to you and they have to be taken into\cb1 \
\cb3 consideration. And so you know the upshot is that nothing you the agent is\cb1 \
\cb3 going to be doing is going to be exact and it's so and it's not going to be optimal. It's going to be approximate\cb1 \
\cb3 when you make a value function. It's going to be of course be an approximate value function. Uh and your policy will be will not be the optimal policy. Your\cb1 \
\cb3 transition models will be much enormous enormous reductions. They're sitting\cb1 \
\cb3 inside your head. You know, your model of the world and the world is out there much much bigger, right? Um\cb1 \
\cb3 even a single state of the world, you can never hold it in your head. You can never hold in your head all the the\cb1 \
\cb3 states in everyone else's minds. So, one of the most important\cb1 \
\cb3 consequences of this is that the world ends up appearing non-stationary.\cb1 \
\cb3 Uh, and I'm citing a little paper on this that Dave Silver and and a coupe\cb1 \
\cb3 and I wrote where we just made this point. The world, you can probably see\cb1 \
\cb3 it. the if you don't have a model, you don't have a good sense of the state of the world and it's it's big and\cb1 \
\cb3 sometimes you're there, sometimes you're there, things look the same, your function approximator cannot capture everything, the world's going to look\cb1 \
\cb3 non-stationary and so you have to learn at runtime because\cb1 \
\cb3 you're going to you can't you can't have built in everything about the the whole big world at design time. You have to\cb1 \
\cb3 learn at runtime. you're going to encounter some particular part of the of the world at runtime and you want to uh\cb1 \
\cb3 customize and be appropriate for that part. You know, you go to you go to work, you're you're you're AI agent\cb1 \
\cb3 that's supposed to be play a productive role in society and it goes to work and it meets its co-workers. It has to\cb1 \
\cb3 remember the name of the guy it's working with. You know, that was not in the domain knowledge. It has to remember uh the work that they've done on that\cb1 \
\cb3 project. What's working well? What's not working well? What what are they trying to do? All those you know everything you\cb1 \
\cb3 do in your life uh is is is un could not have been foreseen\cb1 \
\cb3 and to to um so I know it should be obvious you have\cb1 \
\cb3 to learn during your life. You have to plan during your life. I'm\cb1 \
\cb3 Leslie Caling made this point that planning is required because the world is big.\cb1 \
\cb3 Um, and so this also applies to your abstractions. You know, my third\cb1 \
\cb3 desitter is that we want an open-ended abstraction. We want to be get more and more sophisticated understanding of this\cb1 \
\cb3 particular world. We have to find, you know, what's the right joints and ideas that are involved\cb1 \
\cb3 in the world that we're encountering. And so, um, yeah, you you may be able to also\cb1 \
\cb3 add abstractions. Maybe you believe in objects like Leslie and and you want to build that in, but\cb1 \
\cb3 that doesn't get you out of having to to have the ability to create new abstractions at runtime. And so, if\cb1 \
\cb3 you're going to have to create them at runtime, you know, why don't we just do it in one place and and and\cb1 \
\cb3 uh that'll be a very good start. You know, I think of a design for an AI, the\cb1 \
\cb3 perfect design. It would not be a huge thing. It would not be like an encyclopedia or a library worth of\cb1 \
\cb3 knowledge. It would be like uh well, when I make for me actually it\cb1 \
\cb3 almost fits on a slide. You write in the pseudo code, you know, maybe it's three slides. Okay. I thought I think\cb1 \
\cb3 something of that that order. five pages for your uh description of\cb1 \
\cb3 all the essential elements that are domain independent and and and and yet\cb1 \
\cb3 are capable of arbitrary um open-ended abstractions.\cb1 \
\cb3 Okay. Um yeah, so this talk like hey um\cb1 \
\cb3 I was up till like the like four o'clock last night making making these slides.\cb1 \
\cb3 Um this is this is um this is sort of a new talk for me. This\cb1 \
\cb3 is the first time you're the you're the first guys hearing it. I've been I've been traveling around the world and giving all these uh philosophical talks\cb1 \
\cb3 and political talks and point of view talks and that's great and I've enjoyed that but but uh you know I really\cb1 \
\cb3 thought here I'm I'm at the RLC conference reinforcement learning I should do something uh substantive\cb1 \
\cb3 maybe even technical um so so I um\cb1 \
\cb3 and then all during the week you know I went to every I went to all these talks I talked to all these people And uh I I\cb1 \
\cb3 keep just changing my mind about what I want to say. And uh so anyway, this is all my way of of excusing myself or\cb1 \
\cb3 explaining to you that that this talk is new and it's not quite polished. In in\cb1 \
\cb3 particular, I might say some things more than once. Um so and maybe it's okay to\cb1 \
\cb3 have some repetition if they're important things. Um okay, but just be aware of that. and maybe maybe kick me\cb1 \
\cb3 if I if I uh need to move on to the next thing. Okay, I'll try to be quick.\cb1 \
\cb3 Runtime learning I think always wins over design time because the the world is much bigger than the agent, the big\cb1 \
\cb3 world perspective. Um design time can't cover every case. Runtime learning can customize to the\cb1 \
\cb3 part of the world actually encountered. Runtime learning scales with available\cb1 \
\cb3 compute whereas design time learning or anything done at design time scales with the\cb1 \
\cb3 available human expertise at design time. It was the only thing available\cb1 \
\cb3 and historically scaling with compute wins in the long run. That's what the bitter lesson is is explicitly about.\cb1 \
\cb3 However, today's deep learning methods, runtime deep learning methods, continual\cb1 \
\cb3 learning, they don't work very well. Okay, this is this is a a a big bitter thing\cb1 \
\cb3 for me. I wish they could worked well because I'm talking all about runtime learning and I want to use it.\cb1 \
\cb3 Um yeah, if we if we one last thing about runtime\cb1 \
\cb3 learning, it does enable metalarning. Metal metal learning is where you like try learning one way and then you try\cb1 \
\cb3 learning another way and you notice that oh this way works better in the future I will do this. If you were if you were\cb1 \
\cb3 doing everything in one shot you couldn't do that. This idea of becoming\cb1 \
\cb3 better at learning requires uh one time you're doing learning another time\cb1 \
\cb3 you're trying a different way of learning and you pick the better one. So metalarning really requires this to be\cb1 \
\cb3 done at the at the runtime. Okay. Now\cb1 \
\cb3 let's think about the problem just a little bit more. You know I like to separate things into the problem and the solution. Really almost everything I've\cb1 \
\cb3 been talking to you about is the the problem the quest. What are our goals? What are our desitter?\cb1 \
\cb3 So um one more slide on that. The AI problem is to design an effective\cb1 \
\cb3 purposeful agent that acts in the world. And the classic reinforcement problem is\cb1 \
\cb3 the same thing except we add the purpose is specified by scalar reward signal.\cb1 \
\cb3 The reward and the world is general and incompletely known. But the world can be\cb1 \
\cb3 anything could be grid world or the human world can be stochastic, complex, nonlinear, non-marov.\cb1 \
\cb3 The state space of the big world is effectively infinite and its dynamics\cb1 \
\cb3 are effectively non-stationary. Um let me go ahead to just talk about\cb1 \
\cb3 this one a little bit further. The purpose is specified by a scalar signal.\cb1 \
\cb3 So that's we have a name for this idea. It's called the reward hypothesis. And I\cb1 \
\cb3 I wanted to bring this up because we have thought about it and uh it's not like a quick choice without intention.\cb1 \
\cb3 Um so the reward hypothesis is this that all of what we mean by\cb1 \
\cb3 goals and purposes can be well thought of as the maximization of the expected\cb1 \
\cb3 value of the cumulative sum of a received scalar signal called reward.\cb1 \
\cb3 Um, so there's lots of specific things there like the expectation, like the cumulative sum. Um, yeah, and that's\cb1 \
\cb3 been thought through. And the SC the idea of a scaler reward, I just want to say it's not just a uh something we\cb1 \
\cb3 haven't thought about. In fact, it's a it's a great thing. It's a really clear way to specify the goal. It's become\cb1 \
\cb3 popular in many different disciplines, not just AI, but also economics and psychology, control theory, and forever\cb1 \
\cb3 people have been trying to modify. They've been trying to add things like constraints, multiple objectives, risk\cb1 \
\cb3 sensitivity, and and and I I I\cb1 \
\cb3 don't know. I I hate that. I mean, even if it was good to do, I I I\cb1 \
\cb3 I don't want it to be done. I don't want it to be true. I I you I think you've already gotten the sense. I like things\cb1 \
\cb3 to be simple. You know, that's like a really high high uh uh deserata\cb1 \
\cb3 desire. I want things to be simple and I might even simplify them a little bit too far in order to uh to uh be clear\cb1 \
\cb3 and uh so anyway, I want things to be simple. Do we need to do we need all these things to get generality? That's\cb1 \
\cb3 the real question. and Michael Bowling and others have have written this really\cb1 \
\cb3 nice paper called settling the reward hypothesis where they go through all these cases and and I don't know if I\cb1 \
\cb3 want to uh they they they establish that in a\cb1 \
\cb3 certain sense the reward hypothesis is correct that that you don't um add\cb1 \
\cb3 generality um uh by adding multiple objectives or risk sensitivity or any of\cb1 \
\cb3 these constraints. So they it's one way of validating that choice and you might also probably know\cb1 \
\cb3 the reward is enough paper where we argue that even a simple reward can lead to all the attributes of intelligence in\cb1 \
\cb3 a sufficiently complex world. Okay. So\cb1 \
\cb3 um now I want to talk about the solution methods the architectures\cb1 \
\cb3 obvious starting place is model free reinforcement learning basic reinforcement learning where the agent\cb1 \
\cb3 constructs a policy and value function at runtime\cb1 \
\cb3 both these are functions all all these are runtime RL architectures okay the\cb1 \
\cb3 model free and then you can handle the non-markoff case if you uh construct\cb1 \
\cb3 your feature uh state representation from u uh from from your data and I'll\cb1 \
\cb3 show you the the picture of that in a second. Uh but still better uh would be to make a model of the world and use\cb1 \
\cb3 that model to plan with potentially better. Now the oak architecture is\cb1 \
\cb3 along the same line um of improvement of of extension and the the thing about the\cb1 \
\cb3 oak architecture is it it adds to those things u auxiliary\cb1 \
\cb3 problems subpros and those sub problems are in the form of attaining individual\cb1 \
\cb3 features individual state features and in this way uh we enable the discovery\cb1 \
\cb3 of higher and higher uh levels of abstraction and we we achieve this open-ended uh\cb1 \
\cb3 goal. Okay. So, as a in a picture, this this picture on the on the left is from\cb1 \
\cb3 uh the textbook, the reinforcement learning textbook. Um and this is the very last figure in the book. And so\cb1 \
\cb3 maybe it's familiar to some of you. Uh we have uh the world and then the agent\cb1 \
\cb3 is everything above the world. It's the policy and value functions. It's a model of the world. It's a planner. And it's\cb1 \
\cb3 the Ubox. The viewbox. I want you to notice because this is a is a um kind of\cb1 \
\cb3 reinforcement learning where we don't assume that the state is available to the agent. We only observations are\cb1 \
\cb3 available to the agent. This we send actions to the world. The world sends back observations and and a reward.\cb1 \
\cb3 Okay. And then there's a a process here that that's a part of the agent uh that\cb1 \
\cb3 that computes uh something that we'll use as a state representation uh by the\cb1 \
\cb3 by the policy and value function. Okay. So that's the construction process and\cb1 \
\cb3 um nowadays I draw um things uh a bit\cb1 \
\cb3 differently. Um and this the right figure is the full oak architecture and\cb1 \
\cb3 you see the many of the same components. This U box is now called perception. Perception is a better name for it\cb1 \
\cb3 because what does it do? This perception process it takes in the uh the data\cb1 \
\cb3 that's happening the actions and the observations and it forms a a sense of where the agent is now. That's really\cb1 \
\cb3 what perception is about. take in your sensory input, get a sense of where you are now. Use that sense to make your\cb1 \
\cb3 decisions to to as input to your policies, your value functions and your models.\cb1 \
\cb3 Okay, so the oak architecture has all those things, but it also adds auxiliary subpros and those each subpro will have\cb1 \
\cb3 its own value function and its own policy. Uh so that's what's suggested by\cb1 \
\cb3 having uh these u shadow policies behind the the main policy and these secondary\cb1 \
\cb3 auxiliary value functions behind the main value function. Um\cb1 \
\cb3 also each one of these subpros is going to be based on a different component of the state feature representation. So\cb1 \
\cb3 this is the thing that acts like state and I want you to think of it as a feature vector and each one of these sub\cb1 \
\cb3 problems is going to be based on a different component of that feature vector. So that's what it looks like as\cb1 \
\cb3 a picture. Okay. Uh now we're going to get into the a bit of the nitty-gritty.\cb1 \
\cb3 We're going to look at it. I've given all these these quick introductions to the idea tell you I've told you various\cb1 \
\cb3 properties of the oak architecture. Now I want to tell you exactly what it is or\cb1 \
\cb3 it is these these what eight steps done in parallel at runtime.\cb1 \
\cb3 Okay, it's a lot of steps. Um we're I'm going to come back to this slide a bunch of times uh and develop each part of it.\cb1 \
\cb3 So just relax and let's get started. Let's just learn what some of the the the characters are here and how they\cb1 \
\cb3 interrelate. So we might start with the first line. Um and this line uh is\cb1 \
\cb3 learning the policy and the value function for maximizing rewards. That's like normal reinforcement learning. And\cb1 \
\cb3 I think that is is is almost done. If it was done, there would be a green check\cb1 \
\cb3 mark, but it's blue. And blue means it would be done if we could do this\cb1 \
\cb3 continual deep reinforcement deep learning thing with metalarning. You know if we really could do continual\cb1 \
\cb3 learning that would be uh that would be done. And so we can do this with in very\cb1 \
\cb3 simple simplified cases. We can deal it with uh some of the algorithms like continual backdrop. We can deal with um\cb1 \
\cb3 the linear case. So this is gets a blue check mark conceptually\cb1 \
\cb3 done but um really it's waiting to be done well. it's waiting to uh solve this\cb1 \
\cb3 problem of continual learning for deep learning.\cb1 \
\cb3 Um now the next one uh is red because we don't really have a solution. We have\cb1 \
\cb3 lots of ideas but we don't have a specific proposal and so I'm going to come back to this later. The second one\cb1 \
\cb3 is generating new state features from the existing features. And let me go\cb1 \
\cb3 over this next bit uh quicker. Let me just run all the way down verbally through all eight steps. We we we're\cb1 \
\cb3 going to have some features. We're going to order the features. We're going to take the highest ranked features, the most important features according to our\cb1 \
\cb3 estimation, and we're going to create subpros of of achieving them. So if I decide that being in this lecture room\cb1 \
\cb3 is a is an important sub goal, I would make a a sub problem for that would be rewarded when I when I when I not would\cb1 \
\cb3 would be would would succeed when I am here. If I think uh holding this microphone up sufficiently close to my\cb1 \
\cb3 mouth is a good sub goal. Um, I would I would uh I would I would make that\cb1 \
\cb3 feature into a a good subpro and so on for finding the restroom and finding the\cb1 \
\cb3 the the uh the coffee. Coffee is a really good you know feature for that\cb1 \
\cb3 flowing into your mouth and getting all the sensations involved in that. So that feature becomes a subpro for attaining\cb1 \
\cb3 it and then you learn solutions. So this is the heart of of um the heart of the\cb1 \
\cb3 oak architecture is to have sub problems. You learn the solutions of the sub problems. The sub problems are the options from the from from the O and\cb1 \
\cb3 oak. And uh we also have to learn the value functions that are associated with\cb1 \
\cb3 the sub problem. Okay. And then we're going to have these options. And the\cb1 \
\cb3 next step is we're going to have we're going to learn models of the option. We want to know uh what will happen if you\cb1 \
\cb3 if you were to execute any any one of them. This will be part of your model of\cb1 \
\cb3 the world but it will be a high level model of the world because it'll be about uh an extended way of behaving\cb1 \
\cb3 rather than about a single action. These models will enable you to plan and then\cb1 \
\cb3 and then you so those are all the the full main steps. Okay. So you've seen it\cb1 \
\cb3 once. uh you're going to have to maintain metadata on on the utility of everything and and curate uh throw some\cb1 \
\cb3 things out um and propose new ones. Okay. So now we're going to go through\cb1 \
\cb3 these steps and for a while I'm going to spend a lot of time um actually on the\cb1 \
\cb3 fourth one but yeah the the ordering the features it seems kind of easy but but\cb1 \
\cb3 we can't do it until we have all the rest all the other pieces done that a couple cases that will h that will\cb1 \
\cb3 happen. Okay let's spend some time about the creation of the subpros one for each\cb1 \
\cb3 highly ranked feature. Okay. So on acknowledge there's a long\cb1 \
\cb3 history of looking at subpros that are distinct from the main problem. People talk about curiosity, intrinsic\cb1 \
\cb3 motivation, auxiliary tasks. Some things are settled, some things are unsettled.\cb1 \
\cb3 You don't have to read all this. I want to direct your question to the red part. The key open questions about subpros,\cb1 \
\cb3 which are what should the subpros be? Where do they come from? How can the agent or can the agent generate its own\cb1 \
\cb3 subpros and how do the sub problems help on the main problem? So the contribution\cb1 \
\cb3 of oak is to an is to propose answers to all these questions and to really uh\cb1 \
\cb3 answer questions like uh the third one how can the agent make its own subpros in the affirmative and thus get\cb1 \
\cb3 open-ended abstraction. Um so I like to think of it very\cb1 \
\cb3 basically that we have problems and solutions and these interact with each\cb1 \
\cb3 other. We propose a problem to work on. We work on it. We solve it. As as a part\cb1 \
\cb3 of solving it we will make new features and those features will then be the basis\cb1 \
\cb3 for new sub problems and and and then the sub problems will have to be solved\cb1 \
\cb3 new features and so on in an endless cycle. roughly. That's what I'm talking about. And I wanted to give some\cb1 \
\cb3 examples from nature. Uh here's an orang ba a young orangutang um playing swinging. And so I What is he\cb1 \
\cb3 doing? Like he's not getting food. He's just interested in what it feels like\cb1 \
\cb3 when he swings. Yeah. Okay. That's what I think he's\cb1 \
\cb3 doing. I think it's that sensation is interesting. and and he got it once and now he's trying to get it again and\cb1 \
\cb3 understand how to control it. Um yeah, so here's and also on the other\cb1 \
\cb3 slide we have an orca uh who\cb1 \
\cb3 somebody threw this big uh I don't know what to call it right now.\cb1 \
\cb3 A buoy into his into his pen and he's decided trying to figure out what he could do with it. and he's managed to\cb1 \
\cb3 get it up on his back. So, that was not not random. Um, he got this idea and now\cb1 \
\cb3 he's perfecting it. Yeah. So,\cb1 \
\cb3 animals play, people's play, uh, infants play, young people play. Um,\cb1 \
\cb3 so this is a sped up video of a infant playing. And, uh,\cb1 \
\cb3 this is what we want. We want the the way the the child goes from object to\cb1 \
\cb3 object, learns a little bit about it, gets bored, moves on to the the next object, and just gradually develops a\cb1 \
\cb3 better and better understanding. Maybe next time when he comes back to the to an object, he'll he'll he'll uh have an\cb1 \
\cb3 increased ability, be able to do new things with it. Um, this is what we want. And so I'm trying to think about\cb1 \
\cb3 them as posing sub problems for themselves, things to learn about, things to understand, things to predict,\cb1 \
\cb3 and uh things to control and and figure out where it can make progress in in\cb1 \
\cb3 learning solving the sub problems. Okay, so maybe you're ready to accept this\cb1 \
\cb3 statement. The agent must create its own sub problems. Sub problems can't be given to you. Can't be given to you at\cb1 \
\cb3 design time. You've got to create your own that's far too various and world\cb1 \
\cb3 dependent to have been built in. We have to give the responsibility of the the questions the problems not the solutions\cb1 \
\cb3 not the features the questions okay what is the pro what is the how can we how\cb1 \
\cb3 can we um do this I mean we have much of the machinery the machine of options general value functions off policy\cb1 \
\cb3 learning planning methods these are machinery to help us in this process but\cb1 \
\cb3 we want to create them in a domain independent way and that's challenging Okay. So I want to offer this this\cb1 \
\cb3 possible way to make subpros in a totally domain independent way which is\cb1 \
\cb3 um when you come across a feature when you make up a new feature or you experience a new feature you can make it\cb1 \
\cb3 to be uh the basis of a sub problem uh I call it a reward respecting subpros of\cb1 \
\cb3 feature attainment. So let me show you exactly what that is.\cb1 \
\cb3 um how do we create a subpro from a feature? So a feature is like yeah\cb1 \
\cb3 feature eye. It's it's a bright light that you saw once. It's a it's an\cb1 \
\cb3 interesting sound that happened. It's you you're a baby and you heard the rattle make a sound. You'd like to reproduce that sound. So you have a\cb1 \
\cb3 feature I a feature index I and you have kappa which is how intensely you want\cb1 \
\cb3 that feature. you have to express that uh and you'll get different um subpros\cb1 \
\cb3 if you if you want it at all costs or if you just kind of want it a little bit.\cb1 \
\cb3 So the sub problem is to drive the world to a state where the feature is high\cb1 \
\cb3 without losing too much reward because you will lose some reward if you're not doing what you normally do because what\cb1 \
\cb3 you normally do is uh maximize reward. There is just one reward by the way and\cb1 \
\cb3 so I don't have to qualify that is the real reward and the sub problem is to\cb1 \
\cb3 achieve a state where the feature is high uh without losing too much reward\cb1 \
\cb3 without having to go through something that's painful or having lost opportunities to get something uh\cb1 \
\cb3 pleasurable. Okay, so we're trying to find an option. An option is a pair. It's a policy pi\cb1 \
\cb3 termination function gamma that maximizes the value of the i feature at termination\cb1 \
\cb3 while respecting the rewards and values. So here's the equation. Maybe we can understand the equation. Uh in each\cb1 \
\cb3 state you're trying to choose you're trying to maximize and you're going to choose pi and gamma to maximize. And\cb1 \
\cb3 it's the sum the sum is conditional on starting uh the world in in the indicated state\cb1 \
\cb3 because um yeah for each for each state we say if\cb1 \
\cb3 we started there have a policy pi that gets you rewards\cb1 \
\cb3 um from t+1 to t. T is is the time of\cb1 \
\cb3 termination. You're going to follow the option pi and you're going to terminate when gamma says terminate. And so that\cb1 \
\cb3 will fix establish the random variable which is the time which you terminate\cb1 \
\cb3 capital t. And if you look at all the rewards you receive while you are following the option summing them up.\cb1 \
\cb3 Those are the things that you want to be as big as possible or not as least negative as possible.\cb1 \
\cb3 And then you want to um reward yourself for achieving feature I's feature I at\cb1 \
\cb3 time of termination S sub capital T. And you know there's a there's a waiting by\cb1 \
\cb3 kappa. So you want lots of rewards. You want to be the feature to be true but\cb1 \
\cb3 you know it's got to be traded off the rewards. And you also care about the state that\cb1 \
\cb3 you're in at the time of termination. You don't want to like find a really good way to to uh I don't know\cb1 \
\cb3 get some coffee but has the consequence that you have to break your leg. Okay.\cb1 \
\cb3 Actually, that would be a reward. That would be a bad reward. You don't want a way to get coffee that would um uh leave\cb1 \
\cb3 you in a bad state. Like let's say you got coffee but uh you know you're gonna get arrested or you're going to fall\cb1 \
\cb3 down the stairs. Okay. Um those are bad states. You know I often you know the\cb1 \
\cb3 walk along the edge of a cliff but uh don't fall off. If you fall off, it actually doesn't hurt very much to fall\cb1 \
\cb3 off because, you know, if you can say just terminate while you're in the air, uh the rewards are fine, but the value\cb1 \
\cb3 the value is, you know, bad rewards are coming up. So, your value will be will be poor.\cb1 \
\cb3 Okay, that's how we create a sub problem. And now let's really we're getting into the heart. Uh we have these\cb1 \
\cb3 processes. Um the f the first one is we form the uh the problem just as we just\cb1 \
\cb3 talked about you know given a feature form a problem. We do that with all the high highly ranked features. So we have\cb1 \
\cb3 now we have you know dozens of pro of problems. Each one we work on it to\cb1 \
\cb3 produce an option. The solution to sub problem is an option. Now you've got this options. Well that defines a\cb1 \
\cb3 correct transition model for the option. Is it well defined? Um\cb1 \
\cb3 what we know what the model should be. If you give me the way of behaving and the way of terminating, we know what the model should be. And so this is\cb1 \
\cb3 something you work on. You work on computing this model, approximating that model. Once you have the model and that\cb1 \
\cb3 you have a models of all the different uh options for all the different subpros for all the different features and once\cb1 \
\cb3 you have the model, you use the model of course to plan and to improve your behavior. Okay. So we got these three\cb1 \
\cb3 steps and there's one fourth step which is that uh you have to come up with features right we we we started with a\cb1 \
\cb3 good set uh with the highly ranked features so we have to have a way of ranking the features and um and I just\cb1 \
\cb3 want to point out that we have that because all of these these these three uh pillars the the later three all use\cb1 \
\cb3 features to to do their job right if you're going to find the option the option is a function of state and so you\cb1 \
\cb3 have to look at state features when when should you do the option when when it when is the value function of the option\cb1 \
\cb3 uh when should it have which values it's going to look at the state features to make those decisions when you uh learn\cb1 \
\cb3 the models of the options you're going to look at the state you start in and you're going to look at the state features of that state and you're going\cb1 \
\cb3 to say oh that feature I found useful that other feature was useless to me um\cb1 \
\cb3 and so and and then when you use the models you will find some models are\cb1 \
\cb3 useful And that will sort of trickle back to evaluate the choice of the of the options. And that will also trickle\cb1 \
\cb3 back to evaluate the choice of the feature attainment problems. And at least uh all these learning processes,\cb1 \
\cb3 predictive learning processes will use the features and they will provide feedback to the uh to the features\cb1 \
\cb3 saying these are the ones that have proven useful to us. These ones have not. Okay. So let's draw that the same\cb1 \
\cb3 idea with a different picture. I'm going to have many pictures about the same idea and I'm going to say it a few times\cb1 \
\cb3 so maybe you'll you'll get it. Um this way of talking and thinking. So the\cb1 \
\cb3 perception process is going to is responsible for constructing interesting state features. Um the play process or\cb1 \
\cb3 the problem posing process problem posing and solving. That's where you do the sort of core reinforcement learning\cb1 \
\cb3 things of figuring out your value functions and your policies and you produce the options.\cb1 \
\cb3 And then you have to predict the consequences of those options to form a transition model. And then of course you\cb1 \
\cb3 plan with the transition model to get improved policies and values. And the feed we close the the cycle is we have\cb1 \
\cb3 feedback from the later steps back to the construction of features. And that feedback is mainly say saying I have I\cb1 \
\cb3 have found that feature useful or I have not found that feature useful. Okay. So here's we're back to our eight\cb1 \
\cb3 steps. U we now understand what it means to create the subpros one for each feature\cb1 \
\cb3 and we also know what it means we've talked about how you learn the solutions\cb1 \
\cb3 um and the transition models. Um maybe there's I'll say one more slide about that about this this topic uh how we\cb1 \
\cb3 learn those things. Oh, but also notice they're they're in blue because although we know how to do these things, we don't\cb1 \
\cb3 really know how to do them with continual deep learning or maybe we have to use Shbanch's continual backdrop. You\cb1 \
\cb3 know that this is this is this is a topic we'll come back to. This is incompletely understood. So we kind of know how to do them but we definitely\cb1 \
\cb3 think we can do better. Okay, one one short slide\cb1 \
\cb3 more about that is it to a large extent we could use just standard off-the-shelf algorithms standard offtheshelf usually\cb1 \
\cb3 off policy algorithms for learning general value functions like GTD and emphatic TDD and retrace ABQ uh these\cb1 \
\cb3 are prediction learning methods for generic uh GVFs and so we can use that\cb1 \
\cb3 to learn the main problem how to get reward we can learn that for learning diagrams for the sub problems. We can\cb1 \
\cb3 find the transition models of the options with these methods and the planning can also be done with standard\cb1 \
\cb3 algorithms applicable to all GDFs. And this enables us to say that anything\cb1 \
\cb3 that can be learned can also be planned. That's I just wanted to get to that slogan for you because it's it's a it's\cb1 \
\cb3 a good one. It's a it's a um it's a bit advanced but almost it's\cb1 \
\cb3 almost a next step. Okay, so we got those things and now the\cb1 \
\cb3 other big step and uh I'm going to have to do it a little bit uh not in full detail, but we\cb1 \
\cb3 have to talk about how the planning works. Okay, how does the planning going to work? And I'm going to give that a green check mark because I think we do\cb1 \
\cb3 understand this. So planning, why do we plan? Why do we want these jumpy uh\cb1 \
\cb3 temporally extended models of the world, the option models? And we we but basically why do we want to plan at all?\cb1 \
\cb3 We want to plan because the world changed and the correct values change and it's easier in many cases not in\cb1 \
\cb3 every case but in many cases it's easier to get the model of the world right than to get the values right. So you get the model right and then you do the planning\cb1 \
\cb3 uh to make the values consistent with your model. Um and uh uh so in this big\cb1 \
\cb3 world setting it's it's the world changes or appears to change. Uh most of\cb1 \
\cb3 the world's dynamics or in many cases the world's dynamics including the reward parts don't really change but the\cb1 \
\cb3 values nevertheless change. Like it's always true that I can walk over there and find the restroom but it's not always true that I want to go to the\cb1 \
\cb3 restroom. Um it's not always true that I want to get coffee. it's not always true\cb1 \
\cb3 that I want to go to the library all the things or go to Edmonton. So u to\cb1 \
\cb3 prepare for these later wants um these these different values you plan and u\cb1 \
\cb3 this also has some implications for which sub problem is useful. Okay. Now\cb1 \
\cb3 how does planning work? Um I like to think that planning is is by\cb1 \
\cb3 approximations to value iteration. So, so this this equation is value\cb1 \
\cb3 iteration. You may already know it, but I think maybe you should look first um\cb1 \
\cb3 here. What is the model? Model is something that that takes a state a low-level model takes a state and an\cb1 \
\cb3 action gives you um a probability distribution over next states and the expected reward along the way. And so\cb1 \
\cb3 value iteration then says I'm trying to improve the values of some states. I look at the possible actions and I'm\cb1 \
\cb3 going to maximize over them. I'm going to look at the immediate reward and I'm going to discount and then take the\cb1 \
\cb3 probability or the really the expected value of of the value of the next state.\cb1 \
\cb3 So this is the probability of each next state. You wait by that you take the value of the next state. So you know you\cb1 \
\cb3 probably are are familiar value iteration works like that. Um and really\cb1 \
\cb3 all all planning methods in some sense work like this just apply to different states in in the search tree and you\cb1 \
\cb3 know order which states are updated in such a way uh has is is is\cb1 \
\cb3 varied. Okay. And um\cb1 \
\cb3 the interesting thing about planning with option models is that it's the same. It's really it's really the same.\cb1 \
\cb3 Although life is lived one step at a time, we have to plan it at a higher level. So our knowledge of the world\cb1 \
\cb3 should be about the large scale dynamics. It should not be conditional on single actions but on a sustained way\cb1 \
\cb3 of acting that is on an option. So if we look at\cb1 \
\cb3 the conventional model it's we receives an action an option model you receive an option still you get a probability\cb1 \
\cb3 distribution perhaps over the next states and expected reward not not a onestep reward but some reward while\cb1 \
\cb3 you're following the option and then then value duration is almost unchanged.\cb1 \
\cb3 We just change the actions into options and we still talk about the reward for\cb1 \
\cb3 following that option and the probability of each next state under the option. Okay, so that's good. Um that's\cb1 \
\cb3 how that gives you a flavor of how planning would be done. You you would you would you you basically you say oh\cb1 \
\cb3 here's some state. What are the things I could do there? What's the best I could do? I'll update my value and here's I\cb1 \
\cb3 could imagine another state. Maybe it's the state I'm in. Maybe it's not the state I'm in. But I go through this outer loop of of considering various\cb1 \
\cb3 states and then maxing over the possibilities and uh doing this equation.\cb1 \
\cb3 But I'm sure you are concerned because I've been talking about via s which is\cb1 \
\cb3 the value of an individual state and uh we can't do that of course we have to\cb1 \
\cb3 have function approximation. So I don't know I don't think it's that\cb1 \
\cb3 helpful to go through these equations but yeah the value V of S will become\cb1 \
\cb3 the approximate value of a state given a parameter vector W and also your model\cb1 \
\cb3 of the world will become R hat and P hat they also become parametric\cb1 \
\cb3 and then after you've done that um things are much the same there is more\cb1 \
\cb3 complications which I will skip having to do with what's computationally expensive. You can ask me about that if\cb1 \
\cb3 you want. Um, if you're interested, I'll say some summary things. Um,\cb1 \
\cb3 so, so we we're maybe we're almost done, but\cb1 \
\cb3 remember I said we would come back to some things. So, I said we would come back to um the first two learning how\cb1 \
\cb3 we're going to learn these things and what's we the problems we have. I want to say explicitly what what the the\cb1 \
\cb3 situation is there. So two more slides. Um so the oak architecture requires\cb1 \
\cb3 reliable continual learning, continual deep learning. And so this is one of those things that I said this would be\cb1 \
\cb3 we'd be all done with step one uh if we uh could do continual learning. And but\cb1 \
\cb3 we can't do we can't can we do this yet? Okay. Can we do this? Okay. Do we have\cb1 \
\cb3 reliable continual learning? Well, we do have reliable continual learning for the linear case, for the tabular case, but\cb1 \
\cb3 for the nonlinear case, for the deep learning case, we can't have reliable we\cb1 \
\cb3 don't yet. Do we? No, we sort of do. We have uh we have we have these catastrophic failures\cb1 \
\cb3 like u like uh catastrophic forgetting and the catastrophic loss of plasticity\cb1 \
\cb3 uh that have been figured out long ago and also very\cb1 \
\cb3 recently. Um so we have these catastrophic problems\cb1 \
\cb3 but we also looks like there's a range of solution methods. So I guess this is an area that's that's right now in flux\cb1 \
\cb3 and uh people are figuring things out and you know we're not there I can't\cb1 \
\cb3 give it a green check okay but um but there are lots of ideas continual\cb1 \
\cb3 backrop is one of them the metalarning of new features I think can also help\cb1 \
\cb3 and related to that it's the other the other problematic uh or the other step\cb1 \
\cb3 that I wanted to talk about where we having to do with um creating generating\cb1 \
\cb3 new state features. This is this is also a super old problems going back to the 1960s like Minsky and Selfridge would\cb1 \
\cb3 talk about this. They would talk about representation learning. They would talk about the new terms problem and anyway I\cb1 \
\cb3 like to talk about metalarning nowadays. So backrop back in ' 86 was supposed to\cb1 \
\cb3 solve this. We were supposed to you know learning representations by gradient descent. Um but it really just doesn't.\cb1 \
\cb3 And um I think we're we we accept we recognize\cb1 \
\cb3 that unless we are still in love and think that gradient descent is enough for everything. Um\cb1 \
\cb3 most of the other methods other than gradient descent are based on generate and test ideas where you like generate a\cb1 \
\cb3 bunch of features and then you test them to see if they're useful and so you could generate them randomly and you\cb1 \
\cb3 could test by their utility and u continual backdrop is an instance of\cb1 \
\cb3 that. It also has a real old history. Leslie Keelbing did her uh did some of\cb1 \
\cb3 this work in her PhD thesis in 1993. Uh Rupam Mahmud and I did some work a\cb1 \
\cb3 decade ago. Um there's lots of ideas but not yet a specific proposal for a whole\cb1 \
\cb3 network based on grad descent or any other uh there aren't I anyway don't have a\cb1 \
\cb3 specific proposal uh for solve this problem. I think it's a really really important problem and I think it's like\cb1 \
\cb3 maybe it will be worked out in the next couple of years and then it will literally um take over everything that\cb1 \
\cb3 people have done with with deep learning. If we had a deep learning method that was can do everything we're\cb1 \
\cb3 doing now but can also learn continually that would just um\cb1 \
\cb3 be a really big thing and I think there's no reason why it couldn't happen. Um and so I think it will\cb1 \
\cb3 I also think that something like uh my algorithm called IDBID or IDBD and\cb1 \
\cb3 it's really old will be a key part of that uh solving this problem.\cb1 \
\cb3 Okay. Um\cb1 \
\cb3 I'm sort of done. This this figure was just to um remind you one more time of\cb1 \
\cb3 the cyclical nature how we have state features that produce sub problems that are solved produce options that are that\cb1 \
\cb3 are used to form models and and this is not doesn't look like a cycle but\cb1 \
\cb3 remembering that um there's feedback being sent from each user of the features uh information information on\cb1 \
\cb3 which features are are useful which ones are that informs the features and so it is in fact a cycle.\cb1 \
\cb3 Um so in my the quest uh have we uh succeeded we have something that's\cb1 \
\cb3 doesn't it's totally domain general there's nothing in it specific to any world it's totally experential and has\cb1 \
\cb3 the claim or the hope that it will be uh you know able to find unlimited open-ended abstractions uh limited only\cb1 \
\cb3 by the computational resources and so we might ask you know what do you what what should one think about this\cb1 \
\cb3 and so I think arguably reinforce Enforcement learning and oak offer the first plausible mechanistic answer to\cb1 \
\cb3 several important questions. How can high level knowledge be learned from low-level experience? Where do concepts\cb1 \
\cb3 come from? How do we reason? What is reason? Perhaps reason is just planning\cb1 \
\cb3 in this way. What is the purpose of play to find these these uh subpros which\cb1 \
\cb3 structure our our cognitive um our cognition? And what is the purpose of\cb1 \
\cb3 perception? We're answering the question of how we perception can operate without reference to a a human label or to an\cb1 \
\cb3 external world. Perception um can be concepts that have been formed to solve\cb1 \
\cb3 problems that are the basis of uh subpros. And if you know about cognitive\cb1 \
\cb3 uh David Maher, then we would say that oak is a computational theory of intelligence.\cb1 \
\cb3 Okay. Now, what about someone like yourself, a reinforcement learning AI scientist? I I would hope that you would\cb1 \
\cb3 think that Oak provides a way to think about the parts of AI AI and their interaction and this can guide future\cb1 \
\cb3 research. It's a vision for how to do planning with a learned model which is a key missing ability for today's AIS. Uh\cb1 \
\cb3 it offers a view of perception that's grounded in experience rather than in human labels. It offers incomplete\cb1 \
\cb3 admittedly incomplete but schematic answers to the discovery problem. Where do the subpros, options and features\cb1 \
\cb3 come from? So it's a vision of how we can obtain an open-ended super\cb1 \
\cb3 intelligence entirely grown from experience. uh even if it's not yet\cb1 \
\cb3 fully specified and e even if there are things that I'm saying we don't really know how to do but we should know how to\cb1 \
\cb3 do such as solve continual learning and metalarning um it's a vision of how to grow a super\cb1 \
\cb3 intelligence from experience at runtime his most important capabilities and does it act learn plan model learning sub\cb1 \
\cb3 problems the options with um all the rest and the discovery of of the state\cb1 \
\cb3 features and thereby of the problems, options and models leading into this\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 virtuous open-ended cycle of discovery and is completely general and is thus scalable and a potentially lasting\cb1 \
}