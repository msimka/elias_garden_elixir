{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 ArialMT;}
{\colortbl;\red255\green255\blue255;\red237\green237\blue237;\red255\green255\blue255;\red25\green25\blue25;
}
{\*\expandedcolortbl;;\cssrgb\c94510\c94510\c94510;\cssrgb\c100000\c100000\c100000\c10196;\cssrgb\c12941\c12941\c12941;
}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28\fsmilli14400 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 [Music] hello everyone hello sorry the mics\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 don't work too well I think for here okay no work fine so we're gonna get started thank you very much for coming\cb1 \
\cb4 welcome these are the Aral seminars which uh today are as always sponsored\cb1 \
\cb4 by uh instadp uh Google Deep Mind and ionic and this is a very special seminar\cb1 \
\cb4 uh for us because this was a seminar series that pop up from a reinforcement learning reading group and as a bunch of\cb1 \
\cb4 reinforcement learning researchers uh we all in the group uh initiated our adventures with a very nice book from\cb1 \
\cb4 the person who is now presenting today so I think there is no need of any\cb1 \
\cb4 presentation we're very happy to have today with us Professor Richard salon who is going to be talking about Dynamic\cb1 \
\cb4 deep learning Richard thank you [Applause] [Music]\cb1 \
\cb4 thank you Bo it's really a pleasure to to be here today and have a chance to talk to you share some ideas with you um\cb1 \
\cb4 I was I've never been to Imperial before and uh I've just learned about all kinds of interesting things going on here it's\cb1 \
\cb4 all very exciting and uh it's uh impressive in many\cb1 \
\cb4 ways so today uh let's start with some general remarks\cb1 \
\cb4 before I get into really what I'm going to cover today um AI artifici intell is\cb1 \
\cb4 really ambitious so what I would say I would say that AI researchers seek to\cb1 \
\cb4 understand intelligence well enough to create beings of Greater intelligence than current humans pretty awesome\cb1 \
\cb4 pretty awesome goal uh I think reaching this profound intellectual Milestone\cb1 \
\cb4 will enrich our economies challenge our societal institutions it's going to be unprecedent it's going to be\cb1 \
\cb4 transformational but also a continuation of trends that are thousands of years old people have always created tools\cb1 \
\cb4 technology and Been Changed by it um this is what we do it's what humans do\cb1 \
\cb4 the next big step is to understand ourselves this is a quest Grand and glorious and essentially\cb1 \
\cb4 human yeah of course it's also totally hyped\cb1 \
\cb4 up uh but I don't think I'm hyping it here the things I'm saying here are just\cb1 \
\cb4 true this is a this is a grand Quest um but it is hyped up it's it's not a good\cb1 \
\cb4 climate for science you know science likes Things To Be steady and calm and focus on the important fundamental\cb1 \
\cb4 issues whereas AI is like it's all there's a lot of money going into it\cb1 \
\cb4 there is so many companies uh there is people you know arguing that what I'm doing is is enough you don't\cb1 \
\cb4 need to do anything any else um and uh it's it's not it doesn't feel\cb1 \
\cb4 I don't know if you guys sense it but but I sense that it's not a really good climate for science\cb1 \
\cb4 so for example large language models really hop into the thing really important thing really powerful thing\cb1 \
\cb4 but they can't just accept that they're going to be really important and really valuable they have to say oh this is all\cb1 \
\cb4 that the Mind does um the mind they have to claim the largest language models can\cb1 \
\cb4 do reasoning and and maybe they're conscious and all kinds of crazy stuff instead of you know because the real\cb1 \
\cb4 scientific attitude you find out you figure out what you can do and try to understand what you can't do and uh and\cb1 \
\cb4 quietly make progress I think we're not really that's not really the the the culture that I feel in our fi at the\cb1 \
\cb4 moment so I think I just a little bit disappointed by that but I don't doubt this that it's an\cb1 \
\cb4 important question that we're trying to address okay now some others of my\cb1 \
\cb4 perspective so I'm one of these guys that are trying to understand and create maximally intelligent agents and\cb1 \
\cb4 intelligent an agent is defined to be intelligent to the extent that's able to predict and control its input stream\cb1 \
\cb4 particularly its reward this is the way I would Define intelligence not the way everyone would find intelligence not to\cb1 \
\cb4 pick on the large language models but they don't actually try to predict control their input stream Technically\cb1 \
\cb4 when they're running uh they don't even have an input stream um so we want to control we want\cb1 \
\cb4 to affect our input and uh the creation of these super\cb1 \
\cb4 intelligent agents and super intelligent augmented humans I think will be an unallied good for the world I'm not\cb1 \
\cb4 going to say that the world is going to be an unallied good thoroughly good but\cb1 \
\cb4 because the feature I think is going to be tough because we're in a what's called a fourth turn turning if you don't know what that is you might check\cb1 \
\cb4 it out sometime it's kind of an interesting set of uh ways of viewing societal\cb1 \
\cb4 change um well the path to intelligent agents runs through reinforcement\cb1 \
\cb4 learning now through the lmm um but the current biggest current bottleneck to\cb1 \
\cb4 ambitious reinforcement learning based AI is that our deep learning methods that we rely on are inadequate to the\cb1 \
\cb4 task they're not well suited to the task and that's what I'm going to talk about today that they're they fail when we ask\cb1 \
\cb4 them to be we put them in a dynamic situation where they have to continue to learn as you know I think all deep\cb1 \
\cb4 learning methods uh when when they're deployed when they're inter when they're acting on a robot or or um interacting\cb1 \
\cb4 with humans they they no longer learn they learn only in advance they are sort\cb1 \
\cb4 of transient Learning Systems\cb1 \
\cb4 now one more sort of introductory slide just to tell you where I am where my thoughts are and I I just put this in\cb1 \
\cb4 like like two minutes ago actually um this is the standard architecture of reinforcement learning and if you see\cb1 \
\cb4 all these components of a modelbased reinforcement learning agent that's what MBR is model reinforcement learning\cb1 \
\cb4 agent takes in observations adits actions and it also takes in a reward um and it basically composes of\cb1 \
\cb4 four parts perception takes in the Stream of events\cb1 \
\cb4 and admits a representation of of the current situation or the state probably it's a feature vector and that passed to\cb1 \
\cb4 the policy reactive policy to pick an action and those two right in the middle those two these two perception an\cb1 \
\cb4 action uh those two together form a complete agent but if you want the agent to be adaptive and if we want it to be\cb1 \
\cb4 intelligent it's going to be have to be adaptive as I have defined it then you're going to need a value function to\cb1 \
\cb4 say how well things are going and then use the the TD from the value function to adjust the policy so this diagonal\cb1 \
\cb4 line through the policy if you can see that represents um not an informational transfer like the state comes into the\cb1 \
\cb4 policy adits the action but the diagonal line means you're changing the function implemented by the Box okay so value\cb1 \
\cb4 function changes the policy now at the same time the full agent a full\cb1 \
\cb4 modelbased agent you also learn a transition model which is a model of the\cb1 \
\cb4 world uh and its state transitions observing the states here here you see this the state at a particular point in\cb1 \
\cb4 time and you might uh notice the state at a particular point in time and then you take an action and then you find\cb1 \
\cb4 yourself in a new state after amount of time and that's a transition and your model transition model would model those\cb1 \
\cb4 transitions by observing the data and uh then you can ask questions of your model\cb1 \
\cb4 what if I did this action instead what would happen then oh you to this state then you could apply the value function\cb1 \
\cb4 to that state and you get a TD error and then you could use that to adjust the policy again and we call that planning\cb1 \
\cb4 because whenever you're using a model and not experience to adjust your\cb1 \
\cb4 anything we call it plan just these just definitions of how I'm going going to use the\cb1 \
\cb4 words now in the full um architecture we also we have not just\cb1 \
\cb4 one policy and one body function but a whole set of them and that's meant to be suggested by these these that are\cb1 \
\cb4 stacked up behind the the true policy and and stacked up behind the value\cb1 \
\cb4 function we more more value function so that's where you would posee subtasks for yourself and you learn skills how to\cb1 \
\cb4 achieve achieve other things other than the uh reward while always being cognizant of the effect of those other\cb1 \
\cb4 things on the reward okay this is just on the side right I hope it's interesting but it's on the side and um\cb1 \
\cb4 a lot of this is developed in these papers you might want to it could be checked\cb1 \
\cb4 out um okay now let's get to today today\cb1 \
\cb4 we're going to talk about Dynamic deep learning but really it's about uh uh\cb1 \
\cb4 paper that my colleagues uh and I wrote and the title is helpful for clarifying\cb1 \
\cb4 the topic the topic is loss of plasticity and deep continual\cb1 \
\cb4 learning and I'll Define all those things in a minute but now is my chance\cb1 \
\cb4 to talk about me a little bit more um I acknowledge I have many jobs keing\cb1 \
\cb4 Technologies the University of Alberta Amy the Alberta machine\cb1 \
\cb4 intelligence Institute my own reinforcement learning lab and open mind\cb1 \
\cb4 research I want to tell you about this a little bit this is our our logo open mind is a a new Institute that we've\cb1 \
\cb4 created um that's um devoted to doing\cb1 \
\cb4 fundamental research in along the lines of reinforcing the Alberta plan um for\cb1 \
\cb4 people who have already graduated obtained their degree uh but want to uh do research in this area and it's\cb1 \
\cb4 distributed it's we have kind of a focus in Alberta but it's distributed and people can be all over the world and uh\cb1 \
\cb4 if you might want to do fundamental research in this area and\cb1 \
\cb4 you uh want to focus on that rather than become a professor um you uh may want to\cb1 \
\cb4 apply to be a fellow of the our Institute okay now here's my main\cb1 \
\cb4 message main message is that deep learn learning doesn't work for continual learning um and by not work I mean that\cb1 \
\cb4 learning slows and eventually goes to a very low level of learning we' it's lost\cb1 \
\cb4 you've lost plasticity and by Deep learning I mean this all the standard uh artificial\cb1 \
\cb4 neural network methods specialize as they are for this non-c continual or transient learning and I also mean\cb1 \
\cb4 without replay buffers because I consider themselves an acknowledgement that deep learning doesn't work\cb1 \
\cb4 now uh Better Learning algorithms that are specialized or well suited to\cb1 \
\cb4 continual learning they're not hard to find and they can apply to deep learning uh so I don't think there's anything\cb1 \
\cb4 wrong with artificial neural networks we just need to find better algorithms and\cb1 \
\cb4 we only need to to find we have to start looking for them and uh so that's my\cb1 \
\cb4 main message and then this work\cb1 \
\cb4 um details and substantiates this claim okay so I'm going to have\cb1 \
\cb4 demonstrations of loss of plasticity and deep learning um and this and and and\cb1 \
\cb4 the attempts to maintain it and this will be in classic supervisor learning problems like imet and CFR 100 with\cb1 \
\cb4 residual networks um and also in reinforcement learning but primarily in these classic\cb1 \
\cb4 supervised learning problem domains you know now already that I'm not really a\cb1 \
\cb4 supervised learning guy I'm aiming at reinforcement but I need all those four\cb1 \
\cb4 boxes that I showed you they all need to learn continually because you live in your world and you keep adapting to\cb1 \
\cb4 whatever it throws at you and you you have to model the world and the world changes the world is infinitely complex\cb1 \
\cb4 you'll never be able to anticipate all the ways that might change and so you have to continually learn and then\cb1 \
\cb4 hopefully I'll have just a a little bit on some goals and ideas for Next Generation of deep learning that would\cb1 \
\cb4 be well suited for continual learning\cb1 \
\cb4 settings now this work has just been published in nature so I got I'm really excited about that the first time ever\cb1 \
\cb4 been published in in nature and I just but I want you to realize that that like\cb1 \
\cb4 uh even when you have a publication success it's often proceeded by a large\cb1 \
\cb4 time where you have publication failure uh so this happens all the time it's just if something is fashionable\cb1 \
\cb4 you can get it published in nature right if it's not fashionable you can't get it published at all sometimes I I've\cb1 \
\cb4 experienced this many times all my best Publications were were really hard to get published you know yeah temporal\cb1 \
\cb4 difference learning when I published that really hard options all all almost\cb1 \
\cb4 almost it's almost always true that the more novel and and and\cb1 \
\cb4 groundbreaking your results are the harder it will be to get them published so if you're having trouble you know\cb1 \
\cb4 take heart a little bit and realize you'll have to persevere and if you persevere you may eventually get\cb1 \
\cb4 published but it's not guaranteed it may never become fashionable um so we we in\cb1 \
\cb4 particular tried to publish this over like four and a half years and we're unsuccessful you can see it in archive\cb1 \
\cb4 archive is good um plasticity just means the uh the\cb1 \
\cb4 ability to learn and so loss of plasticity just means means loss of the ability to learn or not being able to learn continually not continual learning\cb1 \
\cb4 hope you're getting comfortable with the way I'm using these words maintaining plasticity is maintaining the ability to\cb1 \
\cb4 learn and so in AI since we want all the four boxes to learn we should prioritize maintaining\cb1 \
\cb4 plasticity and we're not the first to see this problem see at least hints\cb1 \
\cb4 of the problem uh it's very closely related to the idea of catastrophic forgetting where in deep learning you\cb1 \
\cb4 learn some things and you learn some more things you often totally forget the old ones now loss clity you learn some\cb1 \
\cb4 things you learn some new things maybe you learn some new things after that and eventually you can't learn any more new things not the same as loss of as\cb1 \
\cb4 catastrophic forgetting it's sort of cat catastrophic loss of plasticity so this there are also hints\cb1 \
\cb4 of this in the early neural networks work in the sitech literature and there a few works like uh Ashen Adams that\cb1 \
\cb4 showed the failure of warm starting warm starting was like you thought oh I'll teach this this neural network on on\cb1 \
\cb4 part of the data and that'll warm it up so that it'll you I give it the second half of the data it will be faster but\cb1 \
\cb4 the fact they found the opposite if they trained it on first on the first bit then it was slower on the second bit and\cb1 \
\cb4 and they did much better if they just put the two parts into one big thing and trained it all once from scratch was\cb1 \
\cb4 kind of surprise it was surprising seen as a failure um that was one of the earliest demonstrations\cb1 \
\cb4 um and Primacy bias Nic Ain capacity\cb1 \
\cb4 loss CLA ly and others these were shown in in reinforcement\cb1 \
\cb4 learning okay but no one really did a thorough demonstration in supervised\cb1 \
\cb4 learning uh of this phenomenon and and because you have to do it thorough it's a Nega sort of a negative result you if\cb1 \
\cb4 you get hints it doesn't change people's minds and so the significance of this piece of work was that we did it really\cb1 \
\cb4 thoroughly and and dotted all the eyes and crossed all the tees and and considered all the\cb1 \
\cb4 possible ways you can get around it and and uh have have something that's pretty\cb1 \
\cb4 incontrovertible okay so let me show you the first one of these demonstrations law of plasticity in supervised learning\cb1 \
\cb4 and we're going to use the classic uh domain of imag net um so image n you have this database\cb1 \
\cb4 of millions of images and there like this is an image this is a this is\cb1 \
\cb4 a a shark hope you hope that's obvious to you this is somewhat down sample and we have to use somewh down sample images\cb1 \
\cb4 uh but there a thousand glasses each with 700 or more images and it's widely used um but we need to change it because\cb1 \
\cb4 it it's why Lees as a pure supervised learning problem like they take all data\cb1 \
\cb4 they they they they they train on all the data at once and and then they\cb1 \
\cb4 gradually freeze it they train and train and train multiple presentations and\cb1 \
\cb4 then finally they freeze it and actually they gradually freeze it and slow it slow down learning until they have a nice stable State we don't want to do\cb1 \
\cb4 that we want to have something that we continue to learn so how are we going to do that so we're going to make a A variation of the problem which called\cb1 \
\cb4 continual imet so and we see it as a minimal change to the classic one so we\cb1 \
\cb4 do the usual thing we take there's 700 examples remember for each class so we\cb1 \
\cb4 separate that and just training 600 training and 100 test ones and then we take them in pairs\cb1 \
\cb4 so like this might be the first pair we'll show you crocodiles versus guitars\cb1 \
\cb4 and you have to learn to distinguish those two okay and then when you're doing well\cb1 \
\cb4 we've seen we've seen all the examples of those pairs we go on to a second pair\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 like so think about how that that's done we we've uh the network only has two\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb4 options two outlooks a or b or yes or no uh crocodile or guitar and so then when\cb1 \
\cb4 you want to ask a new question you add well since I'm never going to ask you again about crocodiles and guitars I can\cb1 \
\cb4 actually throw out the heads the final uh two neurons of the neural network and\cb1 \
\cb4 and uh and replace them with two new ones two new ones for the new task the\cb1 \
\cb4 new task is distinguish game controllers from fish okay and then once you've\cb1 \
\cb4 learned that I I erase the heads and give you a new problem both ties\cb1 \
\cb4 versus oxin I guess okay and this so This continues and you can do many pairs\cb1 \
\cb4 because remember there's a thousand pairs so you might think I could make 500 pairs but you can actually make many\cb1 \
\cb4 more because you can reuse ones as long as you don't re reuse the pair so how many pairs of uh of thousands of things\cb1 \
\cb4 it's it's it's like it's like almost th squared so you have lots of pairs we can\cb1 \
\cb4 do this forever um performance measure is I'm just going to measure the percent that\cb1 \
\cb4 are correct on the test set remember I present all the examples for the two\cb1 \
\cb4 classes and then I have some saved out ones I measure performance on them and then I average this over many many runs\cb1 \
\cb4 and in the many runs I vary the the pairings of the class I don't you don't have to worry that maybe the first class is hard but the first class is easy\cb1 \
\cb4 since they're all averaged over all the possibilities okay that's sort of the problem and then there's a whole another\cb1 \
\cb4 set of details for the Learning Network and\cb1 \
\cb4 um talked about how the heads are reset the final heads uh the structure of the\cb1 \
\cb4 network is pretty standard it's a little bit narrow because we only ask you to classify two things at once we we don't\cb1 \
\cb4 want it uh to be infinite in its size compared to the things we ask of it\cb1 \
\cb4 um so it has some structure of convolution layers um and interconnected\cb1 \
\cb4 layers and notice the last one is just two those are the last two\cb1 \
\cb4 heads okay and then we use batches we use epics and all the usual ways um now\cb1 \
\cb4 it's important that the weights are initialized in the standard way and that it's only done once it's only for the\cb1 \
\cb4 very the very beginning uh on the first tab before the first task we we\cb1 \
\cb4 randomize all the weights small initial values this is how this is a standard practice uh but it's only done once you\cb1 \
\cb4 know when you get a new start to get new data from the new task you're not told it's a new task and so there's nothing\cb1 \
\cb4 no opportunity to reinitialize or do anything and you maybe you don't want to reinitialize think about it maybe you've\cb1 \
\cb4 learned one thing and now you're going to learn something new maybe what you learned on the first one will maybe the features you learned on the first one\cb1 \
\cb4 will help you um get those last St heads the for the new task um they maybe\cb1 \
\cb4 they'll be able to much better because they have better features okay so we start with just\cb1 \
\cb4 plain backdrop momentum cross entry loss Rel activations the usual things and we\cb1 \
\cb4 of course varied all these things but the the standard case is is this one and\cb1 \
\cb4 yeah many variations get representative results and then we I want to ask you\cb1 \
\cb4 how do you think what what do you think will happen over over the sequence of tasks will performance be better on the\cb1 \
\cb4 first task or the second\cb1 \
\cb4 task were you voting for first Al I think it'll improve andur you\cb1 \
\cb4 think it will improve and since that's that would make sense that it should maybe the second task will be better\cb1 \
\cb4 third task maybe will be better at some point you're not get any more Advantage\cb1 \
\cb4 from Having learned good features\cb1 \
\cb4 that would be good that's the way a learning system should work okay okay uh this is the beginning I'm\cb1 \
\cb4 going to show you just the beginning first and obviously it's going to depend it's going to depend on on your well let\cb1 \
\cb4 me do the axes first the axes are task number along the bottom the first pair the fifth\cb1 \
\cb4 pair um and this is percent correct okay so if you and these are\cb1 \
\cb4 well what are you tuned for this red line was tuned for doing as well as you could on the very first task so it's\cb1 \
\cb4 sort of fast right and so that gets up to about 89% correct on the first task\cb1 \
\cb4 um this is slower the backrop alpha alpha is the step size so um this Red\cb1 \
\cb4 Alpha is is 10 times faster than the orange Alpha okay with orange Alpha you\cb1 \
\cb4 at least after a while maybe get up to a higher level but uh this one's the fastest at\cb1 \
\cb4 the very beginning okay does this all make sense okay now the linear Baseline\cb1 \
\cb4 is well what if you didn't have all this network stuff you just took the the pixels and you learned a linear map from\cb1 \
\cb4 the pixels to the class that's actually not you can do\cb1 \
\cb4 much better than than random what's random random is two classes so it's 5050 50% would be random\cb1 \
\cb4 um yeah and I what else I the shade region is one standard error linear Baseline is importance of the L of the\cb1 \
\cb4 linear heads there's no nonlinearity there's there just a single well two\cb1 \
\cb4 units and so this doesn't have to be run actually with with uh multiple tasks\cb1 \
\cb4 because you know there's nothing to save from one task to the next the two heads are replac the two linear heads are replaced that's the whole network so you\cb1 \
\cb4 don't have to run it many many times well you have to we to run it many times and we establish that you could get 77%\cb1 \
\cb4 approximately correct with with you uh with a linear system okay so we don't\cb1 \
\cb4 that's that's really poor performance right that would be bad because you're not even using all that Network to do\cb1 \
\cb4 anything okay so maybe we're improving at the beginning this guy seems to be\cb1 \
\cb4 improving but what happens in the long run okay that's the real question so this is what happens in the long run\cb1 \
\cb4 and so these are this red and the and the orange are the same curves you saw\cb1 \
\cb4 previously just extended so let's go with the red one which started out at\cb1 \
\cb4 89% which is this point right but this first data point is an a is a bin an\cb1 \
\cb4 average over 50 Ex because there's yeah just for graphing purposes to get rid of\cb1 \
\cb4 all the jaggies it's good to average over the first 50 tasks and so over that\cb1 \
\cb4 first 50 tasks you really can only do about 84% correct because you're already\cb1 \
\cb4 starting to decrease in performance and and then you see that playing out uh as\cb1 \
\cb4 you go more and more tasks you get worse and worse until you're performing worse\cb1 \
\cb4 than just a linear Network and if you use a different set of\cb1 \
\cb4 parameters uh if you're slower you also get worse and worse and then level out\cb1 \
\cb4 at just a little bit better than the linear Network this is an even slower\cb1 \
\cb4 step size and it also does poorly okay\cb1 \
\cb4 so this is this is this is one uh setup\cb1 \
\cb4 but it's representative we've seen this pattern over and over again uh if you vary the details you can get small\cb1 \
\cb4 variations in different different shapes but um this is this is this is the\cb1 \
\cb4 pattern that you always that you will always get um and in the long run you can't do better than the much\cb1 \
\cb4 significantly better much better than the linear Baseline and also if you introduce variations like bring in atom\cb1 \
\cb4 atom actually makes it worse Dropout makes it worse any of these things just makes performance worse\cb1 \
\cb4 um okay so what's the summing for good hyper parameters plus decreases across\cb1 \
\cb4 tasks nearing the performance of the one layer line your network or worse so this\cb1 \
\cb4 is what I'm calling catastrophic loss of plasticity question have any explanation for why is\cb1 \
\cb4 that an al01 you keep seeing this city but that smaller Alphas of level\cb1 \
\cb4 out and retain some Bas\cb1 \
\cb4 um we haven't thought about arguing that the orange line is good you think the\cb1 \
\cb4 orange line is a failure uh\cb1 \
\cb4 um I've never asked the students who did this all the students I listed Shaban\cb1 \
\cb4 doare and Fernando uh all these about that\cb1 \
\cb4 particular question I'm not sure I can answer that any other\cb1 \
\cb4 questions thank you\cb1 \
\cb4 how do we explain uh how the like the the the brown line is definitely\cb1 \
\cb4 increasing and as we saw the orange line also if we looked at detail at the first 10 tasks it also was increasing so like\cb1 \
\cb4 uh that's that's what Alex called for that we should get some improvement in the beginning and so that makes sense to\cb1 \
\cb4 Alex and the The Way We Were understanding it was that it was you learn features during the early task and\cb1 \
\cb4 those features are later are are useful on on the subsequent tasks uh and if you're particularly if\cb1 \
\cb4 you're learning slowly then then then uh well yeah empirically we're seeing\cb1 \
\cb4 that if you learn slowly then you're getting a much greater effect of that nature you're getting savings across the\cb1 \
\cb4 tasks what you learn maybe you maybe takes a while to find good\cb1 \
\cb4 features and if you can only learn really slowly um that you get more accumulation\cb1 \
\cb4 over over over over uh over tasks okay\cb1 \
\cb4 um so there are better algorithms as I said things like Dropout\cb1 \
\cb4 and standard variations make things worse\cb1 \
\cb4 um uh but there are a few this is this red\cb1 \
\cb4 line is the same line we saw before we've just rescaled it you know he's going way down low uh but but if you\cb1 \
\cb4 have a good algorithm you can you can show Improvement so um this is shrink and perturb and L2 regularization L2\cb1 \
\cb4 regularization just means you something you add something that encourages the weights from getting too big prefer\cb1 \
\cb4 small weights and so this uh keeps them in a in a in a a labile range so they\cb1 \
\cb4 can keep training keep learning from more experience um and I I'll show you\cb1 \
\cb4 some details on that in a minute shrinking perturb does L2 regularization but it also does a random perturbation\cb1 \
\cb4 of the out of the of the weights and so these two are good we're going to repeatably see these are good at\cb1 \
\cb4 partially solving the problem and continual backrop is our own algorithm\cb1 \
\cb4 um and it's basically just like backrop it's falling gradi descent but in\cb1 \
\cb4 addition uh we select just a very small proportion of the units and we the very\cb1 \
\cb4 the proportion that are least useful to the network and we reinitialize\cb1 \
\cb4 them so like less than one unit per example very small fraction of them and\cb1 \
\cb4 we have a utility measure so we can measure their util how much they're contributing to the Network's uh\cb1 \
\cb4 behavior and if the ones that are used least uh will be candidates for being\cb1 \
\cb4 reinitialized and that small change uh solves this problem you have a question\cb1 \
\cb4 online about how would you select yes it's a I'll talk about that in a minute let me let me just postpone\cb1 \
\cb4 that um but it is it's a measure of its utility and as you'll see many of the\cb1 \
\cb4 units are not useful at all this is what this is a phenomenon of backrop okay okay and maybe I have it\cb1 \
\cb4 here continual back propop scas grading descent with Selective reinitialization just like backrop except\cb1 \
\cb4 reinitialization this this are only parameter with the raid which we reinitialized I said it's very small\cb1 \
\cb4 very slow um that's what that is a hyperparameter and it's it's it's we\cb1 \
\cb4 show that it's better to reinitialize selectively using a measure of utility\cb1 \
\cb4 so for example shrink and perb also random makes random changes um but it\cb1 \
\cb4 doesn't do it selectively it's always just taking all the weights and wiggling around a little bit and this is an old\cb1 \
\cb4 idea uh it's really sort of a kind of generated in test we we're keeping the\cb1 \
\cb4 units that are contributing to the network and we are taking ones that are not contributing and we're going to\cb1 \
\cb4 randomly vary them um and this is\cb1 \
\cb4 this this idea existed before uh rupan mmud and I did some work\cb1 \
\cb4 on it it goes back to the uh 60s like Oliver Selfridge did pandemonium and it\cb1 \
\cb4 also was like this generating and units in a network and but the thing about\cb1 \
\cb4 continual backdrop is it extends the idea to General multi-layer\cb1 \
\cb4 networks okay so um I I want to show you in some sense\cb1 \
\cb4 convolution networks are old school old style uh now we want to do residual networks which also has many layers like\cb1 \
\cb4 18 layers but we also have has cut shortcut connections or cut through\cb1 \
\cb4 connections so you don't only it's not strictly layered anymore this is more more modern\cb1 \
\cb4 architecture and we're also going to change the problem a little bit we're going to use uh CFR 100 and we're going\cb1 \
\cb4 to present five classes five classes and then we're going to progressively add\cb1 \
\cb4 more so we're not going to like like before we had pairs another pair a different pair just in different pairs\cb1 \
\cb4 same number now we're going to add we start with five then we'll add five more so we'll have 10 so so this is the the\cb1 \
\cb4 old five we're still going to present those yeah and we're also going to present the new five and the next we'll\cb1 \
\cb4 present the old this 10 will keep presenting all that 10 and we'll get five more so there'll be 15 and we just\cb1 \
\cb4 keep going until you get all 100 of them okay now if you think about that the\cb1 \
\cb4 second task will be harder than the first one right the first one just has five classes this one has 10 classes\cb1 \
\cb4 next one have 15 classes so you expect overall accuracy to go down over time and see can't measure overall accuracy\cb1 \
\cb4 if you want to see a loss of performance what we want we want what we use is we show performance relative to a network\cb1 \
\cb4 that's trained from scratch with the same number of classes\cb1 \
\cb4 so say you had 50 classes you could you could just reinitialize your network give it\cb1 \
\cb4 50 in the all way the 50 are all presented at once and that'll perform a certain well and then you could do it in\cb1 \
\cb4 this increment mental way you get 5 10 15 20 so farth and and and what what\cb1 \
\cb4 what will it be will be will it be uh after you get to the 50 will you be\cb1 \
\cb4 performing will you be performing worse or or better than if you see them all\cb1 \
\cb4 all all all at once does the progressivity help you or hurt you okay\cb1 \
\cb4 and what we find is quite so this the y- axis here is the difference between\cb1 \
\cb4 these two did you do better or worse because you're presenting them incrementally got it okay so what we\cb1 \
\cb4 find is that yeah there's definite effect of of you do better uh by doing\cb1 \
\cb4 it incrementally at the beginning you you you gain a couple\cb1 \
\cb4 percentage points of accuracy which is quite a significant effect and then it\cb1 \
\cb4 decreases and if you do usual back poop after about I don't know 40 45 uh it's\cb1 \
\cb4 neutral and then and then you get this loss of plasticity effect where you do much much worse than uh and presenting\cb1 \
\cb4 them all together at once shrink of a turb isn't quite as bad but it's bad\cb1 \
\cb4 you're you're losing you're losing plasticity and continual backdrop you're able to maintain the same level you get\cb1 \
\cb4 exactly what Alex asked for get an improvement at the beginning and then doesn't you\cb1 \
\cb4 plateau okay so that's that's the Z pattern yeah good because you you said\cb1 \
\cb4 you keep the same performance in the long run with Contin backr but at least are you able to reach that performance\cb1 \
\cb4 sooner than with a Network that has been trained from a scratch with with all of them at once so is a learning time at\cb1 \
\cb4 least faster with continual back Pro than when you because we're trying to see the\cb1 \
\cb4 advantage right of just throwing all the data at once to a network learn all the\cb1 \
\cb4 methods are are getting an advantage in in learning time at the beginning right\cb1 \
\cb4 that's what we see yeah that's F but at at the end at the end there there\cb1 \
\cb4 is and then we're measuring performance only on on the full set yeah but my\cb1 \
\cb4 question then is like since performance is the same did you notice that uh continual backrop was reaching that\cb1 \
\cb4 performance sooner than that Network learning from a scratch\cb1 \
\cb4 I don't think that I don't think there is any measure of of sooner\cb1 \
\cb4 um you mean once you're say presenting all 50 of them yeah I mean we so so in\cb1 \
\cb4 the Progressive case yeah you've been you've presenting you've been presenting them in smaller groups all along and so\cb1 \
\cb4 for most of the classes by the time you get to all 50 of them you've already learned a lot that will to be faster but\cb1 \
\cb4 I wanted to confirm if you well you'll you'll get you know 45 of them pretty\cb1 \
\cb4 pretty well at the beginning remember the performance measure is at the at\cb1 \
\cb4 the actually I didn't I didn't say what the performance measure exactly is here and maybe I'm not sure but I am sure for\cb1 \
\cb4 the previous example previous example and I think we're doing the same sort of thing is we present things and and then\cb1 \
\cb4 and then we have this test set so we we we finish learning and then we measure performance there isn't there isn't a\cb1 \
\cb4 speed within within the group you know what I mean is asking is for like the number of\cb1 \
\cb4 classes 50 for example when you train the network from Ranch 50 classes you present as many examples to that Network\cb1 \
\cb4 as you present to the network that doing yeah definitely the same number of examples that's is slower it has seen 49\cb1 \
\cb4 times the train set sence no no no the total is the same the total is the same yeah well the\cb1 \
\cb4 one that is trained yeah yeah is that right\cb1 \
\cb4 yeah question maybe I'm not\cb1 \
\cb4 sure but this I think you'll find interesting here we're looking at we're looking inside the network trying to\cb1 \
\cb4 figure out what's happening okay so as we increase the number of classes\cb1 \
\cb4 this this this graph is showing the percentage of units that are inactive that are dormant or dead they're which\cb1 \
\cb4 means they're defining that as active less than 1% of the time and so you\cb1 \
\cb4 start out with small random weights everyone's active oh I guess I guess\cb1 \
\cb4 active means um they're they're reu remember so if the activity is less than\cb1 \
\cb4 less than uh zero they're they're capped at zero B floored at zero and so active\cb1 \
\cb4 means uh their their activity is greater than zero and uh at the beginning\cb1 \
\cb4 they're all they're going to be greater than zero half the time and so none are\cb1 \
\cb4 active uh less than 0% of the time and then as as we as we train just for\cb1 \
\cb4 standard backdrop um yeah actually I should have said that\cb1 \
\cb4 in in this in this experiment if you do standard back poop you don't even you you you you don't perform it doesn't\cb1 \
\cb4 work at all you have to already use L2 regularization so so that's why in these\cb1 \
\cb4 experiments we don't show you results for L2 regularization because everything is using L2 regularization that's that's needed okay\cb1 \
\cb4 so that anyway that based learning base Learning System\cb1 \
\cb4 um gains up or as increasing numbers of of dormant units more than more than\cb1 \
\cb4 half by the end of 100 tasks um 100 classes 100 classes and um\cb1 \
\cb4 shrink and perturb does much better in terms of many fewer dormant units and continual backrop has hardly any and we\cb1 \
\cb4 also measured this the diversity of the representation\cb1 \
\cb4 uh uh which means often often you have fures that that become similar to one\cb1 \
\cb4 another and they they don't add anything to the uh linear rank of the\cb1 \
\cb4 uh the you have you measure the the number of uh I don't even know how to\cb1 \
\cb4 say it uh number of independent components of in in the uh in the in the\cb1 \
\cb4 features in in the network um and so we scale this between zero and one and we\cb1 \
\cb4 we can see quite quite clearly that just based Learning System uh the uh we lose\cb1 \
\cb4 diversity we lose rank of the representation and with some of the other methods we can U preserve\cb1 \
\cb4 diversity okay now I tried to say here's here's where I emphasize the robustness\cb1 \
\cb4 and in these problems and we've done other problems we've done idealized problems we've done amnest and we're\cb1 \
\cb4 shortly I'll be doing reinforcement learning so across network architectures uh across activation functions not just\cb1 \
\cb4 just ru but a whole variety of them and across the hyper parameters we see that just plain the Deep supervised learning\cb1 \
\cb4 loses P SE dramatically and continual setting L2 regularization improves it\cb1 \
\cb4 but just a little bit uh or make and and shrink and perturb often will help a\cb1 \
\cb4 little bit further weight with the weight randomizing continual back propop does the best in maintaining the\cb1 \
\cb4 plasticity it has one hyperparameter but as long as it's uh it's it's we're in\cb1 \
\cb4 sensitive to that it's just can be set very small so now let's go on and do finally\cb1 \
\cb4 reinforcement learning so we're going to do ant Locomotion um maybe you've seen this problem we\cb1 \
\cb4 have this ant uh I thought I have some videos here let me scoop ahead yeah some\cb1 \
\cb4 videos so here's the ant on the right side um moving rapidly forward the object is\cb1 \
\cb4 to move forward as rapidly as you can this is an ant that's not moving forward\cb1 \
\cb4 at all it's doing very poorly this one's doing very well and we're doing this we\cb1 \
\cb4 measure the the the reward is forward motion and the uh actions is to control\cb1 \
\cb4 these eight joints where they are red marks and this is a standard task and uh\cb1 \
\cb4 here the performance we're showing the time steps we're training for rather a long time often you will only change\cb1 \
\cb4 train for a million or two time steps we're going to go 20 million and we're\cb1 \
\cb4 going to measure the rewards per episode which is one trip of running forward as\cb1 \
\cb4 fast as you can then you're brought back to the beginning to to run some more and\cb1 \
\cb4 here we have a non-stationary problem and that means that um we're going to\cb1 \
\cb4 vary the friction between the feet and the ground so every two million steps\cb1 \
\cb4 we're going to vary friction we're vary it don't I have a\cb1 \
\cb4 I don't have a figure for that but sometimes it's fast sometimes it's the feet are sticky and sometimes they're\cb1 \
\cb4 slippery and um so it changes the nature of the problem every time you you uh\cb1 \
\cb4 vary the friction with the ground so these um sort of actually I call this an\cb1 \
\cb4 alligator graph because it looks like the back of an alligator's tail you uh you're doing well and then there's a\cb1 \
\cb4 switch in the problem and you your performance Falls and then it recovers s and recovers and um so let's first look\cb1 \
\cb4 at the standard reinforcement learning algorithm po um and uh it it does well at the\cb1 \
\cb4 beginning but then at a certain point it plateaus and if you just keep training keep training um it degenerates and your\cb1 \
\cb4 ant that used to look like like this one oh think I must have skipped the slide\cb1 \
\cb4 yeah this was supposed to show the uh the slipperiness uh we're going to vary that\cb1 \
\cb4 and um and uh here's our standard poo\cb1 \
\cb4 performing well but if we just keep going it starts to end up looking like that second one starts looking like this\cb1 \
\cb4 guy and uh performance is actually uh deten so it's worse than how you\cb1 \
\cb4 started and you can so this is this\cb1 \
\cb4 so people have seen this before but you know when this happens they tend to just\cb1 \
\cb4 uh stop they they stop training early on and if you stop training you can you can\cb1 \
\cb4 avoid this this this this fall but the real Learning System would be faced with that okay\cb1 \
\cb4 this Ali online asking could a similar effort be seen if Mass was changed\cb1 \
\cb4 instead of friction excuse me could a similar effect be seen if Mass was\cb1 \
\cb4 changed instead of friction yeah we believe so have every reason to believe so as long as there's something well\cb1 \
\cb4 actually I guess I should move a little more quickly because I'll answer that in the next in the next slide um but yes\cb1 \
\cb4 the quick answer is yes okay now if we tune the learning algorithm we can do better but we still see a a drop uh a\cb1 \
\cb4 loss of plasticity over time and as before L2 L2 and U continual backrop can\cb1 \
\cb4 largely solve the problem largely solve the problem now I want to show you\cb1 \
\cb4 um this one this is the same thing except there's no changing in friction\cb1 \
\cb4 there's just just regular ant okay and\cb1 \
\cb4 um but reinforcement learning does involve changes it always involves changes because the policy gradually\cb1 \
\cb4 changes and um we it involves temporal difference learning temporal difference learning the targets are always changing\cb1 \
\cb4 so really even without changes even in a stationary problem you should get this\cb1 \
\cb4 we get this effect we we realize later we get the same effect and performance just drops and this is where I say well\cb1 \
\cb4 why didn't why doesn't everyone see this well they do see this and they just they just stop training after a few million\cb1 \
\cb4 and they don't they they cuz you arguably it's okay you're just interested in a good policy you can stop\cb1 \
\cb4 and use that policy but if you want to keep learning it's a big problem now if you tune your parameters you last longer\cb1 \
\cb4 and you get higher but still you you suffer and you lose all your plasticity\cb1 \
\cb4 regularization uh can maintain it and continual back bout can maintain it\cb1 \
\cb4 better okay so here now we'll take a closer look inside this algorithm and again we look at the percentage of\cb1 \
\cb4 dormant units and those are increasing to more than half L2 and\cb1 \
\cb4 continual backrop keeping it low stable rank stable rank is the same pattern and here's a new one the average weight\cb1 \
\cb4 magnitude you just looking at the units and look at how big the weights are and with standard backrop the weights are\cb1 \
\cb4 getting bigger and bigger and bigger there's no there's nothing that that forces them to be small and maybe\cb1 \
\cb4 that's the full explanation just the weights get bigger and so they're harder to change um L2 regularization makes them forces\cb1 \
\cb4 them to be small and maybe that will interfere with performance um and so\cb1 \
\cb4 they give some insight into what's what's going on so conclusions deep learning networks\cb1 \
\cb4 are optimized for onetime learning in a sense they totally fail for continual learning and these simple changes like\cb1 \
\cb4 continual backrop can make them effective for continual learning we going to rank units by the utility to\cb1 \
\cb4 the network preserves the most useful so I guess I never got into details of utility and I guess that's because I\cb1 \
\cb4 don't think the specific way we rank them we don't think it's necessarily the very best\cb1 \
\cb4 uh um in fact that's one thing we're working on now how to make how to do it uh better in a better way um you know\cb1 \
\cb4 what if your what if your network is recurrent or what if you're right now we're using a fairly local measure we\cb1 \
\cb4 have the units each unit is looking at its outgoing weights and if the the immediately outgoing weights if they're\cb1 \
\cb4 all zero Z for example you know you're not useful uh but they may some of them may be large um but you don't know that\cb1 \
\cb4 you're connecting to someone who's your large weight is to a unit that is useful which which would be necessary in order\cb1 \
\cb4 for you to be useful so um that we're still experimenting with that um I think\cb1 \
\cb4 there's a large exciting World ahead of deep learning networks can learn continually and and particularly it\cb1 \
\cb4 opens up possibil new possibilities for reinforcement learning which is inherently continual and modelbased\cb1 \
\cb4 reinforcement we have all these different components that that are interacting and all of which are\cb1 \
\cb4 learning simultaneously and continually okay so that's my completion\cb1 \
\cb4 of the demonstration part and now let me just say a few I could I could say a few\cb1 \
\cb4 words about uh ways we're going to try to solve\cb1 \
\cb4 these problems further but it's a good time for questions right now there are a few online actually yeah good so one\cb1 \
\cb4 very link to what you were presenting was what will happen for continual backprop without the L2\cb1 \
\cb4 regularization well it is it is necessary uh in these later in later\cb1 \
\cb4 problems to make it work well so so it wasn't in the first\cb1 \
\cb4 problem um so I I don't know just doesn't perform as well in in some cases yeah\cb1 \
\cb4 kis who as well ask what kind of Advantage continual learning can gain over the normal deep learning well it's\cb1 \
\cb4 it's it's it's really a change in the problem right we're asking a continual problem\cb1 \
\cb4 where well I guess backrop uh uh reinforcement learning is always\cb1 \
\cb4 continual um let see I guess didn't I just show\cb1 \
\cb4 that didn't I just show that if you have a changing problem and you need to do\cb1 \
\cb4 continual backrop or regular back poop totally okay let's just go to uh in Chris so just to wrap my head around a\cb1 \
\cb4 bit more and just kind of talking back yeah their queries on efficiencies of like Baseline training say on like you\cb1 \
\cb4 know and number of tasks versus like n minus 5 to end tasks um I understand\cb1 \
\cb4 that in the supervised setting the Advantage may be more for like fine tuning or something like this where you\cb1 \
\cb4 have like some existing model and you want to addal tasks or something like this yeah so like if say we're doing\cb1 \
\cb4 large language models and we we train them up and then like another uh another\cb1 \
\cb4 week's information comes out on the internet new news stories and You' like to have your large language model be up\cb1 \
\cb4 to date with the new news stories that then you it is that's a big problem nowadays what they actually do\cb1 \
\cb4 nowadays is when they they they update the miles infrequently because every time they\cb1 \
\cb4 update them they have to uh update they have to clear out all the whole network the old Network they clear it to scratch\cb1 \
\cb4 start over from small weights and they train all the data at once they have to start all over again you'd like to be\cb1 \
\cb4 able to just add the new data and like the reinforcement context seems like a lot\cb1 \
\cb4 more it's even it's even from like uh from initial training it seems like it's\cb1 \
\cb4 better because reinforcement learning well more continuous in nature so it seems like in the super bu setting fine\cb1 \
\cb4 tuning is like like sort of this key application because that way you don't have to waste old models whereas in\cb1 \
\cb4 reinforcement learning it's kind of a new paradigm training does that make\cb1 \
\cb4 sense got to say it more concisely sure so in the supervis setting it seems that\cb1 \
\cb4 fine-tuning sort of to me it's seems it's more of the main application because if it's both Cas you need\cb1 \
\cb4 continue learning really uh so just going back to their question if the uh\cb1 \
\cb4 if it's accuracy is the same as the Baseline uh with continual learning and\cb1 \
\cb4 with you know that Baseline number of tasks what's the sort of notable\cb1 \
\cb4 Advantage unless you use like less compute in the continual learning setting\cb1 \
\cb4 number increase the number the other one is one for one fixed number sure life\cb1 \
\cb4 you will always have more numbers coming in so yeah but if you're adding more numbers that's a fine tuning task right\cb1 \
\cb4 if you're adding new tasks later on after initially training a base model are you suggesting that fine tuning is\cb1 \
\cb4 an alternative to continual learning well no I'm just suggesting that for supervised learning it seems like this\cb1 \
\cb4 the continual back propagation seems like more of the uh the usefulness\cb1 \
\cb4 because if the if you're getting the same level of accuracy um getting the same as a as like you know a baseline\cb1 \
\cb4 number of tasks if you go back to the graph right when you have say 50 tasks right when\cb1 \
\cb4 you're comparing it to the continual back propagation the the accuracy is about\cb1 \
\cb4 the same which I showed you're not getting the same level performance um in\cb1 \
\cb4 in sort of the the prior chart right on the supervised supervised task that's sort of what it looks\cb1 \
\cb4 like this was just sort of like you know we'll talk later I'm obviously\cb1 \
\cb4 confused than you hello uh excuse me if this is a bit\cb1 \
\cb4 of a critical question but in the in the evaluation of the where you\cb1 \
\cb4 add classes uh continuously um like you start out with five classes and then add another five\cb1 \
\cb4 and so on uh and you said you mentioned that you you trained on uh equal amount of epoch I assume it\cb1 \
\cb4 could the deterioration of the normal back propagation not just be explained by the fact that it sees fewer examples\cb1 \
\cb4 of the new classes yeah I guess I I was UN unclear\cb1 \
\cb4 about that I and and I I am unclear in my own mind exactly how that's done but\cb1 \
\cb4 uh absolutely the the intent was to do a fair comparison neither one would have\cb1 \
\cb4 more data and yeah I guess I'm not clear how that was done sorry about that no wor\cb1 \
\cb4 sorry what happened to the utility of the recently reinitialized weights you\cb1 \
\cb4 said you reinitialize them based on lowest utility the recently reinitialized ones jump in or or say low\cb1 \
\cb4 this one I can answer when they're reinitialized when I said they reinitialized in the standard\cb1 \
\cb4 way but it's a little bit subtle just a tiny bit subtle um we reize the incoming\cb1 \
\cb4 weights to the new unit to be random in the in the ordinary way but the outgoing weights are re initialized to zero\cb1 \
\cb4 because we don't want them to interfere with the Uno the current performance of the system if the outgoing weights were\cb1 \
\cb4 nonzero they would mess things up a little bit um and then so then um this new this\cb1 \
\cb4 newly initialized unit what will its utility be its utility will be zero because it's hard going with all zero\cb1 \
\cb4 okay so if we really um ranked by strictly by utility when you add a new one that's\cb1 \
\cb4 the one you're G to throw away and reinitialize the next time so we do need to to keep track of the of the age of a\cb1 \
\cb4 of a unit and uh prevent a a newly created unit that\cb1 \
\cb4 has that's very very young from being uh placed good job okay um whether do you\cb1 \
\cb4 see um research in dip learning and reinforcement learning going do you\cb1 \
\cb4 think the focus will be in modifying the backprop algorithm for the continual\cb1 \
\cb4 learning and reinforcement learning setting like in this paper or do you see it going somewhere\cb1 \
\cb4 else well what do we consider it's continual back propop modifying the algorithm backrop algorithm you know\cb1 \
\cb4 it's it sounds like it is it's an adjective onto back propop yeah I think I think I think it it is it's the\cb1 \
\cb4 continual backrop is almost like uh uh more more backrop what is backrop backrop is you do initializing and then\cb1 \
\cb4 you degrad in descent we're just going to do initializing all the time okay as you go along so it's is very much in the\cb1 \
\cb4 same Spirit um and and and and um maybe it's not a new algorithm and what will\cb1 \
\cb4 happen in the future uh do we envision algorithms that are very different from from continual back propop or is that is\cb1 \
\cb4 that about it and I think I think we need to go significantly Beyond backrop\cb1 \
\cb4 and continual backrop uh for for the best um to to achieve all of our goals what\cb1 \
\cb4 we would like from continual learning as I want from continual learning I not only want this bare ability to not lose\cb1 \
\cb4 all your plasticity but I also want to do things like um\cb1 \
\cb4 uh meta learn improve the way I generalize special thing about continual\cb1 \
\cb4 learning is you you learn some and then you learn more and then you learn more\cb1 \
\cb4 so you have this experience with learning and you can see the way I learned at this time that worked out as\cb1 \
\cb4 so so now the way I learn this way it back a little bit better you can evaluate how well your learning has G on\cb1 \
\cb4 this something you can't do if you learn in one shot you learn in one shot just done but if you learn and then you learn\cb1 \
\cb4 more and learn more then you can you can learn to learn say this way of learning works better than that way so if that\cb1 \
\cb4 seems too abstract for you let maybe concrete um uh gen how would you like to would\cb1 \
\cb4 you generalize more on this feature or generalize more on that feature we'd like to to learn that we'd\cb1 \
\cb4 like to learn how to generalize modern deep BL systems don't really learn how to generalize they are structured to\cb1 \
\cb4 generalize in a particular way and maybe they were lucky and they've generalized pretty pretty well or maybe that's the\cb1 \
\cb4 skill of their designers but there definitely is no algorithm that's causing them to learn well and I think\cb1 \
\cb4 we want to have our algorithms learn to generalize well um and so so that's what\cb1 \
\cb4 that's part of the ambition for continual learning it's not going to come just from tweaking back poop we\cb1 \
\cb4 have to add more things and let me just use that we can still have questions but let me uh say a few\cb1 \
\cb4 of these things about the new ideas uh we want methods that we'll learn continually of course we want we want to\cb1 \
\cb4 be able to learn nonlinear functions of course we want them to be very efficient but the last point is we want them to\cb1 \
\cb4 meta learn to generalize better and yeah and this is the slide I want to do\cb1 \
\cb4 ideally a streaming deep learning or or uh what you call Dynamic deep learning\cb1 \
\cb4 should adapt to three levels you should adapt the weights that's the usual one you should also adapt the step sizes\cb1 \
\cb4 every weight should have its own Step At least every weight should have its own step size which a step size just it's\cb1 \
\cb4 sometimes called a learning rate but it's it's a it's the amount by which you move each weight and so you want you\cb1 \
\cb4 want the I I'm claiming we want the ability to to learn at different rates\cb1 \
\cb4 learn faster in some weights and slower in some weights and that this is is how we will get to sculpt our\cb1 \
\cb4 generalization is that confusing like if you have a feature that has uh a large\cb1 \
\cb4 weight then I mean a large step size then you will change its weight a lot\cb1 \
\cb4 when you when that feature is uh present and that means you'll generalize a lot along that feature or the other way if\cb1 \
\cb4 the if that feature has a a zero step size or very small step size then you will not generalize based on that does\cb1 \
\cb4 that sound good does that sound bad do you don't generalize no in a continual learning in in an agent like like me or\cb1 \
\cb4 you there are lots of things that we know that we don't want to learn we don't want to change them anymore because they're just very reliable we've\cb1 \
\cb4 learned over large parts of our life we don't want to change those and we don't\cb1 \
\cb4 want to change those because there are other things we do want to change we are we are actually very skilled at\cb1 \
\cb4 assigning credit to features that are most likely to be relevant and one way to do that is by having different step\cb1 \
\cb4 sizes uh for different weights in principle I'm thinking in principle if we could do that we will get we'd get a\cb1 \
\cb4 whole lot of desirable things including generalization um okay and then the\cb1 \
\cb4 third area we'd like to adapt is the connection between the units this is sort of a\cb1 \
\cb4 a a starting view of the goal for a better A Better Learning algorithm than\cb1 \
\cb4 regular back poop or continuing back poop okay there are lots of questions\cb1 \
\cb4 we we are running out like we're already above the time so true there's going to\cb1 \
\cb4 be an extended QA now we can keep on these questions if you guys want to ask but I want to fre people who might have\cb1 \
\cb4 their commitments so quickly um so our next seminar will be Maxim you knew\cb1 \
\cb4 better the date was it 18 or when was that of November uh so we'll be with um\cb1 \
\cb4 G Kath head of Quant technology at Bloomberg uh from New York um we hope to\cb1 \
\cb4 see many of you there as well know that we also have a reading group if you want to uh dive more into especially in\cb1 \
\cb4 enforcement learning that's what we're focus on uh please come to say hi and\cb1 \
\cb4 also one of sponsors ionic is um having some opening for stent internships so is\cb1 \
\cb4 on video games and Ai and enforcement learning stuff so if you're interested\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 also come to speak to us and that's all go well\cb1 \
}