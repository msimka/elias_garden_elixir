{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Arial-BoldMT;\f1\fswiss\fcharset0 ArialMT;}
{\colortbl;\red255\green255\blue255;\red237\green237\blue237;\red25\green25\blue25;\red0\green0\blue0;
\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c94510\c94510\c94510;\cssrgb\c12941\c12941\c12941;\cssrgb\c0\c0\c0;
\cssrgb\c100000\c100000\c100000\c10196;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\b\fs43\fsmilli21600 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Why care about GFlowNets?
\f1\b0\fs20 \cf0 \cb1 \strokec4 \
\pard\pardeftab720\partightenfactor0

\fs28\fsmilli14400 \cf2 \cb3 \strokec2 Why should you care about GFlowNets?\cb1 \
\cb3 Is it the next Transformer?\cb1 \
\cb3 Is it Yoshua bengio's pet project?\cb1 \
\cb3 Or is it one of those ideas that are so 2010s\'a0\'a0\cb1 \
\cb3 when all the cool kids are training\'a0 large language models in the 2020s?\cb1 \
\cb3 Today, I'll talk about why you should care\'a0 about GFlowNets and why it is the future.\cb1 \
\cb3 My name is Edward Hu. I'm not exactly\'a0 impartial here because Yoshua Bengio\'a0\'a0\cb1 \
\cb3 is my PhD advisor, and I directly worked\'a0 on GFlowNet, but I also love what works!\cb1 \
\cb3 I invented low-rank adaptation, or\'a0 LoRA, when I was a researcher at\'a0\'a0\cb1 \
\cb3 Microsoft working with GPT-3 -- and\'a0 by the way here's a video on LoRA.\cb1 \
\cb3 Today, I'm a research scientist at\'a0 OpenAI. So, I want to talk about what\'a0\'a0\cb1 \
\cb3 makes GFlowNets so exciting, and how\'a0 it's going to shape the future of AI.\cb1 \
\cb3 First of all, GFlowNet sounds like a type of\'a0 neural network like a Transformer or a ResNet.\cb1 \
\cb3 However, it is not. GFlowNet stands for generative\'a0 flow networks and is a learning algorithm.\cb1 \
\cb3 And before I tell you more\'a0 about the algorithm itself,\'a0\'a0\cb1 \
\cb3 I'm going to start with the problem it solves.\cb1 \
\pard\pardeftab720\partightenfactor0

\f0\b\fs43\fsmilli21600 \cf2 \cb3 The problems GFlowNets solve
\f1\b0\fs20 \cf0 \cb1 \strokec4 \
\pard\pardeftab720\partightenfactor0

\fs28\fsmilli14400 \cf2 \cb3 \strokec2 So, if you ask an AI practitioner\'a0 what their worst nightmare is,\'a0\'a0\cb1 \
\cb3 most people will tell you it's either\'a0 over-fitting or hyper-parameter tuning.\cb1 \
\cb3 And by the way, I have another video\'a0 on muTransfer which is a technique that\'a0\'a0\cb1 \
\cb3 allows you to tune hyper-parameters\'a0 for a large model much more easily,\'a0\'a0\cb1 \
\cb3 which I'll link here, but today we're\'a0 going to talk about overfitting.\cb1 \
\cb3 Overfitting usually happens when we\'a0 ask the model to maximize something.\cb1 \
\cb3 For example, we might be maximizing the likelihood\'a0 of a dataset; the best way -- the perfect way\'a0\'a0\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 -- to maximize the likelihood of a dataset is to\'a0 memorize it completely without "understanding" it.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 And if you just memorize the\'a0 answers to certain questions,\'a0\'a0\cb1 \
\cb3 it's not going to help you to generalize\'a0 to the questions you've never seen before.\cb1 \
\cb3 And by the way, something similar happens\'a0 in reinforcement learning as well,\'a0\'a0\cb1 \
\cb3 where if you maximize the reward,\'a0 very often what you find is hacks.\cb1 \
\pard\pardeftab720\partightenfactor0

\f0\b\fs43\fsmilli21600 \cf2 \cb3 A concrete example: drug discovery
\f1\b0\fs20 \cf0 \cb1 \strokec4 \
\pard\pardeftab720\partightenfactor0

\fs28\fsmilli14400 \cf2 \cb3 \strokec2 I'm going to give you a really\'a0 concrete scenario where this shows up.\cb1 \
\cb3 Say you're inventing a new drug\'a0 molecule through trials and errors.\cb1 \
\cb3 What you're doing is basically reinforcement\'a0 learning albeit with a really expensive reward\'a0\'a0\cb1 \
\cb3 function, because the real reward function\'a0 is: you take this drug and run a clinical\'a0\'a0\cb1 \
\cb3 trial and you get a reward in the end,\'a0 but it's extremely slow and expensive.\cb1 \
\cb3 What people do in practice is\'a0 they collect some clinical data\'a0\'a0\cb1 \
\cb3 and then they train a neural network\'a0 to simulate the real reward function.\cb1 \
\cb3 Now, you can imagine what happens\'a0 if you maximize a reward under\'a0\'a0\cb1 \
\cb3 this proxy model that is far from perfect.\cb1 \
\cb3 You're going to find a molecule that obtains\'a0 an extremely high reward under this model,\'a0\'a0\cb1 \
\cb3 but the chance of that molecule being actually\'a0 the drug you want is low, because there's so\'a0\'a0\cb1 \
\cb3 many ways to trick a neural network into\'a0 thinking a molecule has a high reward.\cb1 \
\cb3 However, this reward function is still\'a0 capturing some information about drug\'a0\'a0\cb1 \
\cb3 worthiness, even though we don't trust it 100%.\cb1 \
\cb3 Practically speaking, we don't just want the\'a0\'a0\cb1 \
\cb3 single best molecule under this\'a0 reward -- we want many good ones.\cb1 \
\cb3 Even better, these good molecules should be\'a0 as different as possible from one another.\cb1 \
\cb3 What we do then is we try\'a0 them all in the real world,\'a0\'a0\cb1 \
\cb3 and hopefully some of them are actually good.\cb1 \
\cb3 If we take just the best one,\'a0 it is almost guaranteed that\'a0\'a0\cb1 \
\cb3 this one molecule is exploiting some\'a0 imperfections in our reward model.\cb1 \
\cb3 In this example here, I'm highlighting\'a0 the importance of having diversity as\'a0\'a0\cb1 \
\cb3 opposed to finding just the max like in maximum\'a0 likelihood estimation or reward maximization.\cb1 \
\cb3 In fact, here's an idea: say we have\'a0 a reward function for molecules.\cb1 \
\cb3 What If instead of just getting a single\'a0 molecule, we have a generator of molecules,\'a0\'a0\cb1 \
\cb3 and here's what a generator does: it generates a\'a0 molecule with a probability that is proportional\'a0\'a0\cb1 \
\cb3 to the reward, meaning that if a molecule has a\'a0 really high reward, then it's more likely that\'a0\'a0\cb1 \
\cb3 it's going to be generated, and if they have\'a0 a bunch of molecules that are equally highly\'a0\'a0\cb1 \
\cb3 likely under the reward function, then\'a0 they are equally likely to be generated.\cb1 \
\cb3 Now, as I generate molecules using this generator,\'a0 we're going to get candidates, and most of them\'a0\'a0\cb1 \
\cb3 are going to have high rewards because low-reward\'a0 ones have low probabilities getting generated.\cb1 \
\cb3 Imagine an objective function that allows\'a0 you to train a generator like that.\cb1 \
\cb3 The objective function and the\'a0 algorithm that allows you to do\'a0\'a0\cb1 \
\cb3 that could be generative flow network or GFlowNet,\'a0\'a0\cb1 \
\cb3 and this drug discovery example is actually the\'a0 motivating use case in the first GFlowNet paper.\cb1 \
\cb3 However, GFlowNet is much\'a0 more than just drug discovery.\cb1 \
\pard\pardeftab720\partightenfactor0

\f0\b\fs43\fsmilli21600 \cf2 \cb3 What GFlowNet really is
\f1\b0\fs20 \cf0 \cb1 \strokec4 \
\pard\pardeftab720\partightenfactor0

\fs28\fsmilli14400 \cf2 \cb3 \strokec2 The high level takeway is that\'a0 GFlowNet is a novel training algorithm.\cb1 \
\cb3 Instead of looking at a dataset or\'a0 a reward function and ask "how can I\'a0\'a0\cb1 \
\cb3 find the function that maximizes this?" GFlowNet\'a0 asks "okay, how can I find a sampler -- a neural\'a0\'a0\cb1 \
\cb3 network sampler -- that allows me to sample\'a0 proportional to a given reward function?"\cb1 \
\cb3 If we're given a dataset\'a0 instead of a reward function,\'a0\'a0\cb1 \
\cb3 there's another paper from our lab\'a0 which says "okay, given a dataset,\'a0\'a0\cb1 \
\cb3 I'm going to first learn an energy-based\'a0 model, and then I'll train a sampler."\cb1 \
\cb3 The sampler will sample proportional\'a0 to my energy-based model.\cb1 \
\cb3 So, GFlowNet is really\'a0 shifting the question we ask:\'a0\'a0\cb1 \
\cb3 instead of maximizing something,\'a0 we're matching a distribution.\cb1 \
\cb3 And finally, I'm going to give you a quick example\'a0 of how this can be useful in the real world, and,\'a0\'a0\cb1 \
\cb3 actually I'm going to give you two\'a0 examples -- two papers that I led.\cb1 \
\cb3 And I'm happy to dive into these\'a0 two papers in future videos,\'a0\'a0\cb1 \
\cb3 but today I'm just going to give you a\'a0 quick taste of what it could look like.\cb1 \
\pard\pardeftab720\partightenfactor0

\f0\b\fs43\fsmilli21600 \cf2 \cb3 Applications: GFlowNet-EM
\f1\b0\fs20 \cf0 \cb1 \strokec4 \
\pard\pardeftab720\partightenfactor0

\fs28\fsmilli14400 \cf2 \cb3 \strokec2 The first paper is called GFlowNet-EM,\'a0\'a0\cb1 \
\cb3 and we're tackling a problem\'a0 fundamental to machine learning.\cb1 \
\cb3 And the second one is going to be more empirical:\'a0 it has something to do with large language models.\cb1 \
\cb3 Many of us have heard of the\'a0 expectation-maximization algorithm,\'a0\'a0\cb1 \
\cb3 which is used to find maximum likelihood\'a0 estimates in latent-variable models.\cb1 \
\cb3 So here, the big idea is that in the expectation\'a0 step, we want to sample from a posterior\'a0\'a0\cb1 \
\cb3 distribution over latent variables and what\'a0 happens is that this posterior distribution\'a0\'a0\cb1 \
\cb3 for non-trivial models is usually intractable, so\'a0 people do things like Markov Chain Monte Carlo,\'a0\'a0\cb1 \
\cb3 where they make simplifying assumptions\'a0 so the posterior becomes easy to model.\cb1 \
\cb3 So now, I have this intractable\'a0 posterior distribution which can\'a0\'a0\cb1 \
\cb3 be described by a reward function.\cb1 \
\cb3 Reinforcement learning can help us\'a0 find the maximum of this posterior,\'a0\'a0\cb1 \
\cb3 but we want to match the distribution.\cb1 \
\cb3 We want to draw samples from the distribution\'a0 for learning, which is a hard inference problem,\'a0\'a0\cb1 \
\cb3 and here GFlowNet converts this hard inference\'a0 problem, usually solved with simulation,\'a0\'a0\cb1 \
\cb3 into something we can use a neural network to\'a0 solve, and we love training big neural network\'a0\'a0\cb1 \
\cb3 these days, because we're good at it, which\'a0 makes GFlowNet a bridge between classical\'a0\'a0\cb1 \
\cb3 problems in machine learning and scaling\'a0 neural networks, which is the future of AI.\cb1 \
\pard\pardeftab720\partightenfactor0

\f0\b\fs43\fsmilli21600 \cf2 \cb3 Applications: Better LLM reasoning
\f1\b0\fs20 \cf0 \cb1 \strokec4 \
\pard\pardeftab720\partightenfactor0

\fs28\fsmilli14400 \cf2 \cb3 \strokec2 In the second example, we\'a0 have a large language model,\'a0\'a0\cb1 \
\cb3 and we want to use it to solve, say,\'a0 a certain kind of reasoning task.\cb1 \
\cb3 However, we don't have a lot of data points. What\'a0 we have is, say, maybe 10, 20, or 50 data points.\cb1 \
\cb3 What we going to do, instead of doing\'a0 fine-tuning, which easily leads to overfitting,\'a0\'a0\cb1 \
\cb3 we're actually going to search the posterior\'a0 or potential reasoning chains under this model.\cb1 \
\cb3 Usually, people either find the\'a0 most likely reasoning chain using\'a0\'a0\cb1 \
\cb3 reinforcement learning or\'a0 maybe few-shot prompting,\'a0\'a0\cb1 \
\cb3 basically hoping the model will come up with a\'a0 good reasoning chain if we ask it really nicely.\cb1 \
\cb3 Here, we're using GFlowNet to actually\'a0 train the model to directly sample good\'a0\'a0\cb1 \
\cb3 reasoning chains that could have led\'a0 to the correct answer under the model\'a0\'a0\cb1 \
\cb3 proportional to how likely the reasoning\'a0 chain can lead to the correct answer.\cb1 \
\cb3 So the result here is that we're able\'a0 to boost data efficiency in many cases.\cb1 \
\cb3 This paper will actually be an oral\'a0 presentation at ICLR this year,\'a0\'a0\cb1 \
\cb3 so maybe I'll see many of you in person.\cb1 \
\pard\pardeftab720\partightenfactor0

\f0\b\fs43\fsmilli21600 \cf2 \cb3 Conclusion
\f1\b0\fs20 \cf0 \cb1 \strokec4 \
\pard\pardeftab720\partightenfactor0

\fs28\fsmilli14400 \cf2 \cb3 \strokec2 So long story short, GFlowNet is not\'a0 a new neural network architecture.\cb1 \
\cb3 It's a new learning algorithm that allows you\'a0 to train a sampler that samples proportional\'a0\'a0\cb1 \
\cb3 to a reward function, and it has many\'a0 applications going forward, especially\'a0\'a0\cb1 \
\cb3 as we focus on improving the generalization\'a0 and data efficiency of our neural networks.\cb1 \
\cb3 On the theoretical side, it has connections\'a0 to maximum-entropy reinforcement learning\'a0\'a0\cb1 \
\cb3 with path consistency objectives, which I'm\'a0 happy to dive deeper into in a future video.\cb1 \
\cb3 If you find this video helpful,\'a0 please like, subscribe,\'a0\'a0\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb5 and share it with somebody else who might be\'a0 interested. I'll see you in the next video!\cb1 \
}